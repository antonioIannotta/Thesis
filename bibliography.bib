@article{10.1145/3457607,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {representation learning, deep learning, natural language processing, Fairness and bias in artificial intelligence, machine learning}
}

@inproceedings{10.1145/3194770.3194776,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness Definitions Explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1-7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare '18}
}

@article{GRUETZEMACHER2022102884,
title = {The transformative potential of artificial intelligence},
journal = {Futures},
volume = {135},
pages = {102884},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2021.102884},
url = {https://www.sciencedirect.com/science/article/pii/S0016328721001932},
author = {Ross Gruetzemacher and Jess Whittlestone},
keywords = {Artificial Intelligence, Transformative AI, Human-level AI, Artificial general intelligence},
abstract = {The terms 'human-level artificial intelligence' and 'artificial general intelligence' are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term 'transformative AI' is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be 'transformative' is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.}
}

@inproceedings{10.1145/3308560.3317590,
author = {Roselli, Drew and Matthews, Jeanna and Talagala, Nisha},
title = {Managing Bias in AI},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317590},
doi = {10.1145/3308560.3317590},
abstract = {Recent awareness of the impacts of bias in AI algorithms raises the risk for companies to deploy such algorithms, especially because the algorithms may not be explainable in the same way that non-AI algorithms are. Even with careful review of the algorithms and data sets, it may not be possible to delete all unwanted bias, particularly because AI systems learn from historical data, which encodes historical biases. In this paper, we propose a set of processes that companies can use to mitigate and manage three general classes of bias: those related to mapping the business intent into the AI implementation, those that arise due to the distribution of samples used for training, and those that are present in individual input samples. While there may be no simple or complete solution to this issue, best practices can be used to reduce the effects of bias on algorithmic outcomes.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {539–544},
numpages = {6},
keywords = {production monitoring, Artificial intelligence, bias},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{https://doi.org/10.1002/widm.1356,
author = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and Kompatsiaris, Ioannis and Kinder-Kurlanda, Katharina and Wagner, Claudia and Karimi, Fariba and Fernandez, Miriam and Alani, Harith and Berendt, Bettina and Kruegel, Tina and Heinze, Christian and Broelemann, Klaus and Kasneci, Gjergji and Tiropanis, Thanassis and Staab, Steffen},
title = {Bias in data-driven artificial intelligence systems—An introductory survey},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {10},
number = {3},
pages = {e1356},
keywords = {fairness, fairness-aware AI, fairness-aware machine learning, interpretability, responsible AI},
doi = {https://doi.org/10.1002/widm.1356},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1356},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1356},
abstract = {Abstract Artificial Intelligence (AI)-based systems are widely employed nowadays to make decisions that have far-reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth. This article is categorized under: Commercial, Legal, and Ethical Issues > Fairness in Data Mining Commercial, Legal, and Ethical Issues > Ethical Considerations Commercial, Legal, and Ethical Issues > Legal Issues},
year = {2020}
}

@article{10.1145/2983270,
author = {Kirkpatrick, Keith},
title = {Battling Algorithmic Bias: How Do We Ensure Algorithms Treat Us Fairly?},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2983270},
doi = {10.1145/2983270},
journal = {Commun. ACM},
month = {sep},
pages = {16–17},
numpages = {2}
}

@techreport{https://doi.org/10.5281/zenodo.4050457,
doi = {10.5281/ZENODO.4050457},
url = {https://zenodo.org/record/4050457},
author = {Leslie, David},
keywords = {facial recognition technologies, algorithmic bias, digital ethics, responsible innovation, biometric technologies},
title = {Understanding bias in facial recognition technologies},
publisher = {Zenodo},
year = {2020},
copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{9660177,
author={Noiret, Sophie and Lumetzberger, Jennifer and Kampel, Martin},
booktitle={2021 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
title={Bias and Fairness in Computer Vision Applications of the Criminal Justice System}, 
year={2021},
volume={},
number={},
pages={1-8},
doi={10.1109/SSCI50451.2021.9660177}}

@article{doi:10.1080/10345329.2019.1658694,
author = {Carolyn McKay},
title = {Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making},
journal = {Current Issues in Criminal Justice},
volume = {32},
number = {1},
pages = {22-39},
year = {2020},
publisher = {Routledge},
doi = {10.1080/10345329.2019.1658694}}

@article{gebru2020race,
title={Race and gender},
author={Gebru, Timnit},
journal={The Oxford handbook of ethics of aI},
pages={251--269},
year={2020},
publisher={Oxford University Press Oxford}
}

@inproceedings{mujtaba2019ethical,
title={Ethical considerations in AI-based recruitment},
author={Mujtaba, Dena F and Mahapatra, Nihar R},
booktitle={2019 IEEE International Symposium on Technology and Society (ISTAS)},
pages={1--7},
year={2019},
organization={IEEE}
}

@article{10.1145/3616865,
author = {Caton, Simon and Haas, Christian},
title = {Fairness in Machine Learning: A Survey},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616865},
doi = {10.1145/3616865},
abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {aug},
keywords = {fairness, machine learning, transparency, accountability}
}

@ARTICLE{9442706,
author={Salazar, Teresa and Santos, Miriam Seoane and Araújo, Helder and Abreu, Pedro Henriques},
journal={IEEE Access}, 
title={FAWOS: Fairness-Aware Oversampling Algorithm Based on Distributions of Sensitive Attributes}, 
year={2021},
volume={9},
number={},
pages={81370-81379},
doi={10.1109/ACCESS.2021.3084121}}

@inproceedings{NEURIPS2019_8d5e957f,
 author = {Lamy, Alex and Zhong, Ziyuan and Menon, Aditya K and Verma, Nakul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Noise-tolerant fair classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{sharma2020data,
title={Data augmentation for discrimination prevention and bias disambiguation},
author={Sharma, Shubham and Zhang, Yunfeng and R{\'\i}os Aliaga, Jes{\'u}s M and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R},
booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages={358--364},
year={2020}}

@inproceedings{10.1145/3531146.3534644,
author = {Pastaltzidis, Ioannis and Dimitriou, Nikolaos and Quezada-Tavarez, Katherine and Aidinlis, Stergios and Marquenie, Thomas and Gurzawska, Agata and Tzovaras, Dimitrios},
title = {Data Augmentation for Fairness-Aware Machine Learning: Preventing Algorithmic Bias in Law Enforcement Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534644},
doi = {10.1145/3531146.3534644},
abstract = {Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2302-2314},
numpages = {13},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.14778/3461535.3463474,
author = {Salazar, Ricardo and Neutatz, Felix and Abedjan, Ziawasch},
title = {Automated Feature Engineering for Algorithmic Fairness},
year = {2021},
issue_date = {May 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3461535.3463474},
doi = {10.14778/3461535.3463474},
abstract = {One of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the influence of attributes with sensitive information, such as gender or race, and other causally related attributes on the machine learning task. The state-of-the-art bias reduction algorithm Capuchin breaks the causality chain of such attributes by adding and removing tuples. However, this horizontal approach can be considered invasive because it changes the data distribution. A vertical approach would be to prune sensitive features entirely. While this would ensure fairness without tampering with the data, it could also hurt the machine learning accuracy. Therefore, we propose a novel multi-objective feature selection strategy that leverages feature construction to generate more features that lead to both high accuracy and fairness. On three well-known datasets, our system achieves higher accuracy than other fairness-aware approaches while maintaining similar or higher fairness.},
journal = {Proc. VLDB Endow.},
month = {may},
pages = {1694-1702},
numpages = {9}
}

@inproceedings{NEURIPS2021_64ff7983,
author = {Du, Mengnan and Mukherjee, Subhabrata and Wang, Guanchu and Tang, Ruixiang and Awadallah, Ahmed and Hu, Xia},
booktitle = {Advances in Neural Information Processing Systems},
editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
pages = {12091--12103},
publisher = {Curran Associates, Inc.},
title = {Fairness via Representation Neutralization},
url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/64ff7983a47d331b13a81156e2f4d29d-Paper.pdf},
volume = {34},
year = {2021}
}