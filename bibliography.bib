@article{10.1145/3457607,
author = {Mehrabi, Ninareh and Morstatter, Fred and Saxena, Nripsuta and Lerman, Kristina and Galstyan, Aram},
title = {A Survey on Bias and Fairness in Machine Learning},
year = {2021},
issue_date = {July 2022},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {54},
number = {6},
issn = {0360-0300},
url = {https://doi.org/10.1145/3457607},
doi = {10.1145/3457607},
abstract = {With the widespread use of artificial intelligence (AI) systems and applications in our everyday lives, accounting for fairness has gained significant importance in designing and engineering of such systems. AI systems can be used in many sensitive environments to make important and life-changing decisions; thus, it is crucial to ensure that these decisions do not reflect discriminatory behavior toward certain groups or populations. More recently some work has been developed in traditional machine learning and deep learning that address such challenges in different subdomains. With the commercialization of these systems, researchers are becoming more aware of the biases that these applications can contain and are attempting to address them. In this survey, we investigated different real-world applications that have shown biases in various ways, and we listed different sources of biases that can affect AI applications. We then created a taxonomy for fairness definitions that machine learning researchers have defined to avoid the existing bias in AI systems. In addition to that, we examined different domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes in the state-of-the-art methods and ways they have tried to address them. There are still many future directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping that this survey will motivate researchers to tackle these issues in the near future by observing existing work in their respective fields.},
journal = {ACM Comput. Surv.},
month = {jul},
articleno = {115},
numpages = {35},
keywords = {representation learning, deep learning, natural language processing, Fairness and bias in artificial intelligence, machine learning}
}

@inproceedings{10.1145/3194770.3194776,
author = {Verma, Sahil and Rubin, Julia},
title = {Fairness Definitions Explained},
year = {2018},
isbn = {9781450357463},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3194770.3194776},
doi = {10.1145/3194770.3194776},
abstract = {Algorithm fairness has started to attract the attention of researchers in AI, Software Engineering and Law communities, with more than twenty different notions of fairness proposed in the last few years. Yet, there is no clear agreement on which definition to apply in each situation. Moreover, the detailed differences between multiple definitions are difficult to grasp. To address this issue, this paper collects the most prominent definitions of fairness for the algorithmic classification problem, explains the rationale behind these definitions, and demonstrates each of them on a single unifying case-study. Our analysis intuitively explains why the same case can be considered fair according to some definitions and unfair according to others.},
booktitle = {Proceedings of the International Workshop on Software Fairness},
pages = {1-7},
numpages = {7},
location = {Gothenburg, Sweden},
series = {FairWare '18}
}

@article{GRUETZEMACHER2022102884,
title = {The transformative potential of artificial intelligence},
journal = {Futures},
volume = {135},
pages = {102884},
year = {2022},
issn = {0016-3287},
doi = {https://doi.org/10.1016/j.futures.2021.102884},
url = {https://www.sciencedirect.com/science/article/pii/S0016328721001932},
author = {Ross Gruetzemacher and Jess Whittlestone},
keywords = {Artificial Intelligence, Transformative AI, Human-level AI, Artificial general intelligence},
abstract = {The terms 'human-level artificial intelligence' and 'artificial general intelligence' are widely used to refer to the possibility of advanced artificial intelligence (AI) with potentially extreme impacts on society. These terms are poorly defined and do not necessarily indicate what is most important with respect to future societal impacts. We suggest that the term 'transformative AI' is a helpful alternative, reflecting the possibility that advanced AI systems could have very large impacts on society without reaching human-level cognitive abilities. To be most useful, however, more analysis of what it means for AI to be 'transformative' is needed. In this paper, we propose three different levels on which AI might be said to be transformative, associated with different levels of societal change. We suggest that these distinctions would improve conversations between policy makers and decision makers concerning the mid- to long-term impacts of advances in AI. Further, we feel this would have a positive effect on strategic foresight efforts involving advanced AI, which we expect to illuminate paths to alternative futures. We conclude with a discussion of the benefits of our new framework and by highlighting directions for future work in this area.}
}

@inproceedings{10.1145/3308560.3317590,
author = {Roselli, Drew and Matthews, Jeanna and Talagala, Nisha},
title = {Managing Bias in AI},
year = {2019},
isbn = {9781450366755},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3308560.3317590},
doi = {10.1145/3308560.3317590},
abstract = {Recent awareness of the impacts of bias in AI algorithms raises the risk for companies to deploy such algorithms, especially because the algorithms may not be explainable in the same way that non-AI algorithms are. Even with careful review of the algorithms and data sets, it may not be possible to delete all unwanted bias, particularly because AI systems learn from historical data, which encodes historical biases. In this paper, we propose a set of processes that companies can use to mitigate and manage three general classes of bias: those related to mapping the business intent into the AI implementation, those that arise due to the distribution of samples used for training, and those that are present in individual input samples. While there may be no simple or complete solution to this issue, best practices can be used to reduce the effects of bias on algorithmic outcomes.},
booktitle = {Companion Proceedings of The 2019 World Wide Web Conference},
pages = {539–544},
numpages = {6},
keywords = {production monitoring, Artificial intelligence, bias},
location = {San Francisco, USA},
series = {WWW '19}
}

@article{https://doi.org/10.1002/widm.1356,
author = {Ntoutsi, Eirini and Fafalios, Pavlos and Gadiraju, Ujwal and Iosifidis, Vasileios and Nejdl, Wolfgang and Vidal, Maria-Esther and Ruggieri, Salvatore and Turini, Franco and Papadopoulos, Symeon and Krasanakis, Emmanouil and Kompatsiaris, Ioannis and Kinder-Kurlanda, Katharina and Wagner, Claudia and Karimi, Fariba and Fernandez, Miriam and Alani, Harith and Berendt, Bettina and Kruegel, Tina and Heinze, Christian and Broelemann, Klaus and Kasneci, Gjergji and Tiropanis, Thanassis and Staab, Steffen},
title = {Bias in data-driven artificial intelligence systems—An introductory survey},
journal = {WIREs Data Mining and Knowledge Discovery},
volume = {10},
number = {3},
pages = {e1356},
keywords = {fairness, fairness-aware AI, fairness-aware machine learning, interpretability, responsible AI},
doi = {https://doi.org/10.1002/widm.1356},
url = {https://wires.onlinelibrary.wiley.com/doi/abs/10.1002/widm.1356},
eprint = {https://wires.onlinelibrary.wiley.com/doi/pdf/10.1002/widm.1356},
abstract = {Abstract Artificial Intelligence (AI)-based systems are widely employed nowadays to make decisions that have far-reaching impact on individuals and society. Their decisions might affect everyone, everywhere, and anytime, entailing concerns about potential human rights issues. Therefore, it is necessary to move beyond traditional AI algorithms optimized for predictive performance and embed ethical and legal principles in their design, training, and deployment to ensure social good while still benefiting from the huge potential of the AI technology. The goal of this survey is to provide a broad multidisciplinary overview of the area of bias in AI systems, focusing on technical challenges and solutions as well as to suggest new research directions towards approaches well-grounded in a legal frame. In this survey, we focus on data-driven AI, as a large part of AI is powered nowadays by (big) data and powerful machine learning algorithms. If otherwise not specified, we use the general term bias to describe problems related to the gathering or processing of data that might result in prejudiced decisions on the bases of demographic features such as race, sex, and so forth. This article is categorized under: Commercial, Legal, and Ethical Issues > Fairness in Data Mining Commercial, Legal, and Ethical Issues > Ethical Considerations Commercial, Legal, and Ethical Issues > Legal Issues},
year = {2020}
}

@article{10.1145/2983270,
author = {Kirkpatrick, Keith},
title = {Battling Algorithmic Bias: How Do We Ensure Algorithms Treat Us Fairly?},
year = {2016},
issue_date = {October 2016},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
volume = {59},
number = {10},
issn = {0001-0782},
url = {https://doi.org/10.1145/2983270},
doi = {10.1145/2983270},
journal = {Commun. ACM},
month = {sep},
pages = {16–17},
numpages = {2}
}

@techreport{https://doi.org/10.5281/zenodo.4050457,
doi = {10.5281/ZENODO.4050457},
url = {https://zenodo.org/record/4050457},
author = {Leslie, David},
keywords = {facial recognition technologies, algorithmic bias, digital ethics, responsible innovation, biometric technologies},
title = {Understanding bias in facial recognition technologies},
publisher = {Zenodo},
year = {2020},
copyright = {Creative Commons Attribution 4.0 International}
}

@INPROCEEDINGS{9660177,
author={Noiret, Sophie and Lumetzberger, Jennifer and Kampel, Martin},
booktitle={2021 IEEE Symposium Series on Computational Intelligence (SSCI)}, 
title={Bias and Fairness in Computer Vision Applications of the Criminal Justice System}, 
year={2021},
volume={},
number={},
pages={1-8},
doi={10.1109/SSCI50451.2021.9660177}}

@article{doi:10.1080/10345329.2019.1658694,
author = {Carolyn McKay},
title = {Predicting risk in criminal procedure: actuarial tools, algorithms, AI and judicial decision-making},
journal = {Current Issues in Criminal Justice},
volume = {32},
number = {1},
pages = {22-39},
year = {2020},
publisher = {Routledge},
doi = {10.1080/10345329.2019.1658694}}

@article{gebru2020race,
title={Race and gender},
author={Gebru, Timnit},
journal={The Oxford handbook of ethics of aI},
pages={251--269},
year={2020},
publisher={Oxford University Press Oxford}
}

@inproceedings{mujtaba2019ethical,
title={Ethical considerations in AI-based recruitment},
author={Mujtaba, Dena F and Mahapatra, Nihar R},
booktitle={2019 IEEE International Symposium on Technology and Society (ISTAS)},
pages={1--7},
year={2019},
organization={IEEE}
}

@article{10.1145/3616865,
author = {Caton, Simon and Haas, Christian},
title = {Fairness in Machine Learning: A Survey},
year = {2023},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
issn = {0360-0300},
url = {https://doi.org/10.1145/3616865},
doi = {10.1145/3616865},
abstract = {When Machine Learning technologies are used in contexts that affect citizens, companies as well as researchers need to be confident that there will not be any unexpected social implications, such as bias towards gender, ethnicity, and/or people with disabilities. There is significant literature on approaches to mitigate bias and promote fairness, yet the area is complex and hard to penetrate for newcomers to the domain. This article seeks to provide an overview of the different schools of thought and approaches that aim to increase the fairness of Machine Learning. It organises approaches into the widely accepted framework of pre-processing, in-processing, and post-processing methods, subcategorizing into a further 11 method areas. Although much of the literature emphasizes binary classification, a discussion of fairness in regression, recommender systems, and unsupervised learning is also provided along with a selection of currently available open source libraries. The article concludes by summarising open challenges articulated as five dilemmas for fairness research.},
note = {Just Accepted},
journal = {ACM Comput. Surv.},
month = {aug},
keywords = {fairness, machine learning, transparency, accountability}
}

@ARTICLE{9442706,
author={Salazar, Teresa and Santos, Miriam Seoane and Araújo, Helder and Abreu, Pedro Henriques},
journal={IEEE Access}, 
title={FAWOS: Fairness-Aware Oversampling Algorithm Based on Distributions of Sensitive Attributes}, 
year={2021},
volume={9},
number={},
pages={81370-81379},
doi={10.1109/ACCESS.2021.3084121}}

@inproceedings{NEURIPS2019_8d5e957f,
 author = {Lamy, Alex and Zhong, Ziyuan and Menon, Aditya K and Verma, Nakul},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {Noise-tolerant fair classification},
 url = {https://proceedings.neurips.cc/paper_files/paper/2019/file/8d5e957f297893487bd98fa830fa6413-Paper.pdf},
 volume = {32},
 year = {2019}
}

@inproceedings{sharma2020data,
title={Data augmentation for discrimination prevention and bias disambiguation},
author={Sharma, Shubham and Zhang, Yunfeng and R{\'\i}os Aliaga, Jes{\'u}s M and Bouneffouf, Djallel and Muthusamy, Vinod and Varshney, Kush R},
booktitle={Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society},
pages={358--364},
year={2020}}

@inproceedings{10.1145/3531146.3534644,
author = {Pastaltzidis, Ioannis and Dimitriou, Nikolaos and Quezada-Tavarez, Katherine and Aidinlis, Stergios and Marquenie, Thomas and Gurzawska, Agata and Tzovaras, Dimitrios},
title = {Data Augmentation for Fairness-Aware Machine Learning: Preventing Algorithmic Bias in Law Enforcement Systems},
year = {2022},
isbn = {9781450393522},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3531146.3534644},
doi = {10.1145/3531146.3534644},
abstract = {Researchers and practitioners in the fairness community have highlighted the ethical and legal challenges of using biased datasets in data-driven systems, with algorithmic bias being a major concern. Despite the rapidly growing body of literature on fairness in algorithmic decision-making, there remains a paucity of fairness scholarship on machine learning algorithms for the real-time detection of crime. This contribution presents an approach for fairness-aware machine learning to mitigate the algorithmic bias / discrimination issues posed by the reliance on biased data when building law enforcement technology. Our analysis is based on RWF-2000, which has served as the basis for violent activity recognition tasks in data-driven law enforcement projects. We reveal issues of overrepresentation of minority subjects in violence situations that limit the external validity of the dataset for real-time crime detection systems and propose data augmentation techniques to rebalance the dataset. The experiments on real world data show the potential to create more balanced datasets by synthetically generated samples, thus mitigating bias and discrimination concerns in law enforcement applications.},
booktitle = {Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency},
pages = {2302-2314},
numpages = {13},
location = {Seoul, Republic of Korea},
series = {FAccT '22}
}

@article{10.14778/3461535.3463474,
author = {Salazar, Ricardo and Neutatz, Felix and Abedjan, Ziawasch},
title = {Automated Feature Engineering for Algorithmic Fairness},
year = {2021},
issue_date = {May 2021},
publisher = {VLDB Endowment},
volume = {14},
number = {9},
issn = {2150-8097},
url = {https://doi.org/10.14778/3461535.3463474},
doi = {10.14778/3461535.3463474},
abstract = {One of the fundamental problems of machine ethics is to avoid the perpetuation and amplification of discrimination through machine learning applications. In particular, it is desired to exclude the influence of attributes with sensitive information, such as gender or race, and other causally related attributes on the machine learning task. The state-of-the-art bias reduction algorithm Capuchin breaks the causality chain of such attributes by adding and removing tuples. However, this horizontal approach can be considered invasive because it changes the data distribution. A vertical approach would be to prune sensitive features entirely. While this would ensure fairness without tampering with the data, it could also hurt the machine learning accuracy. Therefore, we propose a novel multi-objective feature selection strategy that leverages feature construction to generate more features that lead to both high accuracy and fairness. On three well-known datasets, our system achieves higher accuracy than other fairness-aware approaches while maintaining similar or higher fairness.},
journal = {Proc. VLDB Endow.},
month = {may},
pages = {1694-1702},
numpages = {9}
}

@inproceedings{NEURIPS2021_64ff7983,
author = {Du, Mengnan and Mukherjee, Subhabrata and Wang, Guanchu and Tang, Ruixiang and Awadallah, Ahmed and Hu, Xia},
booktitle = {Advances in Neural Information Processing Systems},
editor = {M. Ranzato and A. Beygelzimer and Y. Dauphin and P.S. Liang and J. Wortman Vaughan},
pages = {12091--12103},
publisher = {Curran Associates, Inc.},
title = {Fairness via Representation Neutralization},
url = {https://proceedings.neurips.cc/paper_files/paper/2021/file/64ff7983a47d331b13a81156e2f4d29d-Paper.pdf},
volume = {34},
year = {2021}
}

@INPROCEEDINGS{6137441,
author={Kamishima, Toshihiro and Akaho, Shotaro and Sakuma, Jun},
booktitle={2011 IEEE 11th International Conference on Data Mining Workshops}, 
title={Fairness-aware Learning through Regularization Approach}, 
year={2011},
volume={},
number={},
pages={643-650},
doi={10.1109/ICDMW.2011.83}}

@inproceedings{10.1145/3178876.3186133,
author = {Krasanakis, Emmanouil and Spyromitros-Xioufis, Eleftherios and Papadopoulos, Symeon and Kompatsiaris, Yiannis},
title = {Adaptive Sensitive Reweighting to Mitigate Bias in Fairness-Aware Classification},
year = {2018},
isbn = {9781450356398},
publisher = {International World Wide Web Conferences Steering Committee},
address = {Republic and Canton of Geneva, CHE},
url = {https://doi.org/10.1145/3178876.3186133},
doi = {10.1145/3178876.3186133},
abstract = {Machine learning bias and fairness have recently emerged as key issues due to the pervasive deployment of data-driven decision making in a variety of sectors and services. It has often been argued that unfair classifications can be attributed to bias in training data, but previous attempts to 'repair' training data have led to limited success. To circumvent shortcomings prevalent in data repairing approaches, such as those that weight training samples of the sensitive group (e.g. gender, race, financial status) based on their misclassification error, we present a process that iteratively adapts training sample weights with a theoretically grounded model. This model addresses different kinds of bias to better achieve fairness objectives, such as trade-offs between accuracy and disparate impact elimination or disparate mistreatment elimination. We show that, compared to previous fairness-aware approaches, our methodology achieves better or similar trades-offs between accuracy and unfairness mitigation on real-world and synthetic datasets.},
booktitle = {Proceedings of the 2018 World Wide Web Conference},
pages = {853–862},
numpages = {10},
keywords = {classification fairness, reweighting, algorithm bias},
location = {Lyon, France},
series = {WWW '18}
}

@InProceedings{10.1007/978-3-642-33486-3_3,
author="Kamishima, Toshihiro
and Akaho, Shotaro
and Asoh, Hideki
and Sakuma, Jun",
editor="Flach, Peter A.
and De Bie, Tijl
and Cristianini, Nello",
title="Fairness-Aware Classifier with Prejudice Remover Regularizer",
booktitle="Machine Learning and Knowledge Discovery in Databases",
year="2012",
publisher="Springer Berlin Heidelberg",
address="Berlin, Heidelberg",
pages="35--50",
abstract="With the spread of data mining technologies and the accumulation of social data, such technologies and data are being used for determinations that seriously affect individuals' lives. For example, credit scoring is frequently determined based on the records of past credit data together with statistical prediction techniques. Needless to say, such determinations must be nondiscriminatory and fair in sensitive features, such as race, gender, religion, and so on. Several researchers have recently begun to attempt the development of analysis techniques that are aware of social fairness or discrimination. They have shown that simply avoiding the use of sensitive features is insufficient for eliminating biases in determinations, due to the indirect influence of sensitive information. In this paper, we first discuss three causes of unfairness in machine learning. We then propose a regularization approach that is applicable to any prediction algorithm with probabilistic discriminative models. We further apply this approach to logistic regression and empirically show its effectiveness and efficiency.",
isbn="978-3-642-33486-3"
}

@inproceedings{jiang2022generalized,
title={Generalized Demographic Parity for Group Fairness},
author={Zhimeng Jiang and Xiaotian Han and Chao Fan and Fan Yang and Ali Mostafavi and Xia Hu},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=YigKlMJwjye}
}


@InProceedings{pmlr-v139-xu21b,
title={To be Robust or to be Fair: Towards Fairness in Adversarial Training},
author={Xu, Han and Liu, Xiaorui and Li, Yaxin and Jain, Anil and Tang, Jiliang},
booktitle={Proceedings of the 38th International Conference on Machine Learning},
pages={11492--11501},
year={2021},
editor={Meila, Marina and Zhang, Tong},
volume={139},
series={Proceedings of Machine Learning Research},
month={18--24 Jul},
publisher={PMLR},
pdf={http://proceedings.mlr.press/v139/xu21b/xu21b.pdf},
url={https://proceedings.mlr.press/v139/xu21b.html},
abstract={Adversarial training algorithms have been proved to be reliable to improve machine learning models’ robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD l_infty-8 adversarial accuracy on the class ”automobile” but only 65% and 17% on class ”cat”. This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we empirically and theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models’ robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.}
}

@inproceedings{10.1145/3447548.3467251,
author={Cui, Sen and Pan, Weishen and Zhang, Changshui and Wang, Fei},
title={Towards Model-Agnostic Post-Hoc Adjustment for Balancing Ranking Fairness and Algorithm Utility},
year={2021},
isbn={9781450383325},
publisher={Association for Computing Machinery},
address={New York, NY, USA},
url={https://doi.org/10.1145/3447548.3467251},
doi={10.1145/3447548.3467251},
abstract={Bipartite ranking, which aims to learn a scoring function that ranks positive individuals higher than negative ones from labeled data, is widely adopted in various applications where sample prioritization is needed. Recently, there have been rising concerns on whether the learned scoring function can cause systematic disparity across different protected groups defined by sensitive attributes. While there could be trade-off between fairness and performance, in this paper we propose a model agnostic post-processing framework for balancing them in the bipartite ranking scenario. Specifically, we maximize a weighted sum of the utility and fairness by directly adjusting the relative ordering of samples across groups. By formulating this problem as the identification of an optimal warping path across different protected groups, we propose a non-parametric method to search for such an optimal path through a dynamic programming process. Our method is compatible with various classification models and applicable to a variety of ranking fairness metrics. Comprehensive experiments on a suite of benchmark data sets and two real-world patient electronic health record repositories show that our method can achieve a great balance between the algorithm utility and ranking fairness. Furthermore, we experimentally verify the robustness of our method when faced with the fewer training samples and the difference between training and testing ranking score distributions.},
booktitle={Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery \& Data Mining},
pages={207-217},
numpages={11},
location={Virtual Event, Singapore},
series={KDD '21}
}

@inproceedings{NIPS2017_a486cd07,
 author={Kusner, Matt J and Loftus, Joshua and Russell, Chris and Silva, Ricardo},
 booktitle={Advances in Neural Information Processing Systems},
 editor={I. Guyon and U. Von Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
 pages={},
 publisher={Curran Associates, Inc.},
 title={Counterfactual Fairness},
 url={https://proceedings.neurips.cc/paper_files/paper/2017/file/a486cd07e4ac3d270571622f4f316ec5-Paper.pdf},
 volume={30},
 year={2017}
}

@inproceedings{10.1145/3442188.3445902,
author={Mishler, Alan and Kennedy, Edward H. and Chouldechova, Alexandra},
title={Fairness in Risk Assessment Instruments: Post-Processing to Achieve Counterfactual Equalized Odds},
year={2021},
isbn={9781450383097},
publisher={Association for Computing Machinery},
address={New York, NY, USA},
url={https://doi.org/10.1145/3442188.3445902},
doi={10.1145/3442188.3445902},
abstract={In domains such as criminal justice, medicine, and social welfare, decision makers increasingly have access to algorithmic Risk Assessment Instruments (RAIs). RAIs estimate the risk of an adverse outcome such as recidivism or child neglect, potentially informing high-stakes decisions such as whether to release a defendant on bail or initiate a child welfare investigation. It is important to ensure that RAIs are fair, so that the benefits and harms of such decisions are equitably distributed.The most widely used algorithmic fairness criteria are formulated with respect to observable outcomes, such as whether a person actually recidivates, but these criteria are misleading when applied to RAIs. Since RAIs are intended to inform interventions that can reduce risk, the prediction itself affects the downstream outcome. Recent work has argued that fairness criteria for RAIs should instead utilize potential outcomes, i.e. the outcomes that would occur in the absence of an appropriate intervention [11]. However, no methods currently exist to satisfy such fairness criteria.In this paper, we target one such criterion, counterfactual equalized odds. We develop a post-processed predictor that is estimated via doubly robust estimators, extending and adapting previous postprocessing approaches [16] to the counterfactual setting. We also provide doubly robust estimators of the risk and fairness properties of arbitrary fixed post-processed predictors. Our predictor converges to an optimal fair predictor at fast rates. We illustrate properties of our method and show that it performs well on both simulated and real data.},
booktitle={Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency},
pages={386-400},
numpages={15},
keywords={fairness, post-processing, counterfactual, risk assessment},
location={Virtual Event, Canada},
series={FAccT '21}
}

@article{Gupta2018ProxyF,
title={Proxy Fairness},
author={Maya R. Gupta and Andrew Cotter and Mahdi Milani Fard and Serena Lutong Wang},
journal={ArXiv},
year={2018},
volume={abs/1806.11212},
url={https://api.semanticscholar.org/CorpusID:49555790}
}

@inproceedings{agarwal2018reductions,
title={A reductions approach to fair classification},
author={Agarwal, Alekh and Beygelzimer, Alina and Dud{\'\i}k, Miroslav and Langford, John and Wallach, Hanna},
booktitle={International conference on machine learning},
pages={60--69},
year={2018},
organization={PMLR}
}

@inproceedings{10.5555/3157382.3157469,
author = {Hardt, Moritz and Price, Eric and Srebro, Nathan},
title = {Equality of Opportunity in Supervised Learning},
year = {2016},
isbn = {9781510838819},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy. We enourage readers to consult the more complete manuscript on the arXiv.},
booktitle = {Proceedings of the 30th International Conference on Neural Information Processing Systems},
pages = {3323–3331},
numpages = {9},
location = {Barcelona, Spain},
series = {NIPS'16}
}

