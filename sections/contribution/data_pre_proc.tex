\section{Data Pre-processing}
\label{section:pre-proc}

Following the completion of data collection and the incorporation of stakeholder considerations, the subsequent pivotal stage in the workflow is data pre-processing. This critical phase is dedicated to meticulously preparing and cleansing the dataset with the overarching goal of addressing biases, rectifying imbalances, and ensuring a fair and equitable foundation for the ensuing stages of the fair-by-design workflow. The emphasis during data pre-processing is on refining the dataset to foster a more equitable and unbiased representation, thus laying the groundwork for robust and ethical data-driven decision-making processes.

\subsection{Key Considerations}

\begin{enumerate}

    \item \emph{Data cleaning:} 

    This process involves identifying and rectifying errors, inconsistencies, and inaccuracies in the raw dataset to ensure its reliability and quality. Common tasks in data cleaning include handling missing values, correcting data format issues, removing duplicates, and addressing outliers. The goal is to create a clean, standardized dataset that forms a solid foundation for subsequent analysis and model development. Rigorous data cleaning is essential for mitigating the risk of biased or inaccurate model outcomes, as machine learning models heavily depend on the quality of the input data. This meticulous attention to data quality sets the stage for effective feature engineering, model training, and, ultimately, the generation of reliable and fair predictions.

    \item \emph{Handling Protected Attributes:} 
    
    Within the Fair-by-Design workflow, categorical protected attributes undergo a transformation process to ensure fairness in subsequent machine learning stages. This process involves setting a threshold for each protected attribute and replacing values above the threshold with 1, and values below the threshold with 0.

    Let \(P\) be the set of protected attributes, and \(p_i\) represent a specific protected attribute within \(P\). The threshold, denoted as \(t_i\), is determined for each categorical protected attribute \(p_i\) in the dataset through the following formalism:

    \[
    t_i = \frac{\text{min}(p_i) + \text{max}(p_i)}{2}
    \]

    This calculation ensures a nuanced understanding of the prevalent categories within these attributes and establishes a threshold \(t_i\) for distinguishing between them.

    The binary representation preserves essential information about protected attributes, providing a condensed yet informative encoding that reflects the privilege or lack thereof concerning specific attributes.

    During the training phase, the model learns to distinguish instances based on privilege or lack thereof concerning specific attributes, as represented by the binary encoding. This is achieved by replacing values according to the following transformation:

    \[
    \text{Binary Encoding}(p_i) = \begin{cases} 

        1 & \text{if } p_i > t_i \\

        0 & \text{if } p_i \leq t_i 

    \end{cases}
    \]

    This facilitates effective model incorporation and decision-making during training and prediction phases.

    The nuanced encoding strategy, where values over the threshold are replaced with 1 and values under the threshold are replaced with 0, serves as the foundation for unbiased and fair treatment of instances within the model. By transforming categorical protected attributes based on the specified threshold definition, the Fair-by-Design workflow ensures that subsequent machine learning models can effectively incorporate and act upon this information during training and prediction phases, contributing significantly to the development of a more equitable and ethically sound machine learning model.

    \item \emph{Handling Output Variable:} 
    
    The handling of the output variable in the Fair-by-Design workflow is a nuanced process that adapts to the nature of the variable, whether categorical or continuous. This tailored treatment is essential for aligning the prediction task with the fairness objectives of the workflow, ensuring that the model's predictions are not only accurate but also ethically sound.


    In cases where the output variable is categorical, the workflow initiates by determining the most frequent value within this category. Let $Y$ represent the output variable, and $y_i$ be a specific outcome within $Y$. The most frequent value, denoted as $y_{\text{freq}}$, is determined through the following formalism:

    \[
    y_{\text{freq}} = \text{argmax}_y \left( \text{count}(Y = y) \right)
    \]

    This strategic calculation provides insights into the prevailing outcome within the dataset. Subsequently, instances where the output variable matches $y_{\text{freq}}$ are replaced with the binary representation 1, while all other instances take on the binary representation 0. The binary encoding function is defined as follows:

    \[
    \text{Binary Encoding}(y_i) = \begin{cases} 
    1 & \text{if } y_i = y_{\text{freq}} \\ 
    0 & \text{otherwise}
    \end{cases}
    \]

    This transformation effectively redefines the prediction task, shifting the focus to discerning whether a given sample belongs to the most frequent outcome. The binary encoding approach not only simplifies the representation of categorical outcomes but also enables the model to make predictions in a manner that aligns with fairness considerations.


    Conversely, when the output variable is continuous, the workflow employs a distinct strategy. Let $Z$ represent the continuous output variable, and $z_i$ be a specific value within $Z$. Values of the continuous variable that surpass the mean are replaced with 1, while those falling below the mean assume the binary representation 0. This adjustment transforms the prediction task into establishing whether a sample's continuous output is above the mean. The binary encoding function for continuous variables is defined as:

    \[
    \text{Binary Encoding}(z_i) = \begin{cases} 
    1 & \text{if } z_i > \text{mean}(Z) \\ 
    0 & \text{otherwise}
    \end{cases}
    \]

    This approach ensures that the model is not only attentive to the central tendency of the continuous variable but also considers the distribution of values concerning the mean.

    This tailored and meticulous treatment of the output variable underscores the commitment of the Fair-by-Design workflow to equitable and ethical machine learning practices. The workflow not only adapts to the inherent characteristics of the data but also ensures that the subsequent model operates with a heightened awareness of fairness considerations, contributing to the development of a more responsible and unbiased machine learning model.

\end{enumerate}

This nuanced pre-processing step sets the groundwork for a fair and unbiased learning prediction, aligning the dataset with the ethical considerations embedded in the Fair-by-Design workflow.
