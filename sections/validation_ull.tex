\chapter{Validation}
\label{chap:validation}

\section{Introduction}
This chapter unfolds as a meticulous exploration into the performance and reliability of the Fair-by-Design workflow. To conduct a thorough assessment, the Educational Dataset obtained by \emph{Universidad de La Laguna} (ULL) has been strategically chosen as the testing ground. This dataset, rich in attributes that can be both protected and impactful on students' performance in a specific subject, provides a nuanced platform for evaluating the workflow's effectiveness across various machine learning algorithms.

The central emphasis of this validation endeavor is to ascertain not only the accuracy and predictive capabilities of the implemented models but also the extent to which the Fair-by-Design workflow succeeds in fostering fairness. The Educational Dataset allows for a detailed examination of how different algorithms respond to the intricacies of real-world educational data, shedding light on the interplay between accuracy and fairness.

Throughout this chapter, the validation process unfolds systematically, encompassing diverse algorithms applied to the Educational Dataset. The goal is to detect both the strengths and potential limitations of the Fair-by-Design approach, providing valuable insights for its application in educational scenarios where the equitable treatment of students and the accuracy of predictions are of paramount importance. The subsequent sections detail the experimental setup, algorithmic choices, and the rigorous evaluation metrics employed to ensure a comprehensive understanding of the Fair-by-Design workflow's performance on the chosen educational dataset.

\section{Dataset description}

Before delving into the intricate details of the algorithm implementations presented earlier, it is imperative to provide a comprehensive overview of the dataset on which these algorithms have been applied. The chosen dataset for this work is the \emph{Canary Island Educational dataset}, an invaluable resource that underpins the empirical exploration of bias mitigation strategies in the context of the educational system in the Canary Islands. 

The Canary Island Educational dataset is a rich and expansive repository of information, meticulously compiled to capture various facets of the educational landscape within the Canary Islands. This dataset comprises the comprehensive census of students enrolled over four distinct academic years, offering a multifaceted glimpse into the educational ecosystem. 

The dataset encompasses a diverse array of attributes and data points, encapsulating critical information such as student demographics, academic performance, socioeconomic factors, and other pertinent variables. These attributes collectively provide a holistic perspective on the educational landscape, enabling a nuanced analysis of the factors that influence student outcomes and experiences. 

The temporal dimension of the dataset, spanning four academic years, further enriches the analytical potential. It allows for the investigation of temporal trends, shifts in educational policies, and the evolution of student characteristics over time. This temporal depth is particularly valuable when examining the efficacy of bias mitigation strategies, as it facilitates the assessment of their impact across different academic years. 

The Canary Island Educational dataset is not merely a repository of numbers and statistics; it is a window into the educational opportunities and challenges faced by students in the Canary Islands. By harnessing the insights gleaned from this dataset, it becomes possible to proactively address biases and promote equity within the educational system, ultimately striving for a more inclusive and just educational landscape.

\section{Precise Definition of Objectives}
\label{section:val_obj}

In this section, are meticulously outlined the objectives that intricately guide the Fair-by-Design workflow, with a primary focus on achieving effective prediction about the english level of each student while concurrently addressing fairness considerations and mitigating biases. The critical components encompass the meticulous identification of protected attributes, the strategic selection of fairness notions, and the corresponding choice of fairness metrics.

\subsection{Prediction Objective}

The overarching objective is to execute a fairness-driven classification task, discerning whether a student's english level is either above or under a certain threshold for each student enrolled in the 4 grade.

\subsection{Protected Attributes}

The identified protected attributes for scrutiny within the Fair-by-Design framework are the following:

\begin{itemize}

    \item Sex

    \item Capital Island: if the student comes from the capital of the city

    \item Public-Private: if the school is public or private
    
    \item Household Income: mothly houseold income

    \item ESCS: economic, social and cultural satus index

\end{itemize}

These attributes hold particular significance in the context of potential biases and fairness considerations.

\subsection{Fairness Notions}

To operationalize fairness, two distinct notions are employed, each serving specific objectives:

\begin{itemize}
    \item \emph{Demographic Parity:} The objective is to ensure a consistent distribution of predicted outcomes across different subgroups delineated by the protected attributes. This aligns with the broader goal of fostering equity in predictions.

    \item \emph{Group Fairness:} The goal is to guarantee equitable predictions within specific subgroups defined by protected attributes. This notion emphasizes the need for fairness at a more granular level, considering distinct demographic categories.
\end{itemize}

\subsection{Fairness Metrics}

To quantify and assess fairness in the context of the defined notions, specific fairness metrics are employed:

\begin{itemize}
    \item \emph{For Demographic Parity:} The chosen metric is \emph{Disparate Impact}, offering a quantitative measure of the disparate outcomes across different subgroups.

    \item \emph{For Group Fairness:} The metric utilized is \emph{Demographic Parity Difference}, providing a nuanced assessment of fairness within specific demographic subgroups.
\end{itemize}

These meticulously defined objectives, coupled with the chosen fairness notions and metrics, collectively establish a robust foundation for the Fair-by-Design workflow. This ensures a focused approach towards accurate predictions while concurrently fostering fairness and equity across diverse subgroups.

\section{Data Collection Details}
\label{section:val_dc}

In this section, we delve into the specifics of the data collection process, providing a comprehensive overview of the dataset and its division into distinct training and test sets.

\subsection{Dataset Overview}

The dataset comprises two distinct sets:

\begin{itemize}
    \item \emph{Training Set:} Consisting of 9,831 items and 100 attributes.
    \item \emph{Test Set:} Comprising 3,246 items and 100 attributes.
\end{itemize}

\subsection{Dataset Composition}

The dataset encompasses 99 independent features of diverse types, each contributing valuable information for predictive modeling. Additionally, there is one dependent variable, representing whether a student's english level is beyond or above a certain threshold. Each record within the dataset corresponds to an individual, and the output variable is binary, delineating income categories.

\subsection{Protected Attributes and Encoding Consistency}

As identified in \cref{section:val_obj}, five discrete protected attributes play a crucial role in fairness considerations. It is imperative to note that these attributes are discrete in nature. Furthermore, an essential consideration for subsequent analysis pertains to the representation of the output variable. Although it is binary, it is currently encoded as a string. Ensuring consistency in the encoding of this variable across both the training and test sets is of paramount importance for accurate and meaningful analysis in subsequent stages.

This detailed understanding of the dataset's composition and attributes provides a solid foundation for subsequent steps in the Fair-by-Design workflow, facilitating informed decisions regarding model development and fairness considerations.

\section{Data pre-processing}

Starting from the information provided by \cref{section:val_dc} in this section all the variables represend as string as been labeled the same manner. This led to a consistent dataset to be used in the models training.

\section{Algorithm design}
\label{section:val_alg}

In the context of the Fair-by-Design workflow, four key fairness algorithms have been strategically chosen to comprehensively test the workflow's effectiveness. Each algorithm serves a specific purpose within the workflow, contributing to the overarching goal of achieving fairness in predictions.

\subsection{Pre-processing Algorithms}

\subsubsection{Data Augmentation Algorithm}

\begin{itemize}

    \item \emph{Algorithm Description:} The proposed data augmentation algorithm introduces synthetic samples to the training dataset. By augmenting the data, especially focusing on underrepresented groups, the algorithm aims to enhance the model's exposure to diverse scenarios, promoting a more robust understanding of the underlying patterns.

    \item \emph{Reasoning:} Data augmentation is crucial for addressing imbalances in the training data, allowing the model to generalize better across different groups and mitigating biases stemming from insufficient representation.

\end{itemize}

\subsubsection{Fairness Through Unawareness}

\begin{itemize}

    \item \emph{Algorithm Description:} Fairness through unawareness involves deliberately avoiding the use of sensitive attributes in the modeling process. This pre-processing approach aims to promote fairness by excluding potentially biased features, thus reducing the risk of discrimination based on sensitive attributes.

    \item \emph{Reasoning:} Fairness through unawareness is considered to mitigate biases by preventing the model from directly using sensitive attributes, thereby minimizing the potential for biased predictions associated with these attributes.

\end{itemize}

\subsection{In-processing Algorithms}

\subsubsection{ExponentiatedGradient}

\begin{itemize}

    \item \emph{Algorithm Description}: The ExponentiatedGradient algorithm applies iterative re-weighting to the training data, seeking to find a fair classifier by minimizing the disparity in predictions across sensitive groups.

    \item \emph{Reasoning:} ExponentiatedGradient is chosen for its versatility and effectiveness in in-processing fairness. It provides a fine-tuning mechanism during training to achieve parity in predicted outcomes.

\end{itemize}

\subsubsection{GridSearch}

\begin{itemize}

    \item \emph{Algorithm Description:} GridSearch is a generic algorithm that explores a range of hyperparameter values to find the optimal configuration for a fair classifier.

    \item \emph{Reasoning:} GridSearch is included to assess the impact of hyperparameter tuning on fairness outcomes, offering insights into the versatility of the approach.

\end{itemize}

No post-processing algorithm has been considered due to a lack of the time needed to properly study the algorithms.

The choice of these four algorithms is driven by the aim of obtaining a holistic view of the Fair-by-Design workflow's performance. By incorporating diverse pre-processing and in-processing, the evaluation can capture the nuances of fairness considerations at different stages of the machine learning pipeline. This comprehensive approach ensures a thorough exploration of potential biases and fairness enhancements, laying the foundation for a robust and equitable model.

\section{Model training and evaluation}
\label{section:val_mt_eval}

\subsection{Training Models}

The model training process involves the utilization of two distinct classifiers:

\begin{itemize}

    \item \emph{XGBoost}: XGBoost, an optimized gradient boosting algorithm, is selected for its ability to handle diverse datasets and deliver high predictive accuracy.

    \item \emph{RandomForest Classifier:} RandomForest classifier, known for its ensemble learning approach, is chosen to capture complex relationships in the data and enhance predictive performance.

\end{itemize}

subsection{Performance Evaluation}

\subsection{Results Evaluation}

The evaluation of general results encompasses an assessment of the overall model performance and the computation of fairness metrics.

It's important, before to provide a general view of the results, provide some considerations on the pre-processing algorithms:

\begin{itemize}
    \item \emph{Data augmentation}: this algorithm led the dataset from 9,831 to 83,028 items.
    \item \emph{Fairness through unawareness}: in order to apply this algorithm the protected attribute have been replaced with their \emph{median} value.
\end{itemize}

In order to provide a tabular representation of the results it's necessary to assign some labels to have a more compressed representation and two tables are reported, one for each metric:

\begin{itemize}
    \item \emph{Fairness algorithm}: FairAlg
    \begin{itemize}
        \item \emph{Data augmentation}: DA
        \item \emph{Fairness through unawareness}: FtU
        \item \emph{Exponentiated Gradient}: EG
        \item \emph{GridSearch}: GS
    \end{itemize}
    \item \emph{Model}
    \begin{itemize}
        \item \emph{XGB Classifier}: XGB
        \item \emph{RandomForest Classifier}: RF
    \end{itemize}
    \item \emph{Accuracy}: Acc
    \item \emph{Fairness metric}:
    \begin{itemize}
        \item Protected attribute: Sex
        \begin{itemize}
            \item \emph{Disparate Impact }: DIs
            \item \emph{Demographic Parity Difference}: DPDs
        \end{itemize}
        \item Protected attribute: Capital Island
        \begin{itemize}
            \item \emph{Disparate Impact }: DIc
            \item \emph{Demographic Parity Difference}: DPDc
        \end{itemize}
        \item Protected attribute: CPublic or Private school
        \begin{itemize}
            \item \emph{Disparate Impact }: DIp
            \item \emph{Demographic Parity Difference}: DPDp
        \end{itemize}
        \item Protected attribute: Household Income
        \begin{itemize}
            \item \emph{Disparate Impact }: DIh
            \item \emph{Demographic Parity Difference}: DPDh
        \end{itemize}
        \item Protected attribute: ESCS Index
        \begin{itemize}
            \item \emph{Disparate Impact }: DIe
            \item \emph{Demographic Parity Difference}: DPDe
        \end{itemize}
    \end{itemize}
\end{itemize}

\textbf{Fairness metric: Disparate Impact}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Model} & \textbf{Acc} $\uparrow$ & \textbf{DIs} $\uparrow$ & \textbf{DIc} $\uparrow$ & \textbf{DIp} $\uparrow$ & \textbf{DIh} $\uparrow$ & \textbf{DIe} $\uparrow$ \\
        \hline
        DA & XGB & 0.9975 & 0.9670 & 0.9813 & 0.9901 & 0.9340 & \textcolor{darkred}{\textbf{0.0007}} \\
        \hline
        DA & RF & 0.9987 & 0.9670 & 0.9770 & 0.9901 & 0.8610 & \textcolor{darkred}{\textbf{0.0013}} \\
        \hline
        FtU & XGB & 0.9987 & 0.9750 & 0.9813 & 0.9942 & 0.8610 & \textcolor{darkred}{\textbf{0.0019}} \\
        \hline
        FtU & RF & 0.9987 & 0.9697 & 0.9770 & 0.9901 & 0.8610 & \textcolor{darkred}{\textbf{0.0013}} \\
        \hline
        EG & XGB & 0.9944 & 0.9779 & 0.9743 & 0.9919 & 0.8622 & \textcolor{darkred}{\textbf{0.0019}} \\
        \hline
        EG & RF & 0.9987 & 0.9697 & 0.9770 & 0.9901 & 0.8610 & \textcolor{darkred}{\textbf{0.0013}} \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & \textcolor{darkred}{\textbf{XGB}} & 0.5201 & 0.0000 & 0.0000 & 0.0000 & 0.0000 & 0.0759 \\
        \hline
        GS & RF & 0.9987 & 0.9697 & 0.9770 & 0.9901 & 0.8610 & \textcolor{darkred}{\textbf{0.0013}} \\
        \hline
    \end{tabular}
    \caption{Models training results for disparate impact metric}
    \label{tab:results_di}
\end{table}

About the results showed in \cref{tab:results_di} results it's possible to make some consideration:

\begin{itemize}

    \item For each model the metric for \emph{ESCS} is close to 0. This means that each algorithm the demographic parity is not achieved.

    \item On the other hand for every other protected attribute is close to 1, this means that for each algorithm, except the GridSearch, for these protected attributes, the demographic parity is achieved.

    \item The GridSearch algorithm with the XGB model completely fails into the achieving of the demographic parity, since the DI value is 0 for each protected attribute but \emph{ESCS}, for which it's close to 0.
    
\end{itemize}

\textbf{Fairness metric: Demographic Parity}

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|c|c|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Model} & \textbf{Acc} $\uparrow$ & \textbf{DPDs} $\downarrow$ & \textbf{DPDc} $\downarrow$ & \textbf{DPDp} $\downarrow$ & \textbf{DPDh} $\downarrow$ & \textbf{DPDe} $\downarrow$ \\
        \hline
        DA & XGB & 0.9975 & 0.0074 & 0.0026 & 0.0014 & 0.0011 & 0.0007\\
        \hline
        DA & RF & 0.9987 & 0.0068 & 0.0032 & 0.0014 & 0.0023 & 0.0013\\
        \hline
        FtU & XGB & 0.9987 & 0.0056 & 0.0026 & 0.0008 & 0.0023 & 0.0019 \\
        \hline
        FtU & RF & 0.9987 & 0.0068 & 0.0032 & 0.0014 & 0.0023 & 0.0013 \\
        \hline
        EG & XGB & 0.9944 & 0.0049 & 0.0036 & 0.0012 & 0.0023 & 0.0019 \\
        \hline
        EG & RF & 0.9987 & 0.0068 & 0.0032 & 0.0014 & 0.0023 & 0.0013 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & \textcolor{darkred}{\textbf{XGB}} & 0.5201 & 0.1204 & 0.0504 & 0.0556 & 0.0049 & 0.0759 \\
        \hline
        GS & RF & 0.9987 & 0.0068 & 0.0032 & 0.0014 & 0.0023 & 0.0013 \\
        \hline
    \end{tabular}
    \caption{Models training results for demographic parity difference metric}
    \label{tab:results_dpd}
\end{table}

About the results reported in \cref{tab:results_dpd} it's possible to make some consideration:

\begin{itemize}
        \item Every algorithm achieves a reasonable degree of group fairness.

        \item The GridSearch algorithm with the XGB model is the combination of algorithm and model that fails into the achieving of trade-off between fairness and accuracy.

\end{itemize}

For each fairness algorithm employed in the Fair-by-Design workflow, the minimum and maximum accuracy values are presented. This analysis provides insights into the range of accuracy outcomes associated with each fairness algorithm.

\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.9975 & 0.9987 \\
        \hline
        \textcolor{darkgreen}{\textbf{FtU}} & 0.9987 & 0.9987 \\
        \hline
        EG & 0.9944 & 0.9987 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.5201 & 0.9987 \\
        \hline
    \end{tabular}
    \caption{Min-Max accuracy for each fairness algorithm}
    \label{tab:accuracies}
\end{table}

There are some considerations about accuracies represented in \cref{tab:accuracies} that can be made about the accurencies presented:
\begin{itemize}

    \item The algorithm that demonstrates the maximum variance in accuracy results for both the XGB and RF models is Grid Search (GS).

    \item In contrast, the Fairness through Unawareness (FtU) algorithm exhibits the minmum variance in accuracy values across different configurations. More specifically the accuracy is the same for both models. Furthermore this algorithm has the maximum accuracy, values that it shares with other 2 algorithms (DA and EG), but in contrast with them it has the maximum minimum value. This result suggests to choose FtU as fairness algorithm to achieve the maximum accuracy.
\end{itemize}

Similarly, for each fairness algorithm, the minimum and maximum fairness values are presented. This evaluation allows for an understanding of the fairness outcomes associated with different algorithms, providing a comprehensive view of the trade-offs between accuracy and fairness.

\textbf{Disparate Impact Sex}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.9670 & 0.9670 \\
        \hline
        FtU & 0.9697 & 0.9750 \\
        \hline
        \textcolor{darkgreen}{\textbf{EG}} & 0.9697 & 0.9779 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0000 & 0.9697 \\
        \hline
    \end{tabular}
    \caption{Min-Max Disparate Impact value for \emph{Sex} protected attribute}
    \label{tab:s_di}
\end{table}

There are some considerations about the results reported in \cref{tab:s_di}:

\begin{itemize}

    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The Exponentiated Gradient (EG) is the algorithm that performs better since it has the maximum minimum and maximum value.

\end{itemize}

According to the analysis of the results it's possible to conclude that if the goal is to achieve the Demographic Parity with a high accuracy for the protected attribute \emph{Sex} in this scenario the best choice would be the Exponentiated Gradient algorithm.

% Keep going with capital island

\textbf{Disparate Impact Capital Island}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.9770 & 0.9813 \\
        \hline
        FtU & 0.9770 & 0.9813 \\
        \hline
        \textcolor{darkgreen}{\textbf{EG}} & 0.9743 & 0.9770 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0000 & 0.9770 \\
        \hline
    \end{tabular}
    \caption{Min-Max Disparate Impact value for \emph{Capital Island} protected attribute}
    \label{tab:ci_di}
\end{table}

There are some considerations about the results reported in \cref{tab:ci_di}:

\begin{itemize}

    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The Exponentiated Gradient (EG) is the algorithm that has the least variance, this means a very small sensitivity to the model used in the training step.

    \item The performacne of the pre-processing algorithms are the same

\end{itemize}

According to the analysis of the results it's possible to conclude that if the goal is to achieve the Demographic Parity with a high accuracy for the protected attribute \emph{Capital Island} in this scenario the best choice would be adopt a Pre-processing algorithm.

\textbf{Disparate Impact Private-Public}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.9901 & 0.9901 \\
        \hline
        \textcolor{darkgreen}{\textbf{FtU}} & 0.9901 & 0.9942 \\
        \hline
        EG & 0.9901 & 0.9919 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0000 & 0.9901 \\
        \hline
    \end{tabular}
    \caption{Min-Max Disparate Impact value for \emph{Private-Public} protected attribute}
    \label{tab:pp_di}
\end{table}

There are some considerations about the results reported in \cref{tab:pp_di}:

\begin{itemize}

    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The Fairness through Unawareness (FtU) is the algorithm that has the maximum value even if the variance is the greater than (EG) and (DA)

\end{itemize}

According to the analysis of the results it's possible to conclude that if the goal is to achieve the Demographic Parity with a high accuracy for the protected attribute \emph{Private-Public} in this scenario the best choice would be adopt the Exponentiated Gradient algorithm.

\textbf{Disparate Impact Household Income}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        \textcolor{darkgreen}{\textbf{DA}} & 0.8610 & 0.9340 \\
        \hline
        FtU & 0.8610 & 0.8610 \\
        \hline
        EG & 0.8610 & 0.8622 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0000 & 0.8610 \\
        \hline
    \end{tabular}
    \caption{Min-Max Disparate Impact value for \emph{Household Income} protected attribute}
    \label{tab:h_di}
\end{table}

There are some considerations about the results reported in \cref{tab:h_di}:

\begin{itemize}

    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The Fairness through Unawareness (FtU) is the algorithm that has the minimum variance, this means a very small sensitivity to the model used in the training step.

    \item The Data Augmentation is the algorithm that has the greatest maximum value.

\end{itemize}

According to the analysis of the results it's possible to conclude that if the goal is to achieve the Demographic Parity with a high accuracy for the protected attribute \emph{Household Income} in this scenario the best choice would be adopt the Data Augmentation algorithm.

\textbf{Disparate Impact ESCS}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.0007 & 0.0013 \\
        \hline
        FtU & 0.0013 & 0.0019 \\
        \hline
        EG & 0.0013 & 0.0019 \\
        \hline
        GS & 0.0013 & 0.00759 \\
        \hline
    \end{tabular}
    \caption{Min-Max Disparate Impact value for \emph{ESCS} protected attribute}
    \label{tab:e_di}
\end{table}

About the results reported in \cref{tab:e_di} it's possible to assess how the \emph{Demographic Parity} for this protected attribute is never achieved.

%Keep going with demographic parity

\textbf{Demographic Parity Difference Sex}
\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.0068 & 0.0074 \\
        \hline
        FtU & 0.0056 & 0.0068 \\
        \hline
        \textcolor{darkgreen}{\textbf{EG}} & 0.0049 & 0.0068 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0068 & 0.1204 \\
        \hline
    \end{tabular}
    \caption{Min-Max Demographic Parity Difference value for \emph{Sex} protected attribute}
    \label{tab:s_dpd}
\end{table}

There are some considerations about the results reported in \cref{tab:s_dpd}:

\begin{itemize}
    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The value of DPD for the GS algorithm adopted together with the XGB model is the highest value. Although this is a small value it suggests that this combination of model and algorithm is not the best choice to achieve the Group Fairness for the specified protected attribute.

    \item The Exponentiated Gradient (EG) algorithm is the best choice because in the best scenario it has the lowest value, and to achieve Group Fairness the goal is to have values of DPD close to 0, but in the worst scenario it has a value (0.0068) that it is not high on one hand and on the other hand it is the best scenario for other algorithms.

\end{itemize}


\textbf{Demographic Parity Difference Capital Island}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        \textcolor{darkgreen}{\textbf{DA}} & 0.0026 & 0.0032 \\
        \hline
        \textcolor{darkgreen}{\textbf{FtU}} & 0.0026 & 0.0032 \\
        \hline
        EG & 0.0032 & 0.0036 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0032 & 0.0556 \\
        \hline
    \end{tabular}
    \caption{Min-Max Demographic Parity Difference value for \emph{Capital Island} protected attribute}
    \label{tab:ci_dpd}
\end{table}

There are some considerations about the results reported in \cref{tab:ci_dpd}:

\begin{itemize}
    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The value of DPD for the GS algorithm adopted together with the XGB model is the highest value. Although this is a small value it suggests that this combination of model and algorithm is not the best choice to achieve the Group Fairness for the specified protected attribute.

    \item The best choice to ensure Group Fairness for the specified protected attribute is one of the presented pre-processing algorithms. The reason why is that these algorithms have the same performance and in their worst results they present a value (0.0032) that it is equal to the best result for the EG algorithm
\end{itemize}

\textbf{Demographic Parity Difference Public-Private}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        DA & 0.0014 & 0.0014 \\
        \hline
        \textcolor{darkgreen}{\textbf{FtU}} & 0.0008 & 0.0014 \\
        \hline
        EG & 0.0012 & 0.0014 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0014 & 0.0556 \\
        \hline
    \end{tabular}
    \caption{Min-Max Demographic Parity Difference value for \emph{Public-Private} protected attribute}
    \label{tab:pp_dpd}
\end{table}

There are some considerations about the results reported in \cref{tab:pp_dpd}:

\begin{itemize}
    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The value of DPD for the GS algorithm adopted together with the XGB model is the highest value. Although this is a small value it suggests that this combination of model and algorithm is not the best choice to achieve the Group Fairness for the specified protected attribute.

    \item The best choice to guarantee Group Fairness having at the same time a reasonable level of accuracy (99\%) is to adopt the Fairness through Unawareness (FtU) algorithm. Even if the variance is greater than the EG's one, it has the lowest value in the best scenario and in the worst it is a value that is either equal or 0.0002 beyond the best scenario of the other algorithms., 
\end{itemize}

\textbf{Demographic Parity Difference Household Income}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        \textcolor{darkgreen}{\textbf{DA}} & 0.0011 & 0.0023 \\
        \hline
        FtU & 0.0023 & 0.0023 \\
        \hline
        EG & 0.0023 & 0.0023 \\
        \hline
        GS & 0.0023 & 0.0049 \\
        \hline
    \end{tabular}
    \caption{Min-Max Demographic Parity Difference value for \emph{Household Income} protected attribute}
    \label{tab:h_dpd}
\end{table}

There are some considerations about the results reported in \cref{tab:h_dpd}:

\begin{itemize}
    \item The Exponentiated Gradient (EG) and the Fairness through Unawareness (FtU) algorithms present the same results.

    \item In contrast to the analysis for the other protected attributes the results obtained for the Grid Search (GS) algorithm are not outlier and the algorithm itself does not present a huge variance.

    \item The best choice is the Data Augmentation (DA) algorithm. The reason why is that this algorithm has the same performance and its worst result it presents a value (0.0023) that it is equal to the best result for the EG, FtU and GS algorithms.
\end{itemize}

\textbf{Demographic Parity Difference ESCS}
\begin{table}
    \centering
    \begin{tabular}{|c|c|c|}
        \hline
        \textbf{FairAlg} & \textbf{Minimum value} & \textbf{Maximum Value} \\
        \hline
        \textcolor{darkgreen}{\textbf{DA}} & 0.0007 & 0.0013 \\
        \hline
        FtU & 0.0013 & 0.0019 \\
        \hline
        EG & 0.0013 & 0.0019 \\
        \hline
        \textcolor{darkred}{\textbf{GS}} & 0.0013 & 0.0759 \\
        \hline
    \end{tabular}
    \caption{Min-Max Demographic Parity Difference value for \emph{ESCS} protected attribute}
    \label{tab:e_dpd}
\end{table}

There are some considerations about the results reported in \cref{tab:e_dpd}:

\begin{itemize}

    \item The huge variance for the Grid Search (GS) algorithm suggests that it's great sensitivity to the model used in the training step. Even if the models are similar. In this case XGB and RF have completely different behaviors.

    \item The Exponentiated Gradient (EG) and the Fairness through Unawareness (FtU) algorithms present the same results.

    \item The best choice is the Data Augmentation (DA) algorithm. The reason why is that this algorithm has the same performance and its worst result it presents a value (0.0013) that it is equal to the best result for the EG, FtU and GS algorithms.
\end{itemize}