\chapter{Validation}
\label{chap:validation}

\section{Introduction}

This chapter unfolds as a meticulous exploration into the performance and reliability of the Fair-by-Design workflow. To conduct a thorough assessment, the well known dataset Adult has been strategically chosen as the testing ground. The dataset, rich in socioeconomic and demographic attributes, provides a robust platform for evaluating the workflow's effectiveness across various machine learning algorithms.

The central emphasis of this validation endeavor is to ascertain not only the accuracy and predictive capabilities of the implemented models but also the extent to which the Fair-by-Design workflow succeeds in fostering fairness. The Adult dataset, carefully selected for its representativeness and complexity, allows for a nuanced examination of how different algorithms respond to the intricacies of real-world data, shedding light on the interplay between accuracy and fairness.

Throughout this chapter, the validation process unfolds systematically, encompassing diverse algorithms applied to the Adult dataset. The goal is to detect both the strengths and potential limitations of the Fair-by-Design approach, providing valuable insights for its application in scenarios where the equitable treatment of individuals and the accuracy of predictions are of paramount importance. The subsequent sections detail the experimental setup, algorithmic choices, and the rigorous evaluation metrics employed to ensure a comprehensive understanding of the Fair-by-Design workflow's performance on the chosen dataset.

\section{Objective definition}
\label{section:val_obj}

In this section, we precisely outline the objectives guiding the Fair-by-Design workflow, focusing on effective income prediction while addressing fairness and mitigating biases. The key components include the identification of protected attributes, selection of fairness notions, and corresponding fairness metrics.

\subsection{Prediction Objective}

\begin{itemize}
    \item Goal: Classify in a fairness way if an individual earns more or less than 50,000\$ / year.
\end{itemize}

\subsection{Protected Attributes}

\begin{itemize}
    \item Attributes: "Ethnicity" and "Sex"
\end{itemize}

\subsection{Fairness Notions}

\begin{itemize}
    \item Demographic Parity:
    \begin{itemize}
        \item Objective: Ensure consistent distribution of predicted outcomes across different subgroups based on protected attributes.
    \end{itemize}
    
    \item Group Fairness:
    \begin{itemize}
        \item Objective: Guarantee equitable predictions within specific subgroups defined by protected attributes.
    \end{itemize}
\end{itemize}

\subsection{Fairness Metrics}

\begin{itemize}
    \item For Demographic Parity:
    \begin{itemize}
        \item Metric: Disparate Impact
    \end{itemize}
    
    \item For Group Fairness:
    \begin{itemize}
        \item Metric: Demographic Parity Difference
    \end{itemize}
\end{itemize}

These succinctly defined objectives, paired with chosen fairness notions and metrics, lay the groundwork for the Fair-by-Design workflow, ensuring a focused approach toward accurate predictions while fostering fairness across diverse subgroups.

\section{Data collection}
\label{section:val_dc}

The dataset is already divided into two sets:
\begin{itemize}
    \item \emph{Training set}: 32,561 items
    \item \emph{Test set}: 16,281 items
\end{itemize}

There are 14 independent features of different types and one dependent variable. Each record represents one individual and the output represents if the individual earns more or less than 50,000\$ on year basis.

The two protected attributes detected in \cref{section:val_obj} are both discrete ones, while a consideration useful for the next steps is related to the output variable. This attribute is a binary one, but it is represented as a string. So it's important to ensure that the encoding in both training and test set are the same.

\section{Data pre-processing}

Starting from the information provided by \cref{section:val_dc} in this section all the variables represend as string as been labeled the same manner. This led to a consistent dataset to be used in the models training.

\section{Algorithm design}

In the context of the Fair-by-Design workflow, six key fairness algorithms have been strategically chosen to comprehensively test the workflow's effectiveness. Each algorithm serves a specific purpose within the workflow, contributing to the overarching goal of achieving fairness in predictions.

\subsection{Pre-processing Algorithms}

\subsubsection{Data Augmentation Algorithm}

\textbf{Algorithm Description:}

The proposed data augmentation algorithm introduces synthetic samples to the training dataset. By augmenting the data, especially focusing on underrepresented groups, the algorithm aims to enhance the model's exposure to diverse scenarios, promoting a more robust understanding of the underlying patterns.

\textbf{Reasoning:}

Data augmentation is crucial for addressing imbalances in the training data, allowing the model to generalize better across different groups and mitigating biases stemming from insufficient representation.

\subsubsection{Reweighing Algorithm}

\textbf{Algorithm Description:}

The reweighing algorithm adjusts sample weights based on sensitive attributes, aiming to equalize the impact of different groups in the training process. It acts as a pre-processing step to ensure fair model training.

\textbf{Reasoning:}

Reweighing is fundamental for treating different groups fairly during model training. By assigning appropriate weights, the algorithm helps counteract the biases present in the training data.

\subsection{In-processing Algorithms}

\subsubsection{ExponentiatedGradient}

\textbf{Algorithm Description:}

The ExponentiatedGradient algorithm applies iterative re-weighting to the training data, seeking to find a fair classifier by minimizing the disparity in predictions across sensitive groups.

\textbf{Reasoning:}

ExponentiatedGradient is chosen for its versatility and effectiveness in in-processing fairness. It provides a fine-tuning mechanism during training to achieve parity in predicted outcomes.

\subsubsection{LFR (Learning Fair Representations)}

\textbf{Algorithm Description:}

LFR focuses on learning fair representations of the data by mapping it to a latent space. It aims to encode features in a way that separates the influence of sensitive attributes from the overall prediction.

\textbf{Reasoning:}

LFR is selected for its ability to disentangle predictive features from sensitive attributes, allowing for fairer representations and reducing the risk of biased predictions.

\subsection{Post-processing Algorithms}

\subsubsection{MetaFairClassifier}

\textbf{Algorithm Description:}

MetaFairClassifier acts as both a post-processing and in-processing meta-algorithm. It modifies a base classifier to achieve fairness by adjusting decision boundaries and re-weighting samples.

\textbf{Reasoning:}

MetaFairClassifier is chosen to assess the impact of modifying a base classifier in both post-processing and in-processing scenarios, offering insights into the versatility of the approach.

\subsubsection{ThresholdOptimizer}

\textbf{Algorithm Description:}

ThresholdOptimizer focuses on adjusting decision thresholds post-modeling to achieve fairness goals. It provides flexibility in fine-tuning predictions without retraining the entire model.

\textbf{Reasoning:}

ThresholdOptimizer is included to evaluate the effectiveness of post-processing adjustments in achieving fairness objectives. It offers a practical and interpretable way to balance accuracy and fairness.

The choice of these six algorithms is driven by the aim of obtaining a holistic view of the Fair-by-Design workflow's performance. By incorporating diverse pre-processing, in-processing, and post-processing strategies, the evaluation can capture the nuances of fairness considerations at different stages of the machine learning pipeline. This comprehensive approach ensures a thorough exploration of potential biases and fairness enhancements, laying the foundation for a robust and equitable model.

