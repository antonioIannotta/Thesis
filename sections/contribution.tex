%----------------------------------------------------------------------------------------
\chapter{Contribution} % possible chapter for Projects
\label{chap:contribution}

\section{Introduction to the Fair-by-Design Workflow}
\label{section:workflow-introduction}

In recent years, heightened scrutiny of the ethical implications associated with artificial intelligence (AI) and machine learning (ML) systems has arisen in response to the escalating influence of these technologies, particularly in domains where consequential decisions profoundly impact individuals' lives. The evolving landscape of AI ethics necessitates a conscientious examination of the ethical dimensions embedded in the development and deployment of these systems.

Amid this multifaceted ethical discourse, the imperative of integrating fairness into the fabric of AI design has emerged as a paramount consideration. The principle of fairness, when applied as a foundational element in the design process, underscores the need for AI systems to produce outcomes that are not only accurate and efficient but also just and equitable. Achieving fairness in AI development demands a comprehensive and deliberate approach, one that transcends mere post hoc considerations by embedding fairness principles from the very inception of the design process.

The conceptual framework of a Fair-by-Design (FBD) workflow encapsulates a meticulous set of principles and procedural steps, each strategically devised to ensure the attainment of equitable and unbiased outcomes throughout the entire lifecycle of an AI system. This includes not only the initial design and development phases but also extends to the implementation, deployment, and ongoing monitoring stages. The iterative nature of this workflow acknowledges the dynamic interplay between ethical considerations and technological advancements, emphasizing the continuous reassessment and refinement of fairness strategies.

By adopting a Fair-by-Design approach, stakeholders in AI development, ranging from researchers and engineers to policymakers and end-users, commit to a shared responsibility for cultivating a technological landscape where the ethical imperatives of fairness and equity are not afterthoughts but integral components steering the trajectory of AI systems towards societal benefit and justice. In essence, the pursuit of ethical AI underscores not only the advancement of technological capabilities but also the conscientious stewardship of these capabilities to ensure they align with ethical norms and societal values.


\subsection{Principles of Fair-by-Design Workflow}
\label{subsection:workflow-principles}

\begin{enumerate}[label=\arabic*.]
    \item \emph{Proactive Fairness Integration:}
    
    \begin{itemize}
       
        \item The cornerstone of the Fair-by-Design workflow lies in the proactive embedding of fairness considerations at the genesis of the design process, diverging from conventional practices where fairness is often retroactively addressed after system implementation.
        
        \item This proactive approach marks a transformative shift in the landscape of algorithmic development, underscoring the ethical imperative of forethought in anticipating and mitigating biases.
        
        \item Seamless integration of fairness into the initial design stages aims to forestall the emergence of discriminatory outcomes, establishing a bedrock rooted in equitable principles.
        
        \item Beyond its alignment with ethical standards, this proactive integration streamlines the developmental trajectory, fostering a more responsible and inclusive machine learning environment.
       
        \item This meticulous attention to fairness from the inception reflects a commitment to ethical AI practices that transcend mere compliance, embodying a conscientious endeavor to uphold societal values and ensure the equitable treatment of diverse individuals impacted by AI systems.
   
    \end{itemize}

    \item \emph{Transparency and Explainability:}
    
    \begin{itemize}
        
        \item The Fair-by-Design workflow places a premium on transparent documentation of design decisions and algorithmic choices, considering it a fundamental pillar of its ethical framework.
       
        \item Meticulous documentation serves as a deliberate and strategic endeavor to enhance accountability and foster trust among stakeholders.
       
        \item Each decision, ranging from the selection of specific algorithms to the fine-tuning of parameters, is comprehensively recorded, creating a clear and accessible trail of the workflow's development trajectory.
       
        \item This commitment to transparency is deeply rooted in the conviction that open communication of design rationales and choices cultivates a sense of reliability and confidence among stakeholders.
       
        \item By actively promoting transparency, the Fair-by-Design workflow contributes to the creation of a trustworthy and ethically grounded landscape for the deployment of machine learning systems.
       
        \item The deliberate and open documentation not only satisfies ethical considerations but also facilitates a more robust understanding of the system's inner workings, empowering stakeholders to engage meaningfully in the ongoing dialogue surrounding the ethical dimensions of AI technologies.
    
    \end{itemize}

    \item \emph{User-Centered Approach:}
   
    \begin{itemize}
      
        \item Within the Fair-by-Design workflow, the integration of diverse perspectives transcends a passive consideration; it stands as a proactive and integral aspect of the design process.
      
        \item The voices and perspectives of end-users and relevant stakeholders are not only actively sought but thoughtfully incorporated from the early stages of system design.
      
        \item This inclusive approach is designed to ensure that the developed system is finely tuned to the diverse needs, expectations, and concerns of its user base.
     
        \item Stakeholder engagement takes on various forms, ranging from surveys and interviews to collaborative workshops.
      
        \item These mechanisms allow for a comprehensive understanding of the social, cultural, and ethical dimensions that may influence system usage.
       
        \item Actively involving end-users and stakeholders in the design process serves a dual purpose: promoting inclusivity and enhancing the likelihood of developing a system that genuinely serves and respects the interests of its users.
       
        \item This user-centered approach is not merely a procedural step but a foundational commitment to creating AI systems that align with the values and requirements of the communities they impact.
    
    \end{itemize}

    \item \emph{Continuous Monitoring and Iterative Development:}
    
    \begin{itemize}
     
        \item Beyond the initial implementation, the Fair-by-Design system adopts a vigilant and continuous monitoring regimen, constituting a pivotal component of its iterative workflow.
     
        \item This meticulous monitoring process is crafted to detect and address emerging fairness issues that may manifest during system operation.
     
        \item Leveraging advanced monitoring tools and techniques, the workflow ensures that the system's performance is regularly assessed in real-world scenarios.
      
        \item This proactive approach enables the timely identification of potential biases or disparities, allowing for the swift implementation of corrective measures.
     
        \item The iterative nature of the workflow is a testament to its adaptability, facilitating ongoing refinements to ensure that the system evolves in response to changing dynamics and user experiences.
     
        \item This commitment to continuous monitoring and improvement underscores the Fair-by-Design philosophy: an emphasis not only on achieving fairness at a single point in time but on actively maintaining and enhancing fairness throughout the system's entire lifecycle.
      
        \item By embracing a dynamic and iterative model, the Fair-by-Design workflow remains resilient, ensuring that it remains responsive to the evolving landscape of ethical considerations and technological advancements in artificial intelligence.
   
    \end{itemize}

\end{enumerate}

\section{Steps to Implement a Fair-by-Design Workflow}
\label{section:steps}

\begin{enumerate}

    \item \emph{Objective Definition:} Clearly articulate the objectives of the system or process being designed, emphasizing the importance of fairness.

    \item \emph{Data Collection:} Determine the types of data needed for the application, establish ethical data collection protocols.

    \item \emph{Data pre-processing:} Prepare the data for the fairness algorithm, through data cleaning and featyre engineering
    
    \item \emph{Algorithmic Design and Definitions:} Define and implement fairness-enhancing algorithms across pre-processing, in-processing, and post-processing stages. Provide detailed definitions and explanations for each algorithm.

    \item \emph{Model training and evaluation:} Define the training step, performs the training tuning certain parameters and provides an evaluation of the performances of the models based on accuracy and fairness metrics.

    \item \emph{Model deployment:} Define the change of the environment of the model from a development environment to real world application environment.

\end{enumerate}

\textbf{********************ADD FIGURE HERE***************************+}

\subsection{Objective Definition}
\label{subsection:objective}

The first critical step in the Fair-by-Design workflow involves a precise definition of the objectives, laying the groundwork for subsequent fairness considerations. This stage encompasses the identification of protected attributes, the selection of fairness notions, and the choice of an appropriate fairness metric.

\subsubsection{Protected Attributes}

Protected attributes refer to the characteristics of individuals or groups that are explicitly considered to prevent discrimination and bias. The criteria for choosing specific protected attributes must be well-defined to ensure the relevance of fairness considerations. This may involve:

\begin{itemize}

    \item Conducting a comprehensive literature review to identify historically disadvantaged or underrepresented groups.
    
    \item Collaborating with domain experts, ethicists, and stakeholders to determine attributes with potential societal impact.
    
    \item Assessing the legal and ethical implications of including or excluding certain attributes within the specific context of the AI application.

\end{itemize}

The selected protected attributes form the basis for subsequent fairness evaluations and interventions.

\subsubsection{Fairness Notions}

Fairness notions represent the abstract principles guiding the desired equitable behavior within the AI system. The choice of fairness notions is a pivotal decision, influencing subsequent stages of the workflow. Common fairness notions, as explained in detail in \cref{section:fairness_notions}, include:

\begin{itemize}
  
    \item \textbf{Demographic Parity:} Ensuring equal representation of protected groups across different outcomes.
  
    \item \textbf{Equalized Odds:} Balancing the true positive and false positive rates across protected groups.
  
    \item \textbf{Individual Fairness:} Treating similar individuals similarly, regardless of their protected attributes.
   
    \item \textbf{Group Fairness:} Ensuring fair treatment for entire groups, typically based on statistical measures.

\end{itemize}

The selection of a fairness notion should align with the specific goals of the AI application, ethical considerations, and legal requirements.

\subsubsection{Fairness Metric}

A fairness metric quantifies the level of fairness achieved by the AI system with respect to the chosen fairness notion. The choice of a fairness metric depends on the specific fairness notion and the underlying characteristics of the data. Common fairness metrics include:

\begin{itemize}
   
    \item \textbf{Disparate Impact:} Measures the ratio of favorable outcomes between protected and unprotected groups.
   
    \item \textbf{Equalized Odds Difference:} Quantifies the disparity in error rates between protected groups.
   
    \item \textbf{Theil Index:} Evaluates the distribution of outcomes across different demographic groups.
   
    \item \textbf{Statistical Parity Difference:} Measures the difference in favorable outcomes between protected and unprotected groups.

\end{itemize}

\subsubsection{Fairness Notions and Metric Alignment}

The alignment between fairness notions and fairness metrics is a critical aspect of the Fair-by-Design workflow, as it establishes a systematic and measurable framework for evaluating fairness. The following considerations guide the selection process:

\paragraph{Demographic Parity:}

For the fairness notion of Demographic Parity, the primary concern is ensuring equal representation of protected groups across different outcomes. This notion is often aligned with the fairness metric known as \textbf{Disparate Impact}, which quantifies the ratio of favorable outcomes between protected and unprotected groups. Specifically, the Disparate Impact metric is calculated as:

\[
\text{Disparate Impact} = \frac{\text{Favorable Outcome Rate for Protected Group}}{\text{Favorable Outcome Rate for Unprotected Group}}
\]

A Disparate Impact value close to 1 indicates a balanced representation, while values significantly deviating from 1 may suggest unfairness.

\paragraph{Equalized Odds:}

The Equalized Odds fairness notion aims to balance the true positive and false positive rates across protected groups. This aligns with the \textbf{Equalized Odds Difference} fairness metric, which quantifies the disparity in error rates between protected groups. The Equalized Odds Difference is computed as:

\[
\text{Equalized Odds Difference} = \text{True Positive Rate}_{\text{Protected}} - \text{True Positive Rate}_{\text{Unprotected}}
\]

A value of 0 indicates perfect balance, with positive or negative values signifying imbalances in the error rates.

\paragraph{Individual Fairness:}

Individual Fairness focuses on treating similar individuals similarly, regardless of their protected attributes. This notion is inherently challenging to quantify, but proxy metrics such as \textbf{Theil Index} may be employed. The Theil Index evaluates the distribution of outcomes across different demographic groups and is given by:

\[
\text{Theil Index} = \sum \left( \frac{X_{i}}{\mu} \ln \frac{X_{i}}{\mu} \right)
\]

where \(X_{i}\) represents the outcome distribution for a specific group, and \(\mu\) is the overall average outcome. A lower Theil Index indicates a more equitable distribution.

\paragraph{Group Fairness:}

Group Fairness ensures fair treatment for entire groups, typically based on statistical measures. The \textbf{Statistical Parity Difference} is a common metric aligned with this notion. It measures the difference in favorable outcomes between protected and unprotected groups and is computed as:

\[
\text{Statistical Parity Difference} = \text{Favorable Outcome Rate}_{\text{Protected}} - \text{Favorable Outcome Rate}_{\text{Unprotected}}
\]

A value of 0 suggests parity, while positive or negative values indicate disparities.

\subsubsection{Choosing Fairness Metrics Given Notions or Vice Versa}

Choosing a fairness metric given a fairness notion, or vice versa, requires a nuanced understanding of the specific fairness goals and the characteristics of the data. Here's a step-by-step guide:

\paragraph{Given a Fairness Notion:}

\begin{enumerate}
 
    \item \textbf{Define Fairness Goals:} Clearly articulate the fairness goals, considering the desired equitable outcomes for protected groups.
 
    \item \textbf{Identify Relevant Fairness Notion:} Select the fairness notion that aligns most closely with the defined fairness goals (e.g., Demographic Parity, Equalized Odds, Individual Fairness, or Group Fairness).
 
    \item \textbf{Choose Aligned Fairness Metric:} Based on the selected fairness notion, choose the fairness metric that provides a quantitative measure of the specified fairness goals (e.g., Disparate Impact, Equalized Odds Difference, Theil Index, or Statistical Parity Difference).

\end{enumerate}

\paragraph{Given a Fairness Metric:}

\begin{enumerate}

    \item \textbf{Understand Fairness Metric:} Gain a deep understanding of the chosen fairness metric, considering its strengths, limitations, and relevance to specific fairness goals.

    \item \textbf{Determine Fairness Notion Alignment:} Identify the fairness notion that aligns with the underlying principles of the chosen fairness metric (e.g., Disparate Impact aligns with Demographic Parity).
 
    \item \textbf{Ensure Consistency:} Confirm that the selected fairness metric is consistent with the broader fairness goals and ethical considerations of the AI application.

\end{enumerate}

This meticulous alignment process ensures that the chosen fairness metrics quantitatively capture the intended fairness notions, providing a meaningful and interpretable basis for evaluating and comparing fairness across different models and interventions within the Fair-by-Design workflow.

In summary, the objective definition stage of the Fair-by-Design workflow involves a meticulous and well-informed selection of protected attributes, fairness notions, and fairness metrics. This precision ensures the subsequent stages of the workflow are grounded in a robust foundation for addressing bias and promoting fairness in AI systems.

\subsection{Data Collection}
\label{subsection:data_collection}

The data collection stage is fundamental for the Fair-by-Design workflow. It involves systematically gathering data to train machine learning models, coupled with a detailed study of the dataset's statistical properties, with a specific focus on protected attributes.

\subsubsection{Dataset Overview}

\paragraph{Data Representation:}
\begin{itemize}
    \item Define features and their types (categorical, numerical).
    \item Specify the structure of the data, especially in the context of demographic information.
\end{itemize}

\paragraph{Data Sources:}
\begin{itemize}
    \item Transparently describe the origin of the data (surveys, administrative records, etc.).
    \item Indicate methods used for data collection, emphasizing potential biases introduced.
\end{itemize}

\subsubsection{Study of Statistical Properties}

\paragraph{Descriptive Statistics:}
\begin{itemize}
    \item Present essential statistics: mean, median, standard deviation, quartiles (for numerical features).
    \item Provide frequency distributions (for categorical features).
\end{itemize}

\paragraph{Exploration of Protected Attributes:}
\begin{itemize}
    \item Calculate distributional metrics: proportions, counts, entropy.
    \item Focus on understanding the representation of different groups within protected attributes.
\end{itemize}

\paragraph{Correlation Analysis:}
\begin{itemize}
    \item Explore correlations between features, especially with protected attributes.
    \item Utilize correlation matrices and scatter plots for visual assessment.
\end{itemize}

\paragraph{Outlier Detection:}
\begin{itemize}
    \item Employ statistical methods or visualizations to detect and analyze outliers.
    \item Highlight the potential impact of outliers on model training and fairness considerations.
\end{itemize}

\subsubsection{Outputs for Data Pre-processing}

\paragraph{Feature Engineering Possibilities:}
\begin{itemize}
    \item Leverage insights from the dataset overview for feature engineering opportunities.
    \item Consider transformations or combinations of features to enhance model performance and fairness.
\end{itemize}

\paragraph{Bias Identification:}
\begin{itemize}
    \item Use statistical properties, especially focused on protected attributes, to identify potential biases.
    \item Understand the distribution of groups within protected attributes for addressing fairness concerns.
\end{itemize}

\paragraph{Data Cleaning Strategies:}
\begin{itemize}
    \item Formulate strategies based on outlier detection and correlation analysis.
    \item Decide on the treatment of outliers and correlations, impacting overall model quality and fairness.
\end{itemize}

\paragraph{Fairness-Aware Sampling Techniques:}
\begin{itemize}
    \item If disparities exist in the representation of different groups, consider fairness-aware sampling.
    \item Aim to address imbalances during data pre-processing for equitable model training.
\end{itemize}

In summary, the data collection stage provides the necessary foundation for the Fair-by-Design workflow. It includes a comprehensive understanding of the dataset's statistical properties and guides informed data pre-processing decisions, crucial for developing fair and unbiased machine learning models.

