%----------------------------------------------------------------------------------------
\chapter{Contribution} % possible chapter for Projects
\label{chap:contribution}

\section{Introduction to the Fair-by-Design Workflow}
\label{section:workflow-introduction}

In recent years, heightened scrutiny has been directed towards the ethical implications associated with the proliferation of artificial intelligence (AI) and machine learning (ML) systems. The escalating influence of these technologies, particularly in domains where consequential decisions profoundly impact individuals' lives, underscores the critical need for a conscientious examination of the ethical dimensions inherent in the development and deployment of AI systems.

\section{Fairness in AI Design}

Amidst this multifaceted ethical discourse, the integration of fairness into the very fabric of AI design has emerged as a paramount consideration. The principle of fairness, when applied as a foundational element in the design process, emphasizes the necessity for AI systems to produce outcomes that extend beyond accuracy and efficiency, incorporating notions of justice and equity. Achieving fairness in AI development necessitates a comprehensive and deliberate approach that goes beyond post hoc considerations, instead embedding fairness principles from the inception of the design process.

\section{Fair-by-Design (FBD) Workflow}

The conceptual framework of a Fair-by-Design (FBD) workflow represents a meticulous set of principles and procedural steps strategically devised to ensure the attainment of equitable and unbiased outcomes throughout the entire lifecycle of an AI system. This comprehensive approach encompasses not only the initial design and development phases but also extends to the implementation, deployment, and ongoing monitoring stages. The iterative nature of this workflow acknowledges the dynamic interplay between ethical considerations and technological advancements, emphasizing the continuous reassessment and refinement of fairness strategies.

\section{Adopting a Fair-by-Design Approach}

Adopting a Fair-by-Design approach implies a shared responsibility among stakeholders in AI development, ranging from researchers and engineers to policymakers and end-users. This shared commitment aims to cultivate a technological landscape where the ethical imperatives of fairness and equity are integral components, guiding the trajectory of AI systems towards societal benefit and justice. In essence, the pursuit of ethical AI underscores not only the advancement of technological capabilities but also the conscientious stewardship of these capabilities to ensure alignment with ethical norms and societal values.

\section{Fairness and Ethics in AI: Preparing the Ground}

Before immersing ourselves in the intricate details of the Fair-by-Design (FBD) workflow, it is crucial to lay the ethical foundation upon which this comprehensive approach to AI development stands. In this section, we explore specific aspects of AI design, development, and deployment with a keen focus on fairness and ethical considerations.

\subsection{Ethical Scrutiny of AI Influence}

In this first facet, we critically examine the domains where AI and ML technologies wield significant influence over decision-making. By evaluating the ethical considerations stemming from these technologies' consequential impact on individuals' lives, we gain insights into the nuanced challenges and ethical implications associated with AI influence. Additionally, real-world examples and recent cases provide tangible illustrations of the ethical dimensions that arise in various sectors.

\subsection{Foundational Role of Fairness in AI Design}

The second aspect delves into the foundational role of fairness in the AI design process. This nuanced discussion explores fairness as a fundamental element, extending beyond accuracy and efficiency considerations. By emphasizing justice and equity, we illustrate the importance of incorporating fairness considerations in the design phase through relevant examples, setting the stage for a more ethical AI development process.

By delving into these specific areas, a more detailed and schematic understanding of the ethical considerations surrounding AI can be offered, making the discourse both comprehensive and informative.

\subsection{Principles of Fair-by-Design Workflow}
\label{subsection:workflow-principles}

\begin{enumerate}[label=\arabic*.]
    \item \emph{Proactive Fairness Integration:}
    
    \begin{itemize}
       
        \item The foundational principle of the Fair-by-Design (FBD) workflow lies in the deliberate and proactive incorporation of fairness considerations at the inception of the design process. This departure from conventional practices, where fairness is often addressed reactively post-implementation, marks a significant paradigm shift in algorithmic development ethics.
    
        \item This proactive approach signifies a transformative evolution in the landscape of algorithmic development, emphasizing the ethical imperative of anticipatory forethought to identify and mitigate biases. By embracing a proactive stance, ethical AI practitioners aim to preclude the emergence of discriminatory outcomes, establishing a robust foundation firmly grounded in equitable principles.
    
        \item The seamless integration of fairness considerations into the initial stages of the design process serves a dual purpose. Primarily, it aligns with rigorous ethical standards, demonstrating a commitment to responsible AI development. Simultaneously, this integration functions as a preemptive measure to forestall the propagation of biases, fostering a more responsible, inclusive, and ethically grounded machine learning environment.
    
        \item Beyond its ethical alignment, the proactive integration of fairness into the early design phases contributes to the optimization of the developmental trajectory. By addressing potential biases at their roots, this meticulous attention not only aligns with ethical norms but also streamlines the overall development process, reducing the need for extensive retroactive corrections.
    
        \item The meticulous attention to fairness considerations from the very inception of the design process reflects a deep-seated commitment to ethical AI practices. This commitment extends beyond mere compliance, embodying a conscientious endeavor to uphold societal values and ensure the equitable treatment of diverse individuals impacted by AI systems. It underscores a dedication to fostering an environment where the benefits of AI are distributed justly and without reinforcing societal disparities.
       
    \end{itemize}

    \item \emph{Transparency and Explainability:}
    
    \begin{itemize}
        
        \item Within the Fair-by-Design (FBD) workflow, paramount importance is attributed to the meticulous and transparent documentation of design decisions and algorithmic choices, establishing this practice as a foundational and non-negotiable element of its overarching ethical framework.
    
        \item The commitment to meticulous documentation serves as a deliberate and strategic endeavor to fortify accountability and cultivate trust among diverse stakeholders involved in the AI development process. This commitment recognizes that transparency is not merely a procedural requirement but a fundamental pillar essential for ethical AI practices.
    
        \item Comprehensive documentation encapsulates every decision-making juncture within the workflow, ranging from the careful selection of specific algorithms to the nuanced fine-tuning of parameters. This practice ensures the creation of a lucid and accessible trail that meticulously outlines the developmental trajectory of the FBD workflow.
    
        \item The underlying philosophy driving this commitment to transparency is deeply rooted in the conviction that open communication of design rationales and choices plays a pivotal role in cultivating a sense of reliability and confidence among stakeholders. This strategic transparency is envisioned as a means to foster informed participation and engender a shared understanding of the ethical considerations embedded in AI system development.
    
        \item Actively promoting transparency within the Fair-by-Design framework contributes to the establishment of a trustworthy and ethically grounded landscape for the deployment of machine learning systems. This emphasis recognizes that transparency is not only an ethical imperative but also an instrumental factor in shaping perceptions and attitudes towards AI technologies.
    
        \item The deliberate and open documentation practices pursued within the Fair-by-Design workflow extend beyond ethical compliance. They serve as a mechanism for facilitating a more robust comprehension of the system's inner workings, empowering stakeholders to engage meaningfully in the ongoing dialogue surrounding the multifaceted ethical dimensions of AI technologies. This depth of understanding, facilitated by transparent documentation, enables stakeholders to make informed contributions and ensures that ethical considerations remain a focal point throughout the system's lifecycle.
    
    \end{itemize}

    \item \emph{User-Centered Approach:}
   
    \begin{itemize}
      
        \item Within the sophisticated framework of the Fair-by-Design (FBD) workflow, the integration of diverse perspectives is not relegated to a passive consideration; it is established as a proactive and integral aspect woven into the very fabric of the design process.
    
        \item The deliberate incorporation of the voices and perspectives of end-users and relevant stakeholders transcends a mere token gesture; it is a thoughtful and early-stage initiative in the system design process. This proactive engagement strategy is pivotal in ensuring that the developed system is finely tuned to the diverse needs, expectations, and concerns of its intended user base.
    
        \item This inclusive approach is meticulously designed to harness the richness of perspectives, cultivating a system that is not only technologically advanced but also deeply attuned to the social and cultural nuances of its user community.
    
        \item Stakeholder engagement, as envisaged within the FBD workflow, takes on a myriad of forms, ranging from structured surveys and insightful interviews to collaborative workshops designed to foster dynamic and participatory exchanges.
    
        \item These mechanisms, carefully implemented, facilitate a comprehensive understanding of the intricate social, cultural, and ethical dimensions that may profoundly influence the usage and impact of the developed system.
    
        \item The active involvement of end-users and stakeholders in the design process serves a dual purpose: first, it promotes inclusivity by elevating the importance of diverse perspectives, and second, it significantly enhances the likelihood of developing a system that not only meets but genuinely respects the interests and values of its users.
    
        \item This user-centered approach is not a mere procedural step within the FBD workflow; rather, it stands as a foundational commitment, reflecting the overarching goal of creating AI systems that seamlessly align with the values, expectations, and requirements of the diverse communities they impact.
    
    \end{itemize}

    \item \emph{Continuous Monitoring and Iterative Development:}
    
    \begin{itemize}
     
        \item Beyond the initial implementation phase, the Fair-by-Design system manifests a vigilant and continuous monitoring regimen, constituting an indispensable and pivotal component meticulously integrated into its iterative workflow.
    
        \item The meticulous monitoring process is meticulously crafted to transcend mere oversight, actively seeking to detect and address any emergent fairness issues that may manifest during the operational lifespan of the system.
    
        \item Leveraging advanced monitoring tools and sophisticated techniques, the workflow ensures a regular and systematic assessment of the system's performance in real-world scenarios. This proactive approach is designed to facilitate the timely identification of potential biases or disparities, thereby allowing for the swift implementation of corrective measures.
    
        \item The intrinsic adaptability of the workflow, grounded in its iterative nature, serves as a testament to its commitment to ongoing refinement. This adaptability ensures that the system evolves dynamically in response to changing dynamics, user experiences, and emerging ethical considerations.
    
        \item The steadfast commitment to continuous monitoring and improvement within the Fair-by-Design philosophy goes beyond a mere procedural requirement. It embodies a profound emphasis not only on achieving fairness at a singular point in time but on actively maintaining and enhancing fairness throughout the entire lifecycle of the system.
    
        \item By embracing a dynamic and iterative model, the Fair-by-Design workflow stands as a paragon of resilience, ensuring that it remains not only responsive but anticipatory, capable of navigating the evolving landscape of ethical considerations and technological advancements in artificial intelligence.
    
    \end{itemize}

\end{enumerate}

\section{Steps to Implement a Fair-by-Design Workflow}
\label{section:steps}

The sequence of steps needed to implement a Fair-by-Design Workflow, as showed in \cref{fig:steps}, is:

\begin{enumerate}

    \item \emph{Objective Definition:} Clearly articulate the objectives of the system or process being designed, emphasizing the importance of fairness.

    \item \emph{Data Collection:} Determine the types of data needed for the application, establish ethical data collection protocols.

    \item \emph{Data pre-processing:} Prepare the data for the fairness algorithm, through data cleaning and featyre engineering
    
    \item \emph{Algorithmic Design and Definitions:} Define and implement fairness-enhancing algorithms across pre-processing, in-processing, and post-processing stages. Provide detailed definitions and explanations for each algorithm.

    \item \emph{Model training and evaluation:} Define the training step, performs the training tuning certain parameters and provides an evaluation of the performances of the models based on accuracy and fairness metrics.

\end{enumerate}

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (start) [startstop] {Objective Definition};
        \node (dc) [startstop, below of=start] {Data Collection};
        \node (dpr) [startstop, below of=dc] {Data Pre-processing};
        \node (mtpe) [startstop, below of=dpr] {Model traning and Performance evaluation};
        \draw [arrow] (start) -- (dc);
        \draw [arrow] (dc) -- (dpr);
        \draw [arrow] (dpr) -- (mtpe);
    \end{tikzpicture}
    \caption{Fair-by-Design steps}
    \label{fig:steps}
\end{figure}
    

\subsection{Objective Definition}
\label{subsection:objective}

The first critical step in the Fair-by-Design workflow involves a precise definition of the objectives, laying the groundwork for subsequent fairness considerations. This stage encompasses the identification of the goal of the study, the identification of protected attributes, the selection of fairness notions, and the choice of an appropriate fairness metric.

\subsubsection{Objective of the Research}

At the outset of the Fair-by-Design workflow, a foundational consideration is the precise identification of the research objective. This initial step is pivotal in determining the overarching goal of the study, laying the groundwork for a comprehensive understanding of whether the research necessitates a classification approach or a regression model.

In this context, the elucidation of the research goal involves a detailed examination of the problem domain, aiming to discern the nature of the outcomes sought. This discernment is crucial not only for formulating a clear research question but also for guiding subsequent decisions related to the choice of methodologies, algorithms, and evaluation metrics.

The intricacies of the research objective are examined to ascertain whether the study primarily involves categorizing data into distinct classes, indicating a classification task, or predicting numerical values, signifying a regression-oriented inquiry. This nuanced exploration is essential for aligning the research approach with the inherent characteristics of the problem at hand.

By fostering a meticulous understanding of the research objective at this early stage, the Fair-by-Design workflow ensures a deliberate and well-informed trajectory, setting the stage for the subsequent phases of the research process.

\subsubsection{Identification and Definition of Protected Attributes}

The identification and definition of protected attributes within the Fair-by-Design workflow involve a systematic process, as outlined below:

\begin{itemize}
    \item \emph{Literature Review:} Conduct a comprehensive literature review to identify historically disadvantaged or underrepresented groups. This step aligns the selection of protected attributes with broader historical and societal considerations.
    
    \item \emph{Collaboration with Experts:} Collaborate with domain experts, ethicists, and stakeholders to gain additional insights. This collaborative effort aims to discern attributes with profound societal impact, enriching the identification process.
    
    \item \emph{Legal and Ethical Assessment:} Meticulously assess the legal and ethical implications associated with including or excluding specific attributes within the context of the AI application. This assessment navigates the complex landscape of legal frameworks and ethical considerations, safeguarding against unintended consequences.
\end{itemize}

The culmination of these steps results in a well-defined set of protected attributes. This carefully chosen set, derived through a triangulation of literature insights, expert input, and legal-ethical considerations, forms the foundational basis for all subsequent fairness evaluations and interventions within the Fair-by-Design framework.

\subsubsection{Fairness Notions}

Fairness notions represent the abstract principles guiding the desired equitable behavior within the AI system. The choice of fairness notions is a pivotal decision, influencing subsequent stages of the workflow. Common fairness notions, as explained in detail in \cref{section:fairness_notions}, include:

\begin{itemize}
  
    \item \emph{Demographic Parity:} Ensuring equal representation of protected groups across different outcomes.
  
    \item \emph{Equalized Odds:} Balancing the true positive and false positive rates across protected groups.
  
    \item \emph{Individual Fairness:} Treating similar individuals similarly, regardless of their protected attributes.
   
    \item \emph{Group Fairness:} Ensuring fair treatment for entire groups, typically based on statistical measures.

\end{itemize}

The selection of a fairness notion should align with the specific goals of the AI application, ethical considerations, and legal requirements.

\subsubsection{Fairness Metric}

A fairness metric quantifies the level of fairness achieved by the AI system with respect to the chosen fairness notion. The choice of a fairness metric depends on the specific fairness notion and the underlying characteristics of the data. Common fairness metrics include:

\begin{itemize}
   
    \item \emph{Disparate Impact:} Measures the ratio of favorable outcomes between protected and unprotected groups.
   
    \item \emph{Equalized Odds Difference:} Quantifies the disparity in error rates between protected groups.
   
    \item \emph{Theil Index:} Evaluates the distribution of outcomes across different demographic groups.
   
    \item \emph{Dempraphic Parity Difference:} Measures the difference in favorable outcomes between protected and unprotected groups.

\end{itemize}

\subsubsection{Fairness Notions and Metric Alignment}
\label{subsub:alignment}

The alignment between fairness notions and fairness metrics is a critical aspect of the Fair-by-Design workflow, as it establishes a systematic and measurable framework for evaluating fairness. The following considerations guide the selection process with the results also showed in \cref{fig:alignment}:

\paragraph{Demographic Parity:}

For the fairness notion of Demographic Parity, the primary concern is ensuring equal representation of protected groups across different outcomes. This notion is often aligned with the fairness metric known as \emph{Disparate Impact}, which quantifies the ratio of favorable outcomes between protected and unprotected groups. Specifically, the Disparate Impact metric is calculated as:

\[
\text{Disparate Impact} = \frac{\text{Favorable Outcome Rate for Protected Group}}{\text{Favorable Outcome Rate for Unprotected Group}}
\]

A Disparate Impact value close to 1 indicates a balanced representation, while values significantly deviating from 1 may suggest unfairness.

\paragraph{Equalized Odds:}

The Equalized Odds fairness notion aims to balance the true positive and false positive rates across protected groups. This aligns with the \emph{Equalized Odds Difference} fairness metric, which quantifies the disparity in error rates between protected groups. The Equalized Odds Difference is computed as:

\[
\text{Equalized Odds Difference} = \text{True Positive Rate}_{\text{Protected}} - \text{True Positive Rate}_{\text{Unprotected}}
\]

A value of equal or close to 0 indicates perfect balance.

\paragraph{Individual Fairness:}

Individual Fairness focuses on treating similar individuals similarly, regardless of their protected attributes. This notion is inherently challenging to quantify, but proxy metrics such as \emph{Theil Index} may be employed. The Theil Index evaluates the distribution of outcomes across different demographic groups and is given by:

\[
\text{Theil Index} = \sum \left( \frac{X_{i}}{\mu} \ln \frac{X_{i}}{\mu} \right)
\]

where \(X_{i}\) represents the outcome distribution for a specific group, and \(\mu\) is the overall average outcome. A lower Theil Index indicates a more equitable distribution.

\paragraph{Group Fairness:}

Group Fairness ensures fair treatment for entire groups, typically based on statistical measures. The \emph{Demographic Parity Difference} is a common metric aligned with this notion. It measures the difference in favorable outcomes between protected and unprotected groups and is computed as:
\[
\text{Statistical Parity Difference} = \text{Favorable Outcome Rate}_{\text{Prot}} - \text{Favorable Outcome Rate}_{\text{Unprot}}
\]
A value equal or close to 0 suggests parity.

\begin{figure}
    \centering
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Fairness notion} & \textbf{Fairness metric} \\
        \hline
        Demographic Parity & Disparate Impact \\
        \hline
        Equalized Odds & Equalized Odds Difference \\
        \hline
        Individual Fairness & Theil Index \\
        \hline
        Group Fairness & Demographic Parity Difference \\
        \hline
    \end{tabular}
    \caption{Fairness metric and notion alignment}
    \label{fig:alignment}
\end{figure}

\subsubsection{Choosing Fairness Metrics Given Notions or Vice Versa}

Choosing a fairness metric given a fairness notion, or vice versa, requires a nuanced understanding of the specific fairness goals and the characteristics of the data. The process, as well delighted in \cref{fig:choose} is the following:

\paragraph{Given a Fairness Notion:}

\begin{enumerate}
 
    \item \emph{Define Fairness Goals:} Clearly articulate the fairness goals, considering the desired equitable outcomes for protected groups.
 
    \item \emph{Identify Relevant Fairness Notion:} Select the fairness notion that aligns most closely with the defined fairness goals (e.g., Demographic Parity, Equalized Odds, Individual Fairness, or Group Fairness).
 
    \item \emph{Choose Aligned Fairness Metric:} Based on the selected fairness notion, choose the fairness metric that provides a quantitative measure of the specified fairness goals (e.g., Disparate Impact, Equalized Odds Difference, Theil Index, or Demographic Parity Difference).

\end{enumerate}

\paragraph{Given a Fairness Metric:}

\begin{enumerate}

    \item \emph{Understand Fairness Metric:} Gain a deep understanding of the chosen fairness metric, considering its strengths, limitations, and relevance to specific fairness goals.

    \item \emph{Determine Fairness Notion Alignment:} Identify the fairness notion that aligns with the underlying principles of the chosen fairness metric (e.g., Disparate Impact aligns with Demographic Parity).
 
    \item \emph{Ensure Consistency:} Confirm that the selected fairness metric is consistent with the broader fairness goals and ethical considerations of the AI application.

\end{enumerate}

\begin{figure}
    \centering
    \begin{tikzpicture}[node distance=2cm]
        \node (start) [startstop] {Protected attribute chosen};
        \node (choice) [decision, below of=start, yshift=-1cm] {Metric or Notion?};
        \node (fm) [startstop, left of=choice, xshift=-2cm] {Fairness Metric};
        \node (fn) [startstop, right of=choice, xshift=2cm] {Fairness Notion};
        \node (dfg) [startstop, below of=fm] {Definition of fairness goals};
        \node (fns) [startstop, below of=dfg] {Fairness notion study};
        \node (fms) [startstop, below of=fns] {Fairness notion selection};
        \node (dfgfn) [startstop, below of=fn] {Definition of fairness goals};
        \node (fmst) [startstop, below of=dfgfn] {Fairness metric study};
        \node (end) [startstop, below of=fmst] {Fairness metric selection};

        \draw [arrow] (start) -- (choice);
        \draw [arrow] (choice) -- (fm);
        \draw [arrow] (choice) -- (fn);
        \draw [arrow] (fm) -- (dfg);
        \draw [arrow] (dfg) -- (fns);
        \draw [arrow] (fms) -- (fns);
        \draw [arrow] (fn) -- (dfgfn);
        \draw [arrow] (dfgfn) -- (fmst);
        \draw [arrow] (fmst) -- (end);
    \end{tikzpicture}
    \caption{Fairness Notion and Metric choice}
    \label{fig:choose}
\end{figure}

This meticulous alignment process ensures that the chosen fairness metrics quantitatively capture the intended fairness notions, providing a meaningful and interpretable basis for evaluating and comparing fairness across different models and interventions within the Fair-by-Design workflow.

In summary, the objective definition stage of the Fair-by-Design workflow involves a meticulous and well-informed selection of protected attributes, fairness notions, and fairness metrics. This precision ensures the subsequent stages of the workflow are grounded in a robust foundation for addressing bias and promoting fairness in AI systems.

\subsection{Data Collection}
\label{subsection:data_collection}

The data collection stage is fundamental for the Fair-by-Design workflow. It involves systematically gathering data to train machine learning models, coupled with a detailed study of the dataset's statistical properties, with a specific focus on protected attributes.

\subsubsection{Dataset Overview}

\paragraph{Data Representation:}
\begin{itemize}
    \item Define features and their types (categorical, numerical).
    \item Specify the structure of the data, especially in the context of demographic information.
\end{itemize}

\paragraph{Data Sources:}
\begin{itemize}
    \item Transparently describe the origin of the data (surveys, administrative records, etc.).
    \item Indicate methods used for data collection, emphasizing potential biases introduced.
\end{itemize}

\subsubsection{Study of Statistical Properties}

\paragraph{Descriptive Statistics:}
\begin{itemize}
    \item Present essential statistics: mean, median, standard deviation, quartiles (for numerical features).
    \item Provide frequency distributions (for categorical features).
\end{itemize}

\paragraph{Exploration of Protected Attributes:}
\begin{itemize}
    \item Calculate distributional metrics: proportions, counts, entropy.
    \item Focus on understanding the representation of different groups within protected attributes.
\end{itemize}

\paragraph{Correlation Analysis:}
\begin{itemize}
    \item Explore correlations between features, especially with protected attributes.
    \item Utilize correlation matrices and scatter plots for visual assessment.
\end{itemize}

\paragraph{Outlier Detection:}
\begin{itemize}
    \item Employ statistical methods or visualizations to detect and analyze outliers.
    \item Highlight the potential impact of outliers on model training and fairness considerations.
\end{itemize}

\subsubsection{Outputs for Data Pre-processing}

\paragraph{Feature Engineering Possibilities:}
\begin{itemize}
    \item Leverage insights from the dataset overview for feature engineering opportunities.
    \item Consider transformations or combinations of features to enhance model performance and fairness.
\end{itemize}

\paragraph{Bias Identification:}
\begin{itemize}
    \item Use statistical properties, especially focused on protected attributes, to identify potential biases.
    \item Understand the distribution of groups within protected attributes for addressing fairness concerns.
\end{itemize}

\paragraph{Data Cleaning Strategies:}
\begin{itemize}
    \item Formulate strategies based on outlier detection and correlation analysis.
    \item Decide on the treatment of outliers and correlations, impacting overall model quality and fairness.
\end{itemize}

\paragraph{Fairness-Aware Sampling Techniques:}
\begin{itemize}
    \item If disparities exist in the representation of different groups, consider fairness-aware sampling.
    \item Aim to address imbalances during data pre-processing for equitable model training.
\end{itemize}

In summary, the data collection stage provides the necessary foundation for the Fair-by-Design workflow. It includes a comprehensive understanding of the dataset's statistical properties and guides informed data pre-processing decisions, crucial for developing fair and unbiased machine learning models.

\subsection{Data Pre-processing}
\label{subsection:data_pre_proc}

The data pre-processing stage is a critical component of the Fair-by-Design workflow, designed to transform the raw dataset obtained from the data collection stage into a format suitable for training machine learning models. This stage involves intricate processes, including converting categorical attributes into numerical representations and handling protected attributes with specific considerations.

\subsubsection{Categorical to Numerical Transformation}

Categorical attributes, often represented as strings, must be converted into numerical format to facilitate their integration into machine learning models.

\paragraph{One-Hot Encoding:}
One-hot encoding is a widely used method for categorical attributes without a natural order. It transforms a categorical attribute with \(K\) distinct categories into \(K\) binary columns, each indicating the presence or absence of a specific category. This approach prevents the model from assigning unintended ordinal relationships to the categories.

\paragraph{Ordinal Encoding:}
When there exists an ordinal relationship between categories, ordinal encoding is applied. In this method, each category is assigned a numerical value based on its order. This preserves the ordinal information while representing the categorical attribute in a numerical format.

\paragraph{Label Encoding:}
For binary categorical attributes, label encoding is employed. It assigns 0 and 1 to the respective categories, providing a straightforward numerical representation that aligns with the binary nature of the attribute.

\subsubsection{Protected Attribute Transformation}

Protected attributes, which play a pivotal role in ensuring fairness, require special treatment during the data pre-processing stage.

\paragraph{Discrete Protected Attributes:}
If a protected attribute is discrete (e.g., gender, ethnicity), it is left unchanged during the transformation process. This ensures that the inherent categories of the attribute are preserved, and the model can account for these categories without introducing bias.

\paragraph{Continuous Protected Attributes: Quantization Operation}
Continuous protected attributes, such as age, undergo a quantization process. Quantization is a method to discretize the continuous attribute into equal-width bins. This operation is represented mathematically as:

\[ Q(x) = \text{{floor}}\left(\frac{{x - \text{{min}}}}{{\text{{bin width}}}}\right) \]

where \( Q(x) \) denotes the quantized value of \( x \), \(\text{{min}}\) is the minimum value of the attribute, and the bin width is determined based on insights from the data collection stage. The goal is to create discrete bins that accurately represent the distribution of the continuous attribute within the population. This process ensures fairness by preserving the underlying characteristics of the attribute while converting it into a suitable format for machine learning models.

Quantization is particularly essential for continuous protected attributes to avoid introducing bias during the transformation. It aligns with the overarching goal of the Fair-by-Design workflow to maintain fairness throughout the machine learning model development.

In summary, the data pre-processing stage involves sophisticated transformations, ensuring that both categorical attributes are numerically represented and protected attributes are handled with precision. These steps are crucial for developing fair and accurate machine learning models within the Fair-by-Design framework.

\subsection{Algorithm design}
\label{subsection:algorithm}

The stage of Algorithm Design within the Fair-by-Design workflow stands as a pivotal phase wherein the selection, adaptation, or creation of machine learning models takes place to harmonize with the specified objectives and fairness considerations. The primary objective is to craft models that not only excel in accuracy but also steadfastly adhere to ethical and fair principles. This section undertakes an in-depth exploration of the crucial considerations, strategic approaches, and the step-by-step implementation processes entailed in algorithm design. By navigating through these aspects, the aim is to foster the development of models that not only deliver superior performance but also champion the ideals of ethics and fairness in their predictive outcomes.

\subsubsection{Key Considerations for Ensuring Fairness in Machine Learning Models}

The pursuit of fairness in machine learning models involves a nuanced consideration of several key aspects, each contributing to the overarching goal of cultivating just and impartial predictive systems. The following enumerates these considerations in detail:

\begin{itemize}
    
    \item \emph{Fairness-Aware Model Selection:} The selection of machine learning models assumes a pivotal role in addressing fairness objectives. It involves the deliberate consideration of models equipped with inherent mechanisms to tackle biases and promote equitable outcomes. Models such as adversarial networks, fairness-aware classifiers, and re-weighted learning algorithms are systematically explored and evaluated for their capacity to mitigate biases and foster fairness. This strategic approach ensures that the chosen models align with the principles of ethical and fair machine learning.

    \item \emph{Regularization Techniques:} In the pursuit of fair predictions, regularization methods, exemplified by demographic parity constraints or equalized odds constraints, emerge as instrumental tools. These methodologies are strategically employed to guide machine learning models towards fairness objectives. By integrating fairness constraints into the optimization process, regularization techniques play a pivotal role in steering the model to yield equitable outcomes across various demographic groups. This inclusion of constraints serves as a proactive measure to counteract biases and ensure impartial and just predictions.

    \item \emph{Bias Detection and Mitigation:} Embedding mechanisms within the algorithm to detect and mitigate biases is of paramount importance. This involves ongoing and systematic monitoring of the model's predictions for any indications of disparate impact. Subsequently, corrective measures are implemented in real-time to mitigate biases as they manifest. This proactive approach to bias detection and mitigation serves as a fundamental component in ensuring the model's fairness and equity.

    \item \emph{Pre-processing, In-processing, or Post-processing Algorithms:} The strategic decision regarding the selection of pre-processing, in-processing, or post-processing algorithms is intricately tied to the specific characteristics of the data and defined fairness goals. Each approach—pre-processing, in-processing, or post-processing—offers distinct advantages and considerations. The choice among them should be informed by a comprehensive understanding of the specific fairness challenges and objectives inherent to the machine learning task at hand. Factors such as the nature of biases in the data, dataset characteristics, and the desired level of fairness all play a crucial role in this decision-making process.

\end{itemize}

\subsubsection{Strategic choice for the fairness algorithm}

The strategic decision regarding the adoption of pre-processing, in-processing, or post-processing algorithms is a nuanced and crucial aspect of the Fair-by-Design workflow. This decision is intricately tied to the specific characteristics of the data and the defined fairness goals, and it plays a pivotal role in shaping the overall fairness of the machine learning model.

\paragraph{Pre-processing Algorithms}

Pre-processing algorithms play a pivotal role in shaping the initial conditions for fairness considerations within the machine learning workflow. Distinguished by their capability to transform the dataset before model training, these algorithms are instrumental in mitigating biases inherent in raw data. This preprocessing phase aims to optimize the data, rendering it more conducive to addressing fairness considerations during subsequent stages. The selection and application of pre-processing algorithms are closely intertwined with the defined fairness goals for the specific machine learning task.

\begin{itemize}

    \item \emph{Relation to Fairness Goals:} The judicious choice of pre-processing algorithms necessitates alignment with the fairness goals set forth at the commencement of the workflow. For instance, if the overarching objective is to ensure equal representation of different demographic groups, pre-processing algorithms may strategically employ re-sampling or re-weighting strategies to balance the distribution of protected attributes.

    \item \emph{Dependency on Fairness Notion:} The strategic decision between pre-processing, in-processing, or post-processing is intricately linked to the chosen fairness notion. In the context of achieving demographic parity, pre-processing algorithms may involve adjustments to the dataset's composition, ensuring proportional representation and alignment with the chosen fairness criterion.

    \item \emph{Evaluation Metrics:} The efficacy of pre-processing algorithms is evaluated through metrics such as statistical parity, disparate impact, or equal opportunity. The selection of a specific metric is contingent on the defined fairness goals and the nuanced aspects of fairness prioritized for the given machine learning task.

\end{itemize}

\paragraph{In-processing Algorithms}

In-processing algorithms intervene directly during the model training process, introducing fairness mechanisms at this crucial stage. This approach modifies the learning process to ensure fairness considerations are embedded in the model's development.

\begin{itemize}

    \item \emph{Relation to Fairness Goals:} In-processing algorithms should be tailored to the specific fairness goals defined for the task. For instance, if the objective is to minimize disparate impact, in-processing techniques may involve adjusting the model's objective function to incorporate fairness constraints.

    \item \emph{Dependency on Fairness Notion:} The choice of in-processing algorithms depends on the fairness notion guiding the workflow. If the focus is on individual fairness, algorithms may be designed to treat similar individuals similarly during the learning process.

    \item \emph{Evaluation Metrics:} Metrics such as equalized odds, calibration, or predictive parity are often employed to assess the fairness of in-processing algorithms. The selection of the metric aligns with the fairness goals and the particular aspects of fairness under consideration.

\end{itemize}

\paragraph{Post-processing Algorithms}

Post-processing algorithms come into play after model training, adjusting predictions to align with fairness objectives. This approach offers a corrective mechanism to fine-tune model outputs based on fairness considerations.

\begin{itemize}

    \item \emph{Relation to Fairness Goals:} Post-processing algorithms should be chosen to complement the defined fairness goals. If the aim is to rectify disparate impact in predictions, post-processing techniques may involve re-calibrating predicted probabilities.

    \item \emph{Dependency on Fairness Notion:} The choice of post-processing algorithms is contingent on the fairness notion guiding the workflow. If the focus is on group fairness, post-processing methods may involve re-weighting predictions to achieve parity across protected attribute groups.

    \item \emph{Evaluation Metrics:} Common evaluation metrics for post-processing algorithms include reweighted accuracy, demographic parity, or predictive rate parity. The choice of the metric is aligned with the specific fairness goals and considerations.

\end{itemize}

\paragraph{Strategic Decision Factors}

When making the strategic decision among pre-processing, in-processing, or post-processing algorithms, several factors come into play:

\begin{itemize}
    \item \emph{Nature of Biases:} Understanding the nature of biases inherent in the data is crucial. Pre-processing algorithms may be more suitable for mitigating biases present in the raw dataset, while in-processing or post-processing algorithms may address biases emerging during model training or predictions.

    \item \emph{Characteristics of the Dataset:} The characteristics of the available dataset, such as its size, composition, and distribution of protected attributes, influence the choice of algorithmic approaches. Pre-processing may be favored for large-scale dataset adjustments, while in-processing or post-processing may be more suitable for fine-tuning models with specific biases.

    \item \emph{Desired Level of Fairness:} The desired level of fairness, whether it is absolute parity or a more nuanced balance, guides the strategic decision. Pre-processing, in-processing, or post-processing may be chosen based on the granularity required to achieve the defined fairness goals.

\end{itemize}

In essence, the choice among pre-processing, in-processing, or post-processing algorithms should be informed by a comprehensive understanding of the specific fairness challenges and objectives inherent to the machine learning task at hand. The alignment with the chosen fairness notion and the careful consideration of evaluation metrics ensure that the selected algorithmic approach is well-suited to achieve the desired fairness outcomes within the Fair-by-Design workflow.


\subsubsection{Detailed Implementation Steps}

% Include the detailed implementation steps for each key consideration as per the previous discussion.

\paragraph{Fairness-Aware Model Selection}

\begin{itemize}

    \item \emph{Exploration of Fairness-Aware Models:}
     
    \begin{itemize}
    
        \item In the endeavor to develop a Fair-by-Design machine learning model, a pivotal step involves conducting an exhaustive survey and evaluation of existing fairness-aware models tailored to the specific application domain. This meticulous process requires a thorough examination of pertinent literature and available models, taking into account their performance metrics, interpretability, and fairness considerations. By subjecting existing models to rigorous evaluation, practitioners gain valuable insights into the nuanced strengths and limitations of different approaches. This discerning survey and evaluation process empower practitioners to make informed decisions regarding the most suitable model for their particular use case. Such diligence contributes significantly to the foundational aspects of a Fair-by-Design workflow, ensuring that the selected model not only aligns with technical requirements but also upholds ethical considerations inherent to the application domain.
    
        \item When delving into the realm of fairness-aware models tailored for a specific application domain, it is judicious to prioritize models explicitly designed to address bias. This consideration encompasses, but is not confined to, adversarial networks or models featuring inherent fairness constraints. Adversarial networks serve the purpose of mitigating bias by introducing a secondary network that identifies and counteracts any discriminatory patterns discerned in the primary model. Conversely, models with built-in fairness constraints integrate predefined fairness metrics into the optimization process, ensuring the model adheres to fairness criteria throughout training.

        The evaluation of these specialized models yields a nuanced understanding of their efficacy in handling bias, furnishing practitioners with invaluable insights into the most suitable approach for mitigating bias within their specific context. This deliberate consideration solidifies the Fair-by-Design approach, underscoring the paramount importance of selecting models that actively address and rectify biases to foster equitable outcomes in the realm of machine learning applications.    
    
    \end{itemize}
    
    \item \emph{Implementation and Customization:}
    
    \begin{itemize}
    
        \item Following the selection of a fairness-aware model tailored for the specific application domain, the subsequent imperative involves its implementation within the chosen machine learning framework. This intricate process necessitates the translation of model specifications and architecture into code, meticulous configuration of essential parameters, and seamless integration into the overarching machine learning pipeline. The implementation procedure should strictly adhere to industry best practices and guidelines stipulated by the chosen framework, ensuring not only optimal performance but also seamless compatibility.

        Furthermore, developers must prioritize the interpretability of the model, recognizing that transparency is pivotal in comprehending how fairness considerations are intricately embedded within the system. The successful execution of the implementation phase represents a pivotal juncture in the Fair-by-Design workflow. It sets the groundwork for subsequent stages encompassing model training, rigorous evaluation, and eventual deployment, all underscored by an unwavering commitment to fostering equitable and unbiased machine learning outcomes.    
    
        \item Subsequent to the implementation of the chosen fairness-aware model, a pivotal phase entails customizing the model to harmonize with the unique characteristics of the dataset and the precise fairness objectives delineated earlier in the workflow. This customization process necessitates the fine-tuning of model parameters, adjustment of hyperparameters, and incorporation of features that aptly accommodate the nuanced intricacies present in the dataset.

        Moreover, developers may find it imperative to introduce specific fairness-oriented adjustments to the model's architecture. This ensures the model's adeptness in effectively handling and mitigating biases associated with protected attributes. Such meticulous and tailored customization stands as an indispensable step for optimizing the model's performance within the paradigm of the Fair-by-Design approach. Here, the overarching objective extends beyond technical excellence, encompassing the ethical imperative of delivering unbiased machine learning outcomes.    
    
    \end{itemize}

\end{itemize}

\paragraph{Explainability and Interpretability}

\begin{itemize}

    \item \emph{Model Selection:}

    \begin{itemize}

        \item Within the Fair-by-Design workflow, the strategic choice of models renowned for their high explainability and interpretability holds paramount significance. Decision trees and rule-based models, celebrated for their inherent transparency, stand out as preferred choices in this context. The rationale underpinning this selection is deeply rooted in the commitment to transparency and accountability.

        Models characterized by easy interpretability empower stakeholders, including end-users and decision-makers, to comprehend the intricate decision-making processes. This transparency not only cultivates trust but also facilitates the identification and mitigation of any potential biases or fairness concerns embedded within the model. By deliberately opting for models with high explainability, the Fair-by-Design workflow aligns seamlessly with its overarching goal of cultivating machine learning systems that excel not only in technical robustness but also in ethical soundness, ensuring they are readily understandable by a diverse range of stakeholders.

        \item Within the Fair-by-Design workflow, when infusing fairness considerations into model selection, a critical aspect is to judiciously navigate the trade-offs between model complexity and interpretability. The optimal balance hinges significantly on the specific application requirements, playing a pivotal role in the decision-making process.

        Highly complex models, while potentially offering superior predictive performance, often come at the cost of diminished interpretability. Conversely, interpretable models such as decision trees or rule-based models may exhibit limitations in capturing intricate patterns present in the data. Striking the right balance becomes paramount; the Fair-by-Design approach is not solely concerned with ensuring fairness but also places a premium on transparency.
        
        Therefore, the strategic selection of models aligning with the interpretability needs of stakeholders while still meeting performance requirements becomes a key consideration in the design process. This deliberate and conscious decision-making process contributes significantly to the development of machine learning models that are not only fair but also understandable and accountable, embodying the principles of the Fair-by-Design approach.

    \end{itemize}

\end{itemize}

\paragraph{Regularization Techniques}

\begin{itemize}

    \item \emph{Integration of Fairness Constraints:}

    \begin{itemize}

        \item The incorporation of fairness constraints mandates a nuanced comprehension of the precise fairness goals and metrics germane to the application domain. In the proposed approach, there is a pronounced emphasis on the principled and systematic integration of these constraints into the machine learning model's training process. This deliberate embedding of fairness into the training regimen aims to mitigate biases and advocate for equitable outcomes.

        The overarching objective of the workflow extends beyond the mere delivery of accurate predictions. It is intricately woven with the commitment to ethical and fairness considerations, thereby contributing substantively to the cultivation of responsible and unbiased machine learning practices. This concerted effort reinforces the imperative of aligning machine learning models with ethical principles, ensuring they operate within a framework that prioritizes fairness and equity.

        \item Within the Fair-by-Design approach to machine learning, a pivotal facet involves the systematic experimentation with various regularization techniques. These methods, exemplified by demographic parity constraints or equalized odds constraints, assume a substantial role in the concerted effort to mitigate biases and champion fairness within machine learning models. Tailored to address specific fairness considerations linked to protected attributes, these techniques are meticulously designed to ascertain that the model's predictions remain uninfluenced by factors such as gender, race, or other sensitive attributes. This intentional and exploratory engagement with regularization techniques underscores the commitment to cultivating models that not only excel in predictive accuracy but also uphold the ethical imperative of fairness and equity.
    
    \end{itemize}

\end{itemize}

\paragraph{Bias Detection and Mitigation}

\begin{itemize}
    
    \item \emph{Dynamic Bias Mitigation:}
    
    \begin{itemize}
    
        \item A proactive strategy within the Fair-by-Design framework involves the development of adaptive algorithms that dynamically adjust to emerging biases in the data. These algorithms are meticulously designed to engage in continuous monitoring and evaluation of the model's performance, adept at identifying any potential biases that may manifest during operational phases. The adaptive nature of these algorithms empowers them to dynamically tweak parameters or decision boundaries in response to emerging biases, thereby ensuring an ongoing commitment to fairness in real-world applications. This forward-thinking approach aligns seamlessly with the ethos of the framework, emphasizing not only initial fairness but also the continuous vigilance and adaptation necessary for sustained equity in machine learning outcomes.

        \item A pivotal step in the Fair-by-Design workflow involves the implementation of corrective measures, such as re-weighting or re-sampling, to actively mitigate biases as they are detected. As biases are identified through ongoing monitoring, these targeted corrective measures are strategically applied to address imbalances and promote fairness in the model's predictions. This iterative and adaptive approach not only acknowledges the inevitability of biases but also proactively works towards rectifying them, reinforcing the commitment to fairness and equity embedded within the Fair-by-Design methodology.

    \end{itemize}

\end{itemize}

\paragraph{Human-in-the-Loop Approaches}

\begin{itemize}
    
    \item \emph{Stakeholder Collaboration:}
    
    \begin{itemize}
    
        \item Organize collaborative sessions with domain experts and stakeholders to delve into the intricacies of fairness considerations. These sessions serve as a platform for a comprehensive exploration of nuances related to fairness, leveraging the collective insights and expertise of domain specialists and key stakeholders. The aim is to foster a shared understanding of fairness challenges, identify contextual nuances, and coalesce diverse perspectives. Such collaborative endeavors contribute significantly to the workflow, ensuring that the developed models align not only with technical requirements but also with the ethical and contextual considerations of the specific application domain.
    
        
        \item Engage stakeholders actively in both model validation and decision-making processes. This participatory approach ensures that the perspectives and insights of stakeholders are integral to the validation of the model's performance and the decision-making procedures. By involving stakeholders, including end-users and decision-makers, in these crucial stages, the workflow embraces transparency and inclusivity. This collaborative involvement contributes to the development of models that are not only technically sound but also align with the diverse needs and expectations of the stakeholders, fostering a sense of ownership and accountability in the overall machine learning process.
    
    \end{itemize}

\end{itemize}

\subsubsection{Pre-processing Algorithm Proposed}

\paragraph{Fairness through data augmentation}
\label{subsec:ftdr}

In this approach, the paradigm of bias mitigation takes on a unique and innovative perspective, one that prioritizes data augmentation over attribute removal. Unlike traditional approaches that center on the exclusion of specific attributes, this methodology embraces the concept of data augmentation, introducing a distinctive definition of fairness and equity within the AI system.

The essence of this approach revolves around the augmentation of the dataset by introducing new data instances that offer a more comprehensive and inclusive representation of the underlying population. This expanded dataset is designed to be more diverse, representative, and balanced, transcending the limitations of the original data and fostering a more nuanced understanding of fairness. 

The introduction of augmented data instances leads to a redefined notion of fairness within the AI system. Instead of solely focusing on the absence of biased attributes, fairness is now measured in terms of the dataset's inclusivity and its ability to capture the diversity and nuances present within the population it seeks to serve. 

The process of data augmentation necessitates a careful selection of techniques and methodologies that can introduce new data instances while maintaining the integrity and quality of the dataset. These techniques may encompass oversampling, synthetic data generation, or other data synthesis methods, each tailored to the specific context and objectives of the AI system.

In traditional fairness definitions, the focus often revolves around ensuring fair treatment for individual protected attributes, denoted as $A_1, A_2, \ldots, A_k$. While this is undoubtedly crucial, a more comprehensive understanding of fairness calls for an examination of fairness in the context of combinations of protected attributes and the output. A new definition of fairness is proposed, which takes into account the representation of all combinations of $k$ protected attributes and the output, aiming for equitable representation across these combinations.

A fair dataset is defined as one in which, for each combination of protected attributes $\{A_1, A_2, \ldots, A_k\}$ and the output $O_j$, the representation is equal and proportional. Mathematically, a dataset is fair if:

\[
\forall i_1, i_2, \ldots, i_k, j: \frac{|D_{i_1, i_2, \ldots, i_k, j}|}{|D|} = \text{constant}
\]

where:
\begin{itemize}

    \item $D$ is the dataset

    \item $|D_{i_1, i_2, \ldots, i_k, j}|$ is the number of samples with the specific combination of protected attributes $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$ and output $O_j$

    \item $|D|$ is the total number of samples in the dataset
    
\end{itemize}

This entails that any combination of demographic groups, defined by the protected attributes, and the output should have comparable representation, thereby fostering a balanced and unbiased dataset.

By striving for equal representation of combinations of protected attributes, is addressed a fundamental aspect of fairness that transcends individual attributes. This approach provides a more nuanced understanding of fairness by considering the intersections of various demographic groups. It encourages a broader examination of potential biases that may arise when considering multiple attributes simultaneously.

Incorporating this definition of fairness into the data augmentation process enables us to promote a comprehensive notion of fairness, aligning with the principles of equal opportunity and non-discrimination across all combinations of protected attributes. The subsequent algorithm and experimental evaluation are designed to actualize this definition and demonstrate its effectiveness in achieving a more equitable representation within the dataset.

Let $D$ be a dataset $R^{n \times m}$, where $n$ is the number of samples and $m$ is the number of features. Let $k$ be the number of protected variables represented as $R^{n \times 1}$, with $u_i$ the number of unique values for the $i$-th protected variable. Let there be a single output variable represented as $R^{n \times 1}$.

A data augmentation function $\mathcal{R}$ can be formally defined as a mapping:
\[
\mathcal{R}: R^{n \times m} \rightarrow R^{a \times m}
\]
where $a > n$, and the function $\mathcal{R}$ transforms the input dataset $D$ of dimensions $n \times m$ into an output dataset $D'$ of dimensions $a \times m$.

The number of possible combinations of these variables is $\prod_{i=1}^{k+1} u_i$. The limit of the product is ${k + 1}$ because for the combination are considered the protected attributes together with the output.

Consider a set $\text{Combinations}$ with occurrences of all $\prod_{i=1}^{k+1} u_i$ combinations within the dataset. 

Consider a set $\text{Frequencies}$ with frequency of all combination in $\text{Combinations}$ set.

For each combination, the number of rows in which that combination appears should be equal to the maximum of $\text{Frequencies}$ set, denoted as $\text{Max(Frequencies)}$

Mathematically, the number of rows ($a - n$) that will be added to the dataset by the data augmentation algorithm is given by:
\[
a - n = \sum_{i=1}^{|Combinations|}(\text{Max}(\text{Frequency}) - (\text{Frequency\textunderscore i}))
\]

Once having detected the number of rows to add for each combination it's important to define how to are detected the element of each row.

Let $C$ be the sub-dataset of $D$ with a specific combination $\text{c\textunderscore i}$ for the protected attributes. Given the number of rows \text{Max}(\text{Frequency}) - (\text{Frequency\textunderscore i}) each row is added according to the following rules:

\begin{itemize}
    
    \item The values of the protected and output variables are set according to the specific combination.
   
    \item For all other attributes, a random value $\text{random\_value}_{ij}$ is generated, where $j$ represents the specific attribute, and $i$ represents the row being added for that attribute. $\text{random\_value}_{ij}$ is within the minimum and maximum range for attribute $j$.

\end{itemize}

In \cref{alg:prepro} is delighted the pseudocode of the previously described algorithm:

\begin{algorithm}
    \caption{Data augmentation}
    \begin{algorithmic}[1]
        \State \textbf{Input:} combination\_set, combination\_frequency, protected\_attributes, dataset\_attributes
        \State \textbf{Output:} Updated dataset

        \State max\_frequency $\gets$ max(combination\_frequency)

        \For{index \textbf{in} (0, len(combination\_set) - 1)}
            \State combination $\gets$ combination\_set[index]
            \State frequency $\gets$ combination\_frequency[index]
            \State combination\_dataset $\gets$ dataset[dataset[combination] == combination\_set[index]]

            \While{frequency $<$ max\_frequency}
                \State new\_row $\gets$ empty

                \For{(attr, val) \textbf{in} (combination, protected\_attributes)}
                    \State new\_row[attr] $\gets$ val
                \EndFor

                \For{attr \textbf{in} dataset\_attributes \textbf{and not in} protected\_attributes}
                    \State new\_row[attr] $\gets$ random(min(combination\_dataset[attr]), max(combination\_dataset[attr]))
                \EndFor

                \State dataset.add(new\_row)
                \State frequency $+$= 1
            \EndWhile
        \EndFor

        \State \textbf{return} Updated dataset
    \end{algorithmic}
    \caption{Data augmentation algorithm pseudocode}
    \label{alg:prepro}
\end{algorithm}

The proposed data augmentation algorithm aims to achieve fairness by ensuring an equal representation of all possible combinations of protected and output variables. Here, we provide a theoretical analysis of the algorithm's key aspects, including time complexity, space complexity, and additional considerations.

The time complexity is influenced by the dataset size ($n$), the number of features ($m$), and the number of combinations of protected attributes and output. The overall time complexity is given by the number of combinations:
\[
\mathcal{O}(n \cdot m \cdot |Combinations|)
\]
where $|Combinations|$ is the number of combinations that have been computed

The space complexity is determined by the storage of combination frequencies and the occurrence count array. The overall space complexity is given by:
\[
\mathcal{O}(\prod_{i=1}^{k+1} u_i)
\]

\begin{itemize}
    \item \emph{Number of Combinations:} The number of combinations \(\prod_{i=1}^{k+1} u_i\) grows exponentially with the number of protected binary variables (\(k\)), significantly influencing both time and space complexity.
    
    \item \emph{Data Augmentation Techniques:} The algorithm's effectiveness depends on the chosen data augmentation techniques, which may introduce additional computational costs.
    
    \item \emph{Random Value Generation:} The quality of randomness in generated values is crucial for the diversity of the augmented dataset and, consequently, the algorithm's fairness outcomes.
    
    \item \emph{Implementation and Optimization:} The efficiency of the algorithm is affected by implementation details, language choice, and potential optimizations.
\end{itemize}

\subsubsection{Significance}

The Algorithm Design stage is pivotal in shaping the ethical and fair behavior of machine learning models within the workflow. By carefully selecting, customizing, and integrating fairness-aware models, and incorporating transparency and human-in-the-loop approaches, this stage ensures that the resulting models align with ethical considerations and promote equitable outcomes.

\subsection{Model training and evaluation}
\label{subsection:model-training}

\subsubsection{Introduction}

This section delivers a thorough overview of the model training and evaluation process within the Fair-by-Design framework. After the careful application of the chosen pre-processing, in-processing, or post-processing algorithm, the training phase assumes a pivotal role in shaping a machine learning model. The emphasis extends beyond mere accuracy objectives to incorporate the ethical considerations of fairness. This holistic approach underscores the commitment to developing models that excel not only in predictive performance but also adhere to principles of equity and justice, thereby embodying the ethos of the Fair-by-Design framework.

\subsubsection{Training Process}

\paragraph{Data Splitting}

The dataset undergoes a deliberate split into training, validation, and test sets, each serving a distinct strategic purpose. The training set acts as the foundation for the model's learning process, enabling it to grasp patterns and relationships within the data. The validation set plays a crucial role in hyperparameter tuning, fine-tuning the model to enhance its generalization capabilities. Finally, the test set serves as the litmus test for the final model's performance, offering a reliable measure of its predictive capabilities. This meticulous division ensures a comprehensive and robust evaluation of the model's effectiveness and generalizability within the Fair-by-Design framework.

\paragraph{Model Architecture}

The architecture of the selected model is meticulously crafted to strike a delicate balance between accuracy and fairness considerations within the Fair-by-Design framework. Architectures may incorporate innovative components, such as adversarial layers or customized modules specifically designed to enforce fairness constraints. The focal point is to develop a model that excels not only in capturing intricate patterns but is also interpretable and transparent in its decision-making process. This intentional design reflects the commitment to fostering models that go beyond predictive prowess, embodying principles of interpretability and transparency while addressing fairness concerns.

\paragraph{Training Parameters}

Critical training parameters, spanning the learning rate, batch size, and convergence criteria, undergo meticulous tuning in the Fair-by-Design framework. The learning rate assumes significance as it dictates the step size in the optimization process, thereby influencing the pace of updates to model parameters. This parameter is pivotal in determining the balance between the model's adaptability and stability during training.

The batch size, another key parameter, determines the number of samples processed in each iteration. This factor significantly impacts the overall efficiency of the training process, influencing resource utilization and the model's ability to generalize well to diverse datasets.

Convergence criteria hold a crucial role by ensuring that the training process halts when the model achieves optimal performance. These criteria are indispensable in preventing overfitting, safeguarding against the model becoming excessively tailored to the training data to the detriment of its ability to generalize to new, unseen data.

This comprehensive tuning process, applicable beyond deep models, reflects a nuanced approach in optimizing key parameters to foster models that not only exhibit accuracy but also align with fairness considerations in the Fair-by-Design paradigm.

\paragraph{Fairness Constraints}

In scenarios where fairness constraints are seamlessly integrated into the training process within the Fair-by-Design framework, a deliberate and systematic approach is adopted. These constraints are explicitly defined and enforced, often involving the introduction of regularization terms designed to penalize disparate treatment of different demographic groups. This strategic incorporation aims to instill fairness by discouraging the model from exhibiting biased behavior towards specific groups.

Adversarial training components might also be leveraged in this process. These components act as a countermeasure to mitigate bias, introducing a dynamic element that works to identify and neutralize any discriminatory patterns that may emerge during training. The overall objective is to foster models that not only deliver accurate predictions but also adhere to ethical principles of fairness and equity. This intentional integration of fairness constraints reinforces the commitment to cultivating machine learning models that contribute to just and unbiased outcomes.

\paragraph{Model Training}

The model undergoes training for a specified number of iterations, leveraging the training set to iteratively update its parameters and increase accuracy. This iterative training process is not limited to deep learning models and is applicable across various model categories. The overarching goal remains to strike a balance between accuracy and fairness, contributing to the development of a machine learning model proficient in capturing underlying patterns while being conscious of potential biases.

Through successive iterations on the training set, the model refines its understanding of the data, aiming to optimize its performance with due consideration to fairness. This approach aligns with the principles of the Fair-by-Design framework, ensuring that the trained model is both technically robust and ethically sound in its decision-making, irrespective of the specific model category employed.

\subsubsection{Evaluation Metrics}

\paragraph{Accuracy Metrics}

A suite of accuracy metrics is employed to holistically assess the model's overall performance:

\begin{itemize}
    \item \emph{Accuracy:} The ratio of correctly predicted instances to the total instances.
    
    \item \emph{Precision:} The proportion of true positive predictions among instances predicted as positive.
    
    \item \emph{Recall:} The proportion of true positive predictions among actual positive instances.
    
    \item \emph{F1-score:} The harmonic mean of precision and recall, providing a balanced measure of accuracy.
\end{itemize}

\paragraph{Fairness Metrics}

Fairness metrics are instrumental in evaluating the model's behavior across different demographic groups:

\begin{itemize}
    
    \item \emph{Equalized Odds}: The Equalized Odds metric serves as a critical assessment tool for gauging the model's performance concerning fairness. This evaluation metric specifically investigates whether the model exhibits comparable false positive and false negative rates across distinct demographic groups. By scrutinizing disparities in these rates, the Equalized Odds metric offers insights into the model's capacity to provide equitable outcomes for diverse segments of the population, contributing to a comprehensive understanding of its fairness considerations, as reported in \cref{section:metrics}
    
    \item \emph{Demographic Parity Difference:} The Demographic Parity Difference metric is a meticulous examination of the distribution of positive outcomes within each demographic group. This evaluation metric meticulously analyzes the proportion of positive results for each group, aiming to identify and quantify any disparities that may exist. By shedding light on these disparities, the Statistical Parity metric offers a nuanced perspective on how the model's predictions align with different demographic segments. This in-depth analysis contributes to a comprehensive assessment of fairness, facilitating a more detailed understanding of potential imbalances in positive outcomes across diverse groups, as reported in \cref{section:metrics}

    \item \emph{Disparate Impact:} The Disparate Impact metric serves to establish the degree on which a certain group is related to the output to provide a fairness evaluation, as better explained in \cref{section:metrics}

\end{itemize}

\subsubsection{Results and Analysis}

\paragraph{Accuracy Results}

A meticulous analysis of accuracy results stands as a cornerstone in gaining valuable insights into the model's predictive prowess concerning the target variable. Precision, recall, and F1-score metrics play a pivotal role in offering a nuanced understanding of the intricate trade-offs between true positives, false positives, and false negatives.

Precision provides insights into the accuracy of positive predictions, recall assesses the model's ability to capture all positive instances, and the F1-score encapsulates a harmonized view, considering both precision and recall. The overarching goal transcends mere accuracy; it extends to cultivating a balanced and informed prediction capability. This multifaceted evaluation approach ensures a comprehensive grasp of the model's performance, allowing for a nuanced interpretation of its predictive accuracy while considering the intricate interplay between different performance metrics.

\paragraph{Fairness Results}

Fairness metrics serve as a crucial lens through which biases within the model are identified and addressed. Disparate impact, equalized odds, and statistical parity results undergo thorough analysis to evaluate whether the model demonstrates disparate treatment among various demographic groups. The emphasis is not only on recognizing but also on actively mitigating disparities, fostering the development of a model that upholds principles of fairness and equity. By leveraging these metrics, the evaluation process goes beyond traditional performance measures, ensuring a diligent scrutiny of how the model's predictions may impact different demographic segments and actively working towards an unbiased and equitable predictive system.

\paragraph{Trade-offs}

Achieving a delicate equilibrium between accuracy and fairness is a pivotal consideration in the development of a balanced machine learning model. This nuanced trade-off recognition necessitates a thoughtful approach, often involving adjustments to the model architecture and training process. Various techniques, such as re-weighting, re-sampling, or the introduction of customized loss functions, can be strategically employed to specifically address fairness concerns identified during model evaluation.

\begin{itemize}

    \item \emph{Adjustment Techniques:} Techniques like re-weighting involve assigning different weights to instances in the training dataset, with the aim of mitigating biases. Re-sampling methods focus on altering the composition of the dataset to balance representation across different groups. Customized loss functions are designed to explicitly penalize unfair predictions, guiding the model towards equitable outcomes.

    \item \emph{Central Theme: Striking the Right Balance:} The central theme revolves around striking the right balance, ensuring the optimal performance of the model while actively addressing and mitigating fairness considerations. This delicate calibration process is intrinsic to refining the model, aligning it not only with accuracy goals but also with the ethical imperative of fostering fairness and equity in its predictions.

    \item \emph{Ethical Imperative:} This process underscores the ethical imperative of machine learning practitioners to consider and actively manage the trade-offs between accuracy and fairness. By navigating these nuanced considerations, practitioners contribute to the development of models that not only excel in predictive accuracy but also adhere to ethical principles and societal norms.

\end{itemize}

\subsubsection{Discussion}

\paragraph{Interpretability}

The interpretability of the model's decisions holds paramount importance. Techniques such as SHAP (SHapley Additive exPlanations) values or LIME (Local Interpretable Model-agnostic Explanations) are thoroughly explored to enhance transparency in the decision-making process. The focus is on understanding and elucidating how the model arrives at its predictions. This commitment to interpretability contributes significantly to the model's trustworthiness, providing stakeholders with clear insights into the factors influencing predictions. By adopting these interpretability techniques, the model not only meets technical excellence standards but also aligns with the broader goal of fostering trust and confidence in its decision-making capabilities.

\paragraph{Ethical Considerations}

The discussion extends beyond technical aspects to encompass ethical considerations arising from the model's behavior. Rigorous attention is dedicated to uncovering potential biases, identifying unintended consequences, and evaluating the impact on different demographic groups. Strategies for addressing ethical concerns are thoughtfully proposed, emphasizing a steadfast commitment to responsible and ethical AI practices. This holistic examination ensures that the model's deployment aligns not only with technical efficacy but also with a profound awareness of its societal implications, fostering a responsible and ethical approach to artificial intelligence.

\paragraph{Further Refinement}

Derived from the observed results, proposals for further refinement are thoughtfully put forth. This iterative process may encompass adjusting fairness constraints, fine-tuning hyperparameters, or exploring alternative algorithms, all aimed at enhancing both accuracy and fairness in model predictions. The iterative nature of this refinement process underscores a steadfast commitment to continuous improvement and the unwavering pursuit of fairness. This dynamic approach ensures that the model evolves in response to insights gained from its performance, fostering a resilient and adaptive system that continually strives to achieve the highest standards of accuracy and fairness.

