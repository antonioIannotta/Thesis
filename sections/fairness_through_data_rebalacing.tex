\documentclass{article}
\usepackage{amsmath}
\usepackage{algorithmic}
\usepackage{amssymb}
\usepackage{algorithm}
\usepackage[utf8]{inputenc}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}

\section{Fairness through data rebalancing}
\subsection{A New Definition of Fairness}
In traditional fairness definitions, the focus often revolves around ensuring fair treatment for individual protected attributes, denoted as $A_1, A_2, \ldots, A_k$. While this is undoubtedly crucial, a more comprehensive understanding of fairness calls for an examination of fairness in the context of combinations of protected attributes. We propose a new definition of fairness that takes into account the representation of all combinations of $k$ protected attributes, aiming for equitable representation across these combinations.

\subsubsection{Equal Representation of Combinations}

We define a fair dataset as one in which, for each combination of protected attributes $\{A_1, A_2, \ldots, A_k\}$, the representation is equal and proportional. Mathematically, a dataset is fair if:

\[
\forall i_1, i_2, \ldots, i_k: \frac{|D_{i_1, i_2, \ldots, i_k}|}{|D|} = \text{constant}
\]

where:
- $D$ is the dataset,
- $|D_{i_1, i_2, \ldots, i_k}|$ is the number of samples with the specific combination of protected attributes $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$,
- $|D|$ is the total number of samples in the dataset.

This entails that any combination of demographic groups, defined by the protected attributes, should have comparable representation, thereby fostering a balanced and unbiased dataset.

\subsubsection{Promoting Comprehensive Fairness}

By striving for equal representation of combinations of protected attributes, we address a fundamental aspect of fairness that transcends individual attributes. This approach provides a more nuanced understanding of fairness by considering the intersections of various demographic groups. It encourages a broader examination of potential biases that may arise when considering multiple attributes simultaneously.

Incorporating this definition of fairness into the dataset rebalancing process enables us to promote a comprehensive notion of fairness, aligning with the principles of equal opportunity and non-discrimination across all combinations of protected attributes. Our subsequent algorithm and experimental evaluation are designed to actualize this definition and demonstrate its effectiveness in achieving a more equitable representation within the dataset.

\subsection{Algorithm description}

Let \( D \) be a dataset \( R^{n \times m} \), where \( n \) is the number of samples and \( m \) is the number of features. Let \( k \) be the number of protected variables represented as \( R^{n \times 1} \), and let there be a single output variable represented as \( R^{n \times 1} \).

A rebalancing function \( \mathcal{R} \) can be formally defined as a mapping:

\[
\mathcal{R}: R^{n \times m} \rightarrow R^{l \times m}
\]

where \( l > m \), and the function \( \mathcal{R} \) transforms the input dataset \( D \) of dimensions \( n \times m \) into an output dataset \( D' \) of dimensions \( l \times m \).\\
\\
Let \( k \) be the number of binary protected variables in the dataset \( D \), and consider the output variable to be binary as well. The number of possible combinations of these variables is \( 2^{(k+1)} \).

Consider a set \( \text{Combination-frequency} \) where we have occurrences of all \( 2^{(k+1)} \) combinations within the dataset. For each combination, the number of rows in which that combination appears should be equal to the maximum occurrence among all combinations present in the set \( \text{Combination\textunderscore frequency} \). This maximum value is denoted as \( \text{Max}(\text{Combination-frequency}) \).

Mathematically, the number of rows (\( l \)) the final dataset should have for each combination is given by:

\[
l = \text{Max}(\text{Combination-frequency})
\]\\
\\
Let \( l \) be the desired number of rows for the final dataset. For each combination of values, we calculate the occurrence count \( \text{occurrence}_i \), where \( i \) ranges from 1 to \( 2^{(k+1)} \), with \( k \) being the number of protected binary variables and considering the output variable as binary.

The total number of rows to be added is given by:

\[
\text{total\_rows\_to\_add} = l - \sum_{i=1}^{2^{(k+1)}} \text{occurrence}_i
\]

For each iteration:
\begin{itemize}
    \item The values of the protected and output variables are set according to the specific combination.
    \item For all other attributes, a random value \( \text{random\_value}_{ij} \) is generated, where \( j \) represents the specific attribute and \( i \) represents the row being added for that attribute. \( \text{random\_value}_{ij} \) is within the minimum and maximum range for attribute \( j \).
\end{itemize}

\subsection{Pseudocode}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{combination\_set, combination\_frequency, protected\_attributes, dataset\_attributes}
    \Output{Updated dataset}

    max\_frequency $\gets$ max(combination\_frequency)\;
    \For{index $\gets$ 0 \KwTo len(combination\_set)}{
        combination $\gets$ combination\_set[index]\;
        frequency $\gets$ combination\_frequency[index]\;
        \While{frequency $<$ max\_frequency}{
            new\_row $\gets$ empty\;
            \For{(attr, val) \textbf{in} (combination, protected\_attributes)}{
                new\_row[attr] $\gets$ val\;
            }
            \For{attr \textbf{in} dataset\_attributes \textbf{and not in} protected\_attributes}{
                new\_row[attr] $\gets$ random(min(attr), max(attr))\;
            }
            dataset.add(new\_row)\;
            frequency $+$= 1\;
        }
    }
\end{algorithm}