\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{acronym}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


\newcommand{\thesislang}{english} % commentare in caso di tesi in italiano
%\usepackage{thesis-style}
% version
%\newcommand{\versionmajor}{0}
%\newcommand{\versionminor}{1}
%\newcommand{\versionpatch}{2}
%\newcommand{\version}{\versionmajor.\versionminor.\versionpatch}
%\typeout{Document version: \version}

\school{\unibo}
\programme{Corso di Laurea Magistrale in Ingegneria e Scienze Informatiche}
\title{Fair-by-design algoriths for access to education}
\author{Antonio Iannotta}
\date{\today}
\subject{Intelligent Systems Engineering}
\supervisor{Prof. Giovanni Ciatto}
\cosupervisor{Prof. Roberta Calegari}
\morecosupervisor{Prof. Andrea Omicini}
\session{IV}
\academicyear{2022-2023}

% Definition of acronyms
\acrodef{IoT}{Internet of Thing}
\acrodef{vm}[VM]{Virtual Machine}
\acrodef{AI}{Artificial Intelligence}


\mainlinespacing{1.241} % line spacing in mainmatter, comment to default (1)

\begin{document}
	
\frontmatter

\input{front.tex}

\begin{abstract}

    This master's thesis presents an in-depth approach to addressing fairness and bias mitigation in the design and development of data-driven methods. The primary contribution of this study is the proposal and implementation of an innovative Fair-by-Design workflow that incorporates various strategies for bias mitigation within data, algorithms, and decision-making processes. \\
    The research focuses on the educational data of the Canary Islands, leveraging a dataset encompassing detailed information about student performance and educational outcomes. The primary objective is to ensure equitable and unbiased application of data-driven algorithms within the educational context. \\
    The methodology involves the systematic evaluation of multiple bias mitigation strategies, including techniques for algorithmic fairness, data preprocessing, and model calibration. The critical aspect of this research centers on the comparison of these strategies based on their impact on the predictive accuracy of the algorithms. This approach provides practical insights into the trade-offs between fairness and accuracy. \\
    The research findings offer valuable insights into the trade-offs between fairness and accuracy when developing data-driven methods for educational data. Through the proposed Fair-by-Design workflow, the study demonstrates how to strike a balance between ensuring fairness and maintaining the accuracy of predictions. \\
    This thesis contributes to the ongoing discourse on fairness in machine learning and data-driven decision-making. The results provide guidance for stakeholders in the education sector, aiding them in making informed decisions about algorithm deployment to promote fairness and minimize bias within educational systems. \\
    \textbf{Keywords:} Fair-by-Design, Bias Mitigation, Educational Data, Canary Islands, Algorithmic Fairness, Predictive Accuracy, Comparative Analysis.
    
    \end{abstract}
    


\begin{acknowledgements} % this is optional
Never too far down, to come back
\end{acknowledgements}

%----------------------------------------------------------------------------------------
\tableofcontents   
%\listoffigures     % (optional) comment if empty
%\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

Artificial Intelligence (AI) has witnessed a remarkable surge in prominence and utility in recent years, emerging as a transformative force in various domains. From autonomous vehicles to healthcare diagnosis and recommendation systems, AI applications are becoming increasingly pervasive in our daily lives. However, this rapid expansion has brought to the forefront a critical concern: the presence of bias within AI systems.\\
The concept of bias in AI refers to the unintended or systematic favoring of certain groups or characteristics within the data, algorithms, or decision-making processes, leading to unfair and inequitable outcomes. In this era of AI-driven decision-making, it is imperative to address bias to prevent discrimination and inequality. Moreover, fairness, as an ethical principle, represents the aspiration to ensure that AI systems produce equitable and just results for all individuals, regardless of their personal attributes. \\
The impact of bias and unfairness in AI systems extends beyond mere technological concerns. These issues have profound societal implications, affecting areas such as employment, education, and access to critical services. Biased AI systems can perpetuate existing inequalities, reinforce harmful stereotypes, and undermine fundamental principles of justice and equality. \\
In the realm of education, where data-driven decision-making has become increasingly prevalent, the stakes are particularly high. Educational institutions use AI systems to make decisions about student admissions, learning outcomes, and resource allocation. Consequently, mitigating bias and ensuring fairness in AI-driven education systems is not merely a technological challenge but a moral and societal imperative. \\
This work introduces a comprehensive Fair-by-Design workflow, meticulously elaborated in the "Contribution" chapter. The essence of this workflow lies in its capacity to incorporate fairness considerations at the very inception of AI system design. In this thesis, four distinct and promising approaches to guarantee fairness are integrated into the workflow, affording a thorough and unbiased comparison of their effectiveness in bias mitigation. \\
The structure of this thesis is organized as follows: The \textbf{State of the Art} chapter undertakes an exhaustive review of the existing approaches and methodologies aimed at mitigating bias and addressing fairness in the development of AI systems. It provides the foundation upon which the innovative Fair-by-Design workflow is built. The \textbf{Contribution} chapter delves into the intricacies of the workflow, elucidating how it seamlessly integrates various fairness approaches into the design process. The \textbf{Validation} chapter meticulously presents the results obtained from the application of these fairness approaches, offering an empirical comparison of their performance and effectiveness. Finally, the \textbf{Conclusion} chapter not only provides insights gleaned from the research but also outlines prospective directions for further advancements in this critical field.

%----------------------------------------------------------------------------------------
\chapter{State of the Art} % or Background
\label{chap:background}

This chapter provides an overview of the previous works and scientific literature that led us to the implementation of fair-by-design methods. This chapter strats
with an overview on \textbf{artificial intelligence} and its application in both critical sectors and socio-technical systems, going deeply into the issues of \textbf{bias} and \textbf{fairness} in AI systems.

\section{Artificial Intelligence}
Artificial Intelligence (AI) and Machine Learning (ML) are transformative technologies that are redefining the way people interact with the world. AI refers to the development of computer systems capable of performing tasks that typically require human intelligence, such as speech recognition, problem-solving, and learning. \\
Machine Learning is a subset of AI, involving the creation of algorithms that allow computers to learn patterns and make predictions based on data, improving their performance over time. \\
AI encompasses a broad range of techniques, including natural language processing, computer vision, robotics, and expert systems. Machine Learning, on the other hand, involves algorithms that enable machines to learn and improve their performance without being explicitly programmed. ML algorithms are designed to identify patterns and make informed decisions based on data input. \\
Considering examples of AI in critical sectors such as healthcare and autonomous driving it becomes clear how AI could impact people's lives. Beyond these applications it becomes necessary to consider the applications of AI in socio-technical systems. \\
Socio-technical systems refer to complex interplays between people, technology, and social institutions. AI and ML have a profound impact on these systems, influencing how individuals interact with technology and how society functions as a whole. \\
The integration of AI and ML raises ethical questions regarding privacy, biases in algorithms, and job displacement due to automation. It is crucial to develop frameworks that ensure fairness, transparency, and accountability in AI systems to mitigate potential harm and ensure equitable benefits. \cite{GRUETZEMACHER202210288} \\ 

\section{Bias}
Artificial Intelligence (AI) holds immense promise for revolutionizing various aspects of our lives. From aiding decision-making processes to automating routine tasks, AI systems are increasingly becoming integrated into our daily lives. However, a growing concern within the AI community and society at large is the issue of biases embedded in AI algorithms. \cite{10.1145/3308560.3317590}

\subsection{Understanding Bias in AI}
Bias in AI refers to the unfair and skewed representation or treatment of individuals or groups based on factors such as race, gender, age, socioeconomic status, or other characteristics. These biases can be unintentionally perpetuated during the development and training of AI models due to historical data imbalances, societal prejudices, or flawed algorithms.

\subsection{Sources of Bias}
\begin{itemize}
    \item \textbf{Historical data}: Bias arising from historical data is a significant concern in machine learning. When models are trained on historical data, they learn the patterns and biases inherent in that data. These historical biases, often reflective of societal prejudices and inequalities, can perpetuate and reinforce unfair outcomes when the model is deployed. For instance, if historical data contains biases against a particular demographic, such as gender or race, the model will learn and replicate these biases in its predictions or decisions. This perpetuation of historical biases through machine learning systems can lead to discriminatory practices, reinforcing existing inequalities and disadvantaging certain groups. Addressing bias from historical data requires careful consideration and proactive measures, including data preprocessing techniques to identify and mitigate bias, algorithmic fairness interventions, and ongoing monitoring and adjustment of models to ensure fairness and equitable treatment for all individuals, irrespective of their backgrounds. It's crucial to strive for a future where machine learning not only learns from historical data but also actively works to break free from the shackles of bias and promote a fair and just society. \cite{10.1145/3308560.3317590}
    \item \textbf{Human bias}: Bias introduced by human factors is a pervasive challenge in machine learning. Human bias, stemming from societal, cultural, or personal beliefs and attitudes, can inadvertently permeate the entire machine learning pipeline, from data collection and annotation to model training and decision-making. Human bias can manifest in various ways, such as the biased selection of training data, subjective annotations, or implicit prejudices when designing or evaluating algorithms. When humans are involved in the decision-making process or contribute to the development of algorithms, their biases can be unintentionally embedded in the model, leading to skewed predictions and reinforcing societal inequalities. Recognizing and mitigating human bias is crucial for developing fair machine learning models. This involves raising awareness, promoting diversity and inclusion, implementing bias detection and mitigation techniques, and continually refining algorithms to ensure that the impact of human bias is minimized, and the models contribute to a more equitable and unbiased society. \cite{https://doi.org/10.1002/widm.1356}
    \item \textbf{Algorithmic bias}: Certainly! Algorithmic bias refers to the inherent biases that can be present in the design, development, and implementation of machine learning algorithms. These biases can inadvertently emerge due to a variety of factors, such as biased training data, skewed feature selection, or implicit assumptions made during algorithm development. Algorithmic bias can perpetuate and amplify existing societal prejudices and inequalities, leading to unfair and discriminatory outcomes. For example, if a model is trained on historical data that reflects societal biases, the algorithm can inadvertently learn and reinforce these biases, resulting in biased predictions and decisions. Detecting and mitigating algorithmic bias is of utmost importance in building fair and just AI systems. This involves careful scrutiny of the entire machine learning pipeline, addressing bias in training data, utilizing fairness-aware algorithms, and incorporating transparency and fairness into the decision-making process to ensure that the algorithms are equitable, unbiased, and promote fairness and equal treatment for all. \cite{10.1145/2983270}
\end{itemize}

\subsection{Example of Bias in AI}
\subsubsection{Race and Gender Bias in Facial Recognition}
Race and gender bias in facial recognition technology is a pressing and concerning issue that highlights the ethical challenges associated with AI development. Facial recognition systems, often trained on large datasets, inadvertently perpetuate biases present in these datasets, particularly biases related to race and gender. The lack of diversity in training data, predominantly skewed towards certain demographics, results in algorithmic bias, where the system may struggle to accurately recognize individuals from underrepresented racial or gender groups. Studies have shown that these systems are often more accurate for individuals with lighter skin tones compared to those with darker skin tones, demonstrating a clear racial bias. Similarly, gender recognition algorithms may exhibit inaccuracies, especially for gender-nonconforming individuals, further exacerbating biases. \cite{https://doi.org/10.5281/zenodo.4050457}

These biases have far-reaching implications. Law enforcement agencies using facial recognition may disproportionately target and misidentify individuals from minority communities, leading to wrongful arrests or increased surveillance. In commercial applications, biased facial recognition can impact hiring processes, access to services, and overall societal fairness. Addressing these biases requires comprehensive strategies, including diversifying training data to be more representative, developing bias detection and mitigation techniques, promoting interdisciplinary research, and fostering discussions around the ethical implications of facial recognition technology. Additionally, regulatory frameworks and guidelines are essential to ensure that these technologies are deployed responsibly, with transparency, accountability, and fairness in mind, to mitigate the adverse effects of bias and foster a more equitable future.

\subsubsection{Criminal Justice Bias}
Criminal justice bias is a deeply ingrained issue within the legal system that manifests through unequal treatment of individuals based on their race, socioeconomic status, gender, and other factors. The criminal justice system should ideally operate on principles of fairness, justice, and equality before the law. However, biases at various stages of the criminal justice process, from policing and arrest to trial and sentencing, often lead to discriminatory outcomes. \cite{doi:10.1080/10345329.2019.1658694}

Racial bias is a significant concern, with people of color, especially Black individuals and communities, experiencing disproportionately higher rates of arrest, harsher sentencing, and a lack of trust in the system. Discriminatory practices such as racial profiling and racial disparities in sentencing contribute to this bias. Socioeconomic bias is another critical factor, where individuals from marginalized and low-income communities may face prejudice in the form of limited access to legal resources and unequal treatment within the legal process. \cite{9660177} 

Gender bias is prevalent, particularly against women and gender-diverse individuals. Women can face stereotypes and discriminatory attitudes that affect their treatment by law enforcement, the courts, and correctional facilities. Additionally, biases against LGBTQ+ individuals can result in unfair treatment and disparities in the criminal justice system. \cite{gebru2020race}

Addressing criminal justice bias necessitates comprehensive reform. This includes implementing policies to combat racial and socioeconomic disparities, providing anti-bias training to law enforcement, encouraging diversity within the legal profession, and promoting transparency and accountability in the criminal justice process. Legislation, sentencing reform, community engagement, and the support of marginalized communities are also vital steps toward achieving a fair and impartial criminal justice system that upholds the principles of equity and justice for all.


\subsubsection{Recruitment Bias}
Recruitment bias is a critical issue within the hiring process, where unconscious or conscious prejudices and preconceived notions influence decision-making during candidate selection. It manifests in various forms, such as racial, gender, age, socio-economic, educational, or even appearance-based biases, and can significantly impact the composition of the workforce. \cite{mujtaba2019ethical}

One of the most prevalent forms of recruitment bias is racial or ethnic bias. Hiring decisions can be influenced by stereotypes, leading to the underrepresentation of certain racial or ethnic groups in the workplace. Similarly, gender bias can result in disparities in hiring and promotion opportunities, favoring one gender over another. Age bias often affects older candidates who may be overlooked in favor of younger, perceived to be more 'tech-savvy' individuals.

Educational and socio-economic biases can also seep into the hiring process, where candidates from prestigious institutions or privileged backgrounds may be given preferential treatment. Appearance-based biases, although highly unfortunate, can influence decisions, impacting individuals based on their physical attributes, such as weight, height, or even hairstyle.

Addressing recruitment bias requires a multipronged approach. Firstly, raising awareness and providing training on unconscious bias is essential for hiring teams. Implementing structured and standardized interview processes, blind recruitment techniques (removing personally identifiable information), and diverse interview panels can help mitigate biases. Moreover, organizations should focus on promoting diversity and inclusion, fostering a culture that values different perspectives and backgrounds, and monitoring and analyzing recruitment data to identify patterns of bias. Striving for fairness and inclusivity in the hiring process not only leads to a more diverse workforce but also improves organizational innovation, creativity, and overall success.

\section{Fairness}
Fairness in AI systems is a crucial and evolving area of focus within the realm of artificial intelligence. It revolves around the ethical and moral obligation to ensure that AI technologies and algorithms treat all individuals equitably and without bias or discrimination. As AI becomes increasingly integrated into various aspects of society, including decision-making processes, job recruitment, lending, and law enforcement, the importance of fairness cannot be overstated.

The concept of fairness in AI encompasses a range of factors, including the avoidance of bias based on attributes like race, gender, age, ethnicity, and socio-economic status. Bias in AI can manifest in various forms, often stemming from biased training data or inherent biases present in the algorithms themselves. Fair AI systems strive to minimize such biases to provide impartial and just outcomes for everyone involved.

Different types of fairness metrics and criteria have been proposed to assess and quantify fairness, such as disparate impact, equal opportunity, and demographic parity. Disparate impact evaluates the differential impact of an AI system on various demographic groups, while equal opportunity ensures that the probability of a positive outcome remains consistent across different groups. Demographic parity, on the other hand, focuses on proportional representation of groups in AI outcomes.

Addressing fairness in AI involves employing various techniques, including pre-processing data to reduce biases, modifying algorithms to be fairness-aware, and implementing post-processing methods to ensure equitable outcomes. Moreover, transparency and explainability in AI models are vital to understanding and addressing potential biases and fostering trust in the technology. \cite{10.1145/3457607}  \cite{10.1145/3194770.3194776}

Ethical considerations are an essential aspect of fairness in AI. Ensuring fairness requires not only technical solutions but also stakeholder involvement, diverse perspectives, and adherence to guidelines and regulations that prioritize fairness and impartiality. In summary, the pursuit of fairness in AI systems is an ongoing journey, necessitating continuous research, collaboration, and ethical vigilance to create AI technologies that uphold the principles of fairness and contribute to a more inclusive and just society.


\subsection{Fairness Techniques in AI}

\subsubsection{Pre-processing:}
Pre-processing focuses on handling the initial data before it is used to train the AI model. This stage is crucial to ensure that the data is balanced and representative of the diversity in the population. Common pre-processing techniques include oversampling, undersampling, noise removal from the data, and creating balanced synthetic datasets.

\subsubsection{In-processing:}
In-processing techniques intervene directly during the model training phase. This is a more specific approach to ensure fairness in the model. For example, regularization techniques can be used to penalize the model if it shows a discriminatory tendency based on certain sensitive features (such as gender or ethnic origin). Other techniques include modifying the cost functions or manipulating the model's predictions to ensure fair treatment across different categories.

\subsubsection{Post-processing:}
Post-processing occurs after the model has been trained and has generated predictions. This phase focuses on corrections or modifications to the model's predictions to ensure fairness. For instance, realignments or adjustments can be applied to balance predictions so that there are no unjustified disparities among demographic groups. This process may involve recalibrations or other transformations of the model's results to mitigate biases.

Implementing these techniques requires a deep understanding of the specific fairness challenges in the data and models, as well as ongoing evaluation to ensure that AI adheres to ethical standards and promotes fairness. \cite{10.1145/3616865}

\subsection{Pre-processing Techniques for Addressing Fairness in AI}

Pre-processing techniques in AI systems are a critical component of the machine learning pipeline, laying the foundation for robust and accurate model training. Pre-processing involves preparing and cleaning the raw data to ensure it is suitable for feeding into machine learning algorithms. This step is essential as the quality of input data significantly impacts the performance and effectiveness of the AI system.

The pre-processing phase encompasses a variety of operations, including data cleaning, data transformation, feature selection, and feature engineering. Data cleaning involves handling missing or erroneous data, removing duplicates, and addressing inconsistencies. Data transformation includes normalization and scaling, ensuring that features are on a consistent scale to prevent biases in model training. Feature selection involves identifying and selecting the most relevant features for the model, reducing complexity and improving efficiency. Feature engineering involves creating new features or modifying existing ones to enhance the model's ability to capture patterns and make accurate predictions.

Additionally, pre-processing techniques are crucial for handling imbalanced data, where one class significantly outnumbers the others. Techniques like oversampling, undersampling, or generating synthetic samples can help address this imbalance and improve model performance. Handling categorical data through techniques like one-hot encoding or label encoding is another vital pre-processing step to convert categorical variables into a format suitable for model training.

\subsubsection{Oversampling and Undersampling}
Oversampling and undersampling are techniques used to address class imbalance, where one class significantly outnumbers the others in a dataset. In the context of fairness, these techniques are employed to ensure that the AI model is not biased towards the majority class and that the predictions are fair and equitable for all classes, particularly when sensitive attributes like race, gender, or ethnicity are involved. \cite{9442706}
\begin{enumerate}
    \item \textbf{Oversampling}

    \begin{itemize}
        \item \textbf{Definition:} Oversampling involves increasing the number of instances in the minority class by generating synthetic samples or replicating existing ones.
        \item \textbf{Fairness Context:} Oversampling aims to boost the representation of underrepresented groups, promoting fairness and equal consideration of all groups. It prevents the model from exhibiting bias towards the majority group.
    \end{itemize}

    \item \textbf{Undersampling}

    \begin{itemize}
        \item \textbf{Definition:} Undersampling involves reducing the number of instances in the majority class by removing samples, ideally in a strategic and unbiased manner.
        \item \textbf{Fairness Context:} Undersampling can be employed to level the playing field by reducing the dominance of the majority group. This ensures that the model's predictions are not disproportionately influenced by the majority group, promoting fairness in the model's outcomes.
    \end{itemize}

\end{enumerate}

\textbf{Example:} In a credit approval scenario, if a particular group is underrepresented in the dataset, oversampling can be employed to create synthetic instances for that group, ensuring fair credit evaluations.

\subsubsection{Noise Removal}
Noise removal in the context of fairness typically refers to the process of identifying and mitigating the effects of noisy or incorrect data points that may introduce biases or distortions in the data used to train or evaluate machine learning models. In fairness considerations, noise removal plays a crucial role in ensuring that the AI system's predictions and decisions are as accurate and unbiased as possible, particularly when sensitive attributes like race, gender, or ethnicity are involved. \cite{NEURIPS2019_8d5e957f}

\textbf{Example:} In sentiment analysis, if a dataset contains biased reviews based on certain demographics, such as gender, removing these biased reviews helps in training a fair sentiment analysis model.

\subsubsection{Data Augmentation}
Data augmentation is a technique often used in machine learning and data preprocessing to artificially increase the size and diversity of a training dataset by generating new data points based on the existing ones. In the context of fairness, data augmentation can play a crucial role in addressing imbalances and biases in the data, particularly when sensitive attributes are involved. Here's how data augmentation can be applied in the context of fairness:
\begin{enumerate}
    \item \textbf{Generating Additional Data for Underrepresented Groups}
    \begin{itemize}
        \item In scenarios where certain groups or classes are underrepresented in the training data, data augmentation techniques can be used to create additional examples for those groups. \cite{sharma2020data}
    \end{itemize}
    \item \textbf{Balancing Class or Group Representation}
    \begin{itemize}
        \item Data augmentation can be employed to balance class or group representation in the training data. By creating synthetic data points for underrepresented groups, it helps ensure that the model is not biased towards majority groups.
    \end{itemize}
    \item \textbf{Feature Engineering for Fairness}
    \begin{itemize}
        \item Data augmentation can also involve feature engineering that considers sensitive attributes. For example, it can create new features that better capture the nuances and characteristics of underrepresented groups. \cite{10.14778/3461535.3463474}
    \end{itemize}
    \item \textbf{Fair Data Augmentation}
    \begin{itemize}
        \item In the fairness context, it's important to ensure that data augmentation techniques do not introduce additional biases. Care should be taken to create synthetic data that aligns with the fairness and equity goals of the AI system. \cite{10.1145/3531146.3534644}
    \end{itemize}
\end{enumerate}
\textbf{Example:} In image recognition, augmenting images by rotating or flipping them can provide a more balanced representation of different features, promoting fairness in the model's predictions.

\subsubsection{Bias Mitigation Algorithms}
Bias mitigation in the context of fairness refers to the process of identifying, reducing, or eliminating biases within machine learning models and algorithms, particularly those that could lead to unfair or discriminatory outcomes, often associated with sensitive attributes like race, gender, age, or ethnicity. The goal of bias mitigation is to ensure that AI systems provide equitable and unbiased predictions and decisions for all individuals or groups.

\textbf{Example:} When training a model for a hiring platform, using a bias mitigation algorithm to adjust the dataset to have an equal representation of genders can lead to a more fair hiring process.

\subsubsection{Sensitive Attribute Removal or Neutralization}
In some cases, sensitive attributes (e.g., race, gender) can be removed from the dataset or transformed into more neutral representations. This prevents the model from relying on these attributes to make predictions, promoting fairness. \cite{NEURIPS2021_64ff7983}

\textbf{Example:} In a loan approval system, sensitive attributes like race can be neutralized by removing them or encoding them in a way that the model cannot identify them, ensuring unbiased loan decisions.


These pre-processing techniques are essential steps in the AI development pipeline to ensure that the subsequent models are fair, unbiased, and capable of providing equitable outcomes across various demographic categories.

\subsection{In-processing Techniques for Addressing Fairness in AI}

In-processing techniques aim to mitigate fairness issues directly during the model training phase, influencing the learning process to ensure fairness in model predictions. These approaches target bias reduction and fairness promotion within the model's decision-making process. Several techniques can be employed during model training to achieve fairness:

\subsubsection{Regularization}
Regularization  aims to address and mitigate potential biases within models, particularly when sensitive attributes like race, gender, or ethnicity are involved. Regularization techniques work by adding constraints or penalties to the model's training process to reduce the impact of sensitive attributes on predictions and ensure that fairness is maintained. \cite{6137441}

\textbf{Example:} In a hiring model, a regularization term can be added to the loss function to penalize the model for making biased predictions related to gender or ethnicity.

\subsubsection{Reweighting Training Samples}
Reweighting training samples is a technique used to address bias and promote fairness in machine learning models, particularly when sensitive attributes are involved. This approach involves assigning different weights to training samples to influence the learning process of the model in a way that mitigates bias and ensures that predictions are more equitable. \cite{10.1145/3178876.3186133}

\textbf{Example:} In a credit scoring model, training samples from underrepresented demographics can be assigned higher weights to ensure their impact on model training is more significant.

\subsubsection{Prejudice Remover Regularizer}
The Prejudice Remover Regularizer is a technique used to mitigate bias and promote equitable outcomes in machine learning models. It's a form of regularization that aims to reduce discrimination by encouraging the model to make predictions that are less influenced by sensitive attributes such as race, gender, or ethnicity. \cite{10.1007/978-3-642-33486-3_3}

\textbf{Example:} In a recommendation system, the prejudice remover regularizer can help ensure that recommendations are not influenced by the gender of the user.

\subsubsection{Demographic Parity Loss}
The Demographic Parity Loss is a fairness metric and regularization technique used to promote fairness and reduce bias, particularly with regard to sensitive attributes like race, gender, or ethnicity. It is designed to ensure that the predictions made by a model are distributed equally or fairly across different demographic groups. \cite{jiang2022generalized}

\textbf{Example:} In a loan approval system, the demographic parity loss can be added to the loss function to minimize disparities in approval rates between different ethnic groups.

\subsubsection{Fair Adversarial Training}
air adversarial training is a technique used in the context of fairness to reduce bias and discrimination in machine learning models, particularly when sensitive attributes like race, gender, or ethnicity are involved. This approach incorporates adversarial networks into the training process to promote fairness and equitable outcomes. \cite{pmlr-v139-xu21b}

\textbf{Example:} In a criminal recidivism prediction model, fair adversarial training can help mitigate the biased prediction rates for different racial groups.

These in-processing techniques are vital tools in promoting fairness within AI models. Integrating them appropriately during model training can significantly contribute to reducing biases and achieving equitable outcomes across various demographic categories.

\subsection{Post-processing Techniques for Addressing Fairness in AI}

Post-processing techniques are applied after the model has been trained and predictions have been generated. Their purpose is to rectify any biases or disparities in the model's outputs and ensure fairness in the final outcomes. Several techniques can be employed during post-processing to promote fairness:

\subsubsection{Threshold Adjustments}
Threshold adjustment is a technique used to promote equity and reduce bias in machine learning models, especially in scenarios where sensitive attributes like race, gender, or age play a significant role. It involves modifying the decision threshold that determines whether a model's output is classified as a positive or negative prediction. This adjustment aims to balance the rates of false positives and false negatives across different demographic or group categories, ensuring that all groups are treated more fairly. \cite{10.1145/3447548.3467251}

\textbf{Example:} In a predictive policing model, different arrest thresholds can be applied to balance false arrest rates between different neighborhoods, ensuring fair law enforcement practices.

\subsubsection{Additive Counterfactuals}
Additive counterfactual explanations in the context of fairness refer to a method used to assess and promote fairness in machine learning models. Counterfactual explanations are designed to provide insights into the impact of sensitive attributes on model predictions and help identify potential bias or discrimination. The "additive" aspect suggests that changes are made to the original input to create counterfactual scenarios, allowing for a better understanding of the fairness implications. \cite{NIPS2017_a486cd07}

\textbf{Example:} In a loan approval system, generating counterfactuals can show rejected applicants how their outcomes would have differed with a fairer decision-making process.

\subsubsection{Equalized Odds Post-processing}
Equalized Odds Post-processing is a technique used to mitigate bias and promote equal treatment in machine learning models, particularly in scenarios involving binary classification tasks. This technique is applied after a model has made predictions and aims to adjust those predictions to ensure that equal error rates are achieved across different demographic or group categories. \cite{10.1145/3442188.3445902}

\textbf{Example:} In a job application screening model, the predicted probabilities of being shortlisted can be equalized for candidates from different age groups to minimize age-based discrimination.


These post-processing techniques are crucial in rectifying biases and promoting fairness in AI models. Utilizing them effectively can lead to more equitable outcomes and decisions across various demographic categories.

In conclusion, fairness in AI is not just an ethical necessity but a fundamental requirement for fostering trust and ensuring the responsible and equitable deployment of AI systems. Moving into the evolving landscape of AI, the collective efforts must prioritize fairness to harness the true potential of AI for the greater good.
%Write background here.

%This section is likely to contain a lot of citations.
%
%For instance in \cite{AnzengruberSocInfo2013} the authors propose a novel means for tackling with the problem of preventing bad things from happening.

%----------------------------------------------------------------------------------------
\chapter{Contribution} % possible chapter for Projects
\label{chap:contibution}
%----------------------------------------------------------------------------------------
\section{Introduction to the Fair-by-Design Workflow}
The following chapter unveils the core of the researchâ€”an innovative Fair-by-Design workflow meticulously crafted to address the pressing issue of bias mitigation within AI systems. It is important to note that, among all the techniques presented in the "State of the Art" chapter, this workflow primarily focuses on the concepts of pre-processing techniques for bias mitigation. \\
The Fair-by-Design concept underlines the commitment to infusing fairness principles into the very fabric of AI development. It signifies the endeavor to create AI systems that are not only technologically proficient but also ethically responsible. \\
The Fair-by-Design workflow serves as the linchpin of this research. It guides the approach to designing and developing AI systems that are inherently fair, impartial, and equitable. The challenge of bias mitigation cannot be relegated to a mere post hoc consideration; it must be foundational. By introducing fairness considerations at the outset of the AI system design, the aim is to rectify biases before they become ingrained in decision-making processes. \\
In the pages that follow, the intricacies of this workflow will be explored, providing a step-by-step elucidation of its various components and how they collectively contribute to the realization of a fair and unbiased AI system. This workflow will not only be a central theme throughout this chapter but also a thread that ties the entire thesis together. It signifies the commitment to addressing bias and promoting fairness within AI systems, particularly in the context of the educational domain. \\
The journey will embark on a comprehensive exploration of the Fair-by-Design workflow, with the ultimate goal of mitigating bias, ensuring fairness, and achieving equitable outcomes in AI-driven decision-making.

%----------------------------------------------------------------------------------------
\chapter{Validation} % possible chapter for Projects
\label{chap:validation}
%----------------------------------------------------------------------------------------
For each grade have been applied the following approaches, in order to perform a proper comparaison:
\begin{enumerate}
    \item No fairness assumptions are made
    \item Fairness through unawareness
    \item Fairness through unawareness with proxy detection via apriori
    \item Fairness through unawareness with proxy detection via variables only
    \item Fairness through data rebalancing
\end{enumerate}

\section{Experiment setup}
For each of the previous approaches have been chosen 3 models to perform the prediction:
\begin{enumerate}
    \item RandomForest Classifier
    \item XGBoost Classifier
    \item DecisionTree Classifier
\end{enumerate}

On the previous models is applied the GridSearch with a cv of 10 in order to select the best parameters combination for each one. In the following are reported the specific parameters for each model:
\begin{enumerate}
    \item RandomForest Classifier:
    \begin{enumerate}
        \item \emph{n\textunderscore estimators}: [10, 100, 10]
        \item \emph{criterion}: [gini, entropy, log\textunderscore loss]
        \item \emph{max\textunderscore depth}: [10, 50, 10]
        \item \emph{max\textunderscore leaf\textunderscore nodes}: [10, 50, 10]
    \end{enumerate}
    \item XGBoost Classifier:
    \begin{enumerate}
        \item \emph{min\textunderscore child\textunderscore weight}: [1, 5, 10]
        \item \emph{gamma}: [0.5, 1, 1.5, 2.5]
        \item \emph{subsample}: [0.6, 0.8, 1.0]
        \item \emph{colsample\textunderscore bytree}: [0.6, 0.6, 1.0]
        \item \emph{max\textunderscore depth}: [3, 4, 5]
    \end{enumerate}
    \item DecisionTree Classifier:
    \begin{enumerate}
        \item \emph{criterion}: [gini, entropy, log\textunderscore loss]
        \item \emph{max\textunderscore depth}: [10, 50, 10]
        \item \emph{max\textunderscore leaf\textunderscore nodes}: [10, 50, 10]
    \end{enumerate}
\end{enumerate}

Each model is trained with the 67\% of the dataset while the 33\% is left for the test.
In the following are reported the results for each grade. More specifically for each model is reported the best model selected and the accuracy on the test set.

\section{Grade 3}
\subsection{No-fair approach}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fairness throguh unawareness}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fairness through unawareness with proxy detection via apriori}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fariness through unawareness with proxy detection via variables only}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fairness through data rebalancing}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}


\section{Grade 4}
\subsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}
\section{Grade 6}
\subsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

%----------------------------------------------------------------------------------------
\chapter{Conclusion}
\label{chap:conclusions}
%----------------------------------------------------------------------------------------

Write conclusions here.


%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

%\nocite{*} % comment this to only show the referenced entries from the .bib file

\bibliographystyle{alpha}
\bibliography{bibliography}


\end{document}