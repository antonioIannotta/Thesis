\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{listings}
\usepackage{algpseudocode}
\usepackage{tabularx}
\usepackage{graphicx}
\usepackage{disi-thesis}
\usepackage{code-lstlistings}
\usepackage{notes}
\usepackage{shortcuts}
\usepackage{acronym}
\usepackage[linesnumbered,ruled,vlined]{algorithm2e}


%\newcommand{\thesislang}{english} % commentare in caso di tesi in italiano
%\usepackage{thesis-style}
% version
%\newcommand{\versionmajor}{0}
%\newcommand{\versionminor}{1}
%\newcommand{\versionpatch}{2}
%\newcommand{\version}{\versionmajor.\versionminor.\versionpatch}
%\typeout{Document version: \version}

\school{\unibo}
\programme{Corso di Laurea [Magistrale?] in Ingegneria e Scienze Informatiche}
\title{Fancy Title}
\author{Candidate Name}
\date{\today}
\subject{Supervisor's course name}
\supervisor{Prof. Supervisor Here}
\cosupervisor{Dott. CoSupervisor 1}
\morecosupervisor{Dott. CoSupervisor 2}
\session{I}
\academicyear{2022-2023}

% Definition of acronyms
\acrodef{IoT}{Internet of Thing}
\acrodef{vm}[VM]{Virtual Machine}


\mainlinespacing{1.241} % line spacing in mainmatter, comment to default (1)

\begin{document}
	
\frontmatter

\input{front.tex}

\begin{abstract}	
Nowdays a lot of companies and social entities are wondering about the implications of Artificial Intelligence usages in the daily life of people, with a 
huge interests for the implications in discriminations that may occour using these systems. \\
A lot of methodologies in several fields like computer science, statistics and social sciences have been formalized in order to 
afford the problem of the fairness in social systems and this problem is being dealt with during the development of the modern AI systems. \\
In this work we propose several methods that have been developed considering fairness from the design phase, that's why they are called \emph{fair-by-design-method}
In order to develop these methods we used well-known Python libraries such as \emph{Pandas}, \emph{Sci-kit-learn} and other libraries implemented by ourselves.
We begun our work with mitigation techniques in fairness, also known as pre-processing techniques.
Our work has been evaluated exploiting a real-world dataset on the education on Canary Island. So we compared the results of the prediction of a certain result for students applying fair-by-design methods and 
the prediction obtained without consider fairness.
\end{abstract}


\begin{acknowledgements} % this is optional
Never too far down, to come back
\end{acknowledgements}

%----------------------------------------------------------------------------------------
\tableofcontents   
%\listoffigures     % (optional) comment if empty
%\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{Introduction}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

In recent years, there has been a notable increase in the utilization of artificial intelligence (AI) systems across various sectors. This rise is attributed to advancements in technology, the accessibility of extensive data, and heightened computational capabilities. AI has transcended its role as a mere performance enhancer and has begun exerting a profound impact on social contexts. These systems are now pervasive in healthcare, education, criminal justice, and other crucial domains, fundamentally reshaping decision-making processes and societal interactions. However, the reliance of AI systems on data for learning and decision-making implies that they inherit the biases embedded in these data. This inherent bias has the potential to perpetuate existing societal imbalances and prejudices. Addressing bias and ensuring fairness in AI algorithms is hence a pressing concern, necessitating continuous research, responsible development practices, and vigilant oversight to guarantee equitable outcomes for all. In this discussion, we will delve into the intricate dynamics of biases and fairness in AI systems, exploring viable strategies to effectively mitigate these challenges.

\paragraph{Artificial Intelligence}
In recent years, artificial intelligence (AI) has undergone a remarkable evolution, transforming from a theoretical concept to a pervasive force shaping various facets of our lives. Advances in machine learning, particularly deep learning, have fueled this growth, enabling AI systems to process vast amounts of data and extract meaningful insights. Notably, breakthroughs in natural language processing have led to more conversational AI, making interactions with machines increasingly human-like. Moreover, AI applications have expanded across sectors, encompassing healthcare, finance, transportation, and more. However, ethical considerations and the responsible development of AI have gained significant traction, prompting discussions on privacy, bias mitigation, and ensuring that AI benefits all of society. The future of AI promises continued innovation, fostering a symbiotic relationship between humans and machines for a more technologically enriched world.
\paragraph{Bias}
In the realm of artificial intelligence, the issue of bias is a critical concern. AI systems are trained on historical data, and if this data carries biases related to gender, race, or other factors, the AI can inadvertently perpetuate and amplify these biases in its predictions or decisions. Addressing bias in AI necessitates a proactive approach, from carefully curating training data to developing algorithms that are designed to detect and mitigate bias. Furthermore, promoting diversity and inclusivity in the development teams is vital to ensuring that biases are recognized and addressed effectively.
\paragraph{Fairness}
Ensuring fairness in AI is an ongoing and essential goal. It involves designing and implementing AI systems in a way that does not discriminate against individuals or groups based on characteristics like race, gender, or socioeconomic status. Achieving fairness demands a multidimensional approach, including a clear definition of fairness metrics, active involvement of diverse stakeholders, and regular audits of AI systems to detect and rectify any unfair biases. Additionally, incorporating transparency and accountability into the AI development process is crucial for building trust and ensuring fair outcomes for all users.
\paragraph{Machine Learning and data}
! Machine learning algorithms heavily rely on data to make predictions and decisions. However, the quality and representativeness of the data used are critical factors, especially in the context of fairness. Biases and unfairness can be inadvertently ingrained in machine learning models if the training data is skewed or reflects historical biases present in society. The data source plays a pivotal role in fairness issues, as biases present in the data, whether due to societal, cultural, or historical factors, can perpetuate and reinforce discriminatory outcomes. If the training data disproportionately represents a certain group, the model may learn to favor that group, leading to biased predictions and decisions. It is imperative to carefully curate and preprocess data, strive for diversity and representativeness, and continually evaluate and mitigate biases to promote fairness and ensure equitable outcomes in machine learning applications.
\paragraph{Fair-by-design method}
Since the huge impact that AI systems are having on daily life and more in general on the whole socio-technical systems it becomes crucial to design new AI methods and model that take account of fairness since the design step. That's why we call them fair-by-design methods.
%----------------------------------------------------------------------------------------
\chapter{State of the Art} % or Background
\label{chap:background}
%----------------------------------------------------------------------------------------

This chapter provides an overview of the previous works and scientific literature that led us to the implementation of fair-by-design methods. This chapter strats
with an overview on \textbf{artificial intelligence} and its application in both critical sectors and socio-technical systems. Then we go deeply into the issues of \textbf{bias} and \textbf{fairness} in AI systems.

\section{Artificial Intelligence}
Artificial Intelligence (AI) and Machine Learning (ML) are transformative technologies that are redefining the way we interact with the world. AI refers to the development of computer systems capable of performing tasks that typically require human intelligence, such as speech recognition, problem-solving, and learning. Machine Learning is a subset of AI, involving the creation of algorithms that allow computers to learn patterns and make predictions based on data, improving their performance over time. \\
AI encompasses a broad range of techniques, including natural language processing, computer vision, robotics, and expert systems. Machine Learning, on the other hand, involves algorithms that enable machines to learn and improve their performance without being explicitly programmed. ML algorithms are designed to identify patterns and make informed decisions based on data input. \\
If we consider examples of AI in critical sectors such as healthcare and autonomous driving it becomes clear how AI could impact our lives. Beyond these applications it becomes necessary to consider the applications of AI in socio-technical systems. \\
Socio-technical systems refer to complex interplays between people, technology, and social institutions. AI and ML have a profound impact on these systems, influencing how individuals interact with technology and how society functions as a whole. \\
The integration of AI and ML raises ethical questions regarding privacy, biases in algorithms, and job displacement due to automation. It is crucial to develop frameworks that ensure fairness, transparency, and accountability in AI systems to mitigate potential harm and ensure equitable benefits. \\

\section{Bias}
Artificial Intelligence (AI) holds immense promise for revolutionizing various aspects of our lives. From aiding decision-making processes to automating routine tasks, AI systems are increasingly becoming integrated into our daily lives. However, a growing concern within the AI community and society at large is the issue of biases embedded in AI algorithms.

\subsection{Understanding Bias in AI}
Bias in AI refers to the unfair and skewed representation or treatment of individuals or groups based on factors such as race, gender, age, socioeconomic status, or other characteristics. These biases can be unintentionally perpetuated during the development and training of AI models due to historical data imbalances, societal prejudices, or flawed algorithms.

\subsection{Sources of Bias}
\begin{itemize}
    \item \textbf{Historical data}: Bias arising from historical data is a significant concern in machine learning. When models are trained on historical data, they learn the patterns and biases inherent in that data. These historical biases, often reflective of societal prejudices and inequalities, can perpetuate and reinforce unfair outcomes when the model is deployed. For instance, if historical data contains biases against a particular demographic, such as gender or race, the model will learn and replicate these biases in its predictions or decisions. This perpetuation of historical biases through machine learning systems can lead to discriminatory practices, reinforcing existing inequalities and disadvantaging certain groups. Addressing bias from historical data requires careful consideration and proactive measures, including data preprocessing techniques to identify and mitigate bias, algorithmic fairness interventions, and ongoing monitoring and adjustment of models to ensure fairness and equitable treatment for all individuals, irrespective of their backgrounds. It's crucial to strive for a future where machine learning not only learns from historical data but also actively works to break free from the shackles of bias and promote a fair and just society.
    \item \textbf{Human bias}: Bias introduced by human factors is a pervasive challenge in machine learning. Human bias, stemming from societal, cultural, or personal beliefs and attitudes, can inadvertently permeate the entire machine learning pipeline, from data collection and annotation to model training and decision-making. Human bias can manifest in various ways, such as the biased selection of training data, subjective annotations, or implicit prejudices when designing or evaluating algorithms. When humans are involved in the decision-making process or contribute to the development of algorithms, their biases can be unintentionally embedded in the model, leading to skewed predictions and reinforcing societal inequalities. Recognizing and mitigating human bias is crucial for developing fair machine learning models. This involves raising awareness, promoting diversity and inclusion, implementing bias detection and mitigation techniques, and continually refining algorithms to ensure that the impact of human bias is minimized, and the models contribute to a more equitable and unbiased society.
    \item \textbf{Algorithmic bias}: Certainly! Algorithmic bias refers to the inherent biases that can be present in the design, development, and implementation of machine learning algorithms. These biases can inadvertently emerge due to a variety of factors, such as biased training data, skewed feature selection, or implicit assumptions made during algorithm development. Algorithmic bias can perpetuate and amplify existing societal prejudices and inequalities, leading to unfair and discriminatory outcomes. For example, if a model is trained on historical data that reflects societal biases, the algorithm can inadvertently learn and reinforce these biases, resulting in biased predictions and decisions. Detecting and mitigating algorithmic bias is of utmost importance in building fair and just AI systems. This involves careful scrutiny of the entire machine learning pipeline, addressing bias in training data, utilizing fairness-aware algorithms, and incorporating transparency and fairness into the decision-making process to ensure that the algorithms are equitable, unbiased, and promote fairness and equal treatment for all.
\end{itemize}

\subsection{Example of Bias in AI}
\subsubsection{Race and Gender Bias in Facial Recognition}
Race and gender bias in facial recognition technology is a pressing and concerning issue that highlights the ethical challenges associated with AI development. Facial recognition systems, often trained on large datasets, inadvertently perpetuate biases present in these datasets, particularly biases related to race and gender. The lack of diversity in training data, predominantly skewed towards certain demographics, results in algorithmic bias, where the system may struggle to accurately recognize individuals from underrepresented racial or gender groups. Studies have shown that these systems are often more accurate for individuals with lighter skin tones compared to those with darker skin tones, demonstrating a clear racial bias. Similarly, gender recognition algorithms may exhibit inaccuracies, especially for gender-nonconforming individuals, further exacerbating biases.

These biases have far-reaching implications. Law enforcement agencies using facial recognition may disproportionately target and misidentify individuals from minority communities, leading to wrongful arrests or increased surveillance. In commercial applications, biased facial recognition can impact hiring processes, access to services, and overall societal fairness. Addressing these biases requires comprehensive strategies, including diversifying training data to be more representative, developing bias detection and mitigation techniques, promoting interdisciplinary research, and fostering discussions around the ethical implications of facial recognition technology. Additionally, regulatory frameworks and guidelines are essential to ensure that these technologies are deployed responsibly, with transparency, accountability, and fairness in mind, to mitigate the adverse effects of bias and foster a more equitable future.

\subsubsection{Criminal Justice Bias}
Criminal justice bias is a deeply ingrained issue within the legal system that manifests through unequal treatment of individuals based on their race, socioeconomic status, gender, and other factors. The criminal justice system should ideally operate on principles of fairness, justice, and equality before the law. However, biases at various stages of the criminal justice process, from policing and arrest to trial and sentencing, often lead to discriminatory outcomes.

Racial bias is a significant concern, with people of color, especially Black individuals and communities, experiencing disproportionately higher rates of arrest, harsher sentencing, and a lack of trust in the system. Discriminatory practices such as racial profiling and racial disparities in sentencing contribute to this bias. Socioeconomic bias is another critical factor, where individuals from marginalized and low-income communities may face prejudice in the form of limited access to legal resources and unequal treatment within the legal process.

Gender bias is prevalent, particularly against women and gender-diverse individuals. Women can face stereotypes and discriminatory attitudes that affect their treatment by law enforcement, the courts, and correctional facilities. Additionally, biases against LGBTQ+ individuals can result in unfair treatment and disparities in the criminal justice system.

Addressing criminal justice bias necessitates comprehensive reform. This includes implementing policies to combat racial and socioeconomic disparities, providing anti-bias training to law enforcement, encouraging diversity within the legal profession, and promoting transparency and accountability in the criminal justice process. Legislation, sentencing reform, community engagement, and the support of marginalized communities are also vital steps toward achieving a fair and impartial criminal justice system that upholds the principles of equity and justice for all.

\subsubsection{Recruitment Bias}
Recruitment bias is a critical issue within the hiring process, where unconscious or conscious prejudices and preconceived notions influence decision-making during candidate selection. It manifests in various forms, such as racial, gender, age, socio-economic, educational, or even appearance-based biases, and can significantly impact the composition of the workforce.

One of the most prevalent forms of recruitment bias is racial or ethnic bias. Hiring decisions can be influenced by stereotypes, leading to the underrepresentation of certain racial or ethnic groups in the workplace. Similarly, gender bias can result in disparities in hiring and promotion opportunities, favoring one gender over another. Age bias often affects older candidates who may be overlooked in favor of younger, perceived to be more 'tech-savvy' individuals.

Educational and socio-economic biases can also seep into the hiring process, where candidates from prestigious institutions or privileged backgrounds may be given preferential treatment. Appearance-based biases, although highly unfortunate, can influence decisions, impacting individuals based on their physical attributes, such as weight, height, or even hairstyle.

Addressing recruitment bias requires a multipronged approach. Firstly, raising awareness and providing training on unconscious bias is essential for hiring teams. Implementing structured and standardized interview processes, blind recruitment techniques (removing personally identifiable information), and diverse interview panels can help mitigate biases. Moreover, organizations should focus on promoting diversity and inclusion, fostering a culture that values different perspectives and backgrounds, and monitoring and analyzing recruitment data to identify patterns of bias. Striving for fairness and inclusivity in the hiring process not only leads to a more diverse workforce but also improves organizational innovation, creativity, and overall success.
\section{Fairness}
Fairness in AI systems is a crucial and evolving area of focus within the realm of artificial intelligence. It revolves around the ethical and moral obligation to ensure that AI technologies and algorithms treat all individuals equitably and without bias or discrimination. As AI becomes increasingly integrated into various aspects of society, including decision-making processes, job recruitment, lending, and law enforcement, the importance of fairness cannot be overstated.

The concept of fairness in AI encompasses a range of factors, including the avoidance of bias based on attributes like race, gender, age, ethnicity, and socio-economic status. Bias in AI can manifest in various forms, often stemming from biased training data or inherent biases present in the algorithms themselves. Fair AI systems strive to minimize such biases to provide impartial and just outcomes for everyone involved.

Different types of fairness metrics and criteria have been proposed to assess and quantify fairness, such as disparate impact, equal opportunity, and demographic parity. Disparate impact evaluates the differential impact of an AI system on various demographic groups, while equal opportunity ensures that the probability of a positive outcome remains consistent across different groups. Demographic parity, on the other hand, focuses on proportional representation of groups in AI outcomes.

Addressing fairness in AI involves employing various techniques, including pre-processing data to reduce biases, modifying algorithms to be fairness-aware, and implementing post-processing methods to ensure equitable outcomes. Moreover, transparency and explainability in AI models are vital to understanding and addressing potential biases and fostering trust in the technology.

Ethical considerations are an essential aspect of fairness in AI. Ensuring fairness requires not only technical solutions but also stakeholder involvement, diverse perspectives, and adherence to guidelines and regulations that prioritize fairness and impartiality. In summary, the pursuit of fairness in AI systems is an ongoing journey, necessitating continuous research, collaboration, and ethical vigilance to create AI technologies that uphold the principles of fairness and contribute to a more inclusive and just society.
\subsection{Fairness Techniques in AI}

\subsubsection{Pre-processing:}
Pre-processing focuses on handling the initial data before it is used to train the AI model. This stage is crucial to ensure that the data is balanced and representative of the diversity in the population. Common pre-processing techniques include oversampling, undersampling, noise removal from the data, and creating balanced synthetic datasets.

\subsubsection{In-processing:}
In-processing techniques intervene directly during the model training phase. This is a more specific approach to ensure fairness in the model. For example, regularization techniques can be used to penalize the model if it shows a discriminatory tendency based on certain sensitive features (such as gender or ethnic origin). Other techniques include modifying the cost functions or manipulating the model's predictions to ensure fair treatment across different categories.

\subsubsection{Post-processing:}
Post-processing occurs after the model has been trained and has generated predictions. This phase focuses on corrections or modifications to the model's predictions to ensure fairness. For instance, realignments or adjustments can be applied to balance predictions so that there are no unjustified disparities among demographic groups. This process may involve recalibrations or other transformations of the model's results to mitigate biases.

Implementing these techniques requires a deep understanding of the specific fairness challenges in the data and models, as well as ongoing evaluation to ensure that AI adheres to ethical standards and promotes fairness.

\subsection{Pre-processing Techniques for Addressing Fairness in AI}

Pre-processing techniques in AI systems are a critical component of the machine learning pipeline, laying the foundation for robust and accurate model training. Pre-processing involves preparing and cleaning the raw data to ensure it is suitable for feeding into machine learning algorithms. This step is essential as the quality of input data significantly impacts the performance and effectiveness of the AI system.

The pre-processing phase encompasses a variety of operations, including data cleaning, data transformation, feature selection, and feature engineering. Data cleaning involves handling missing or erroneous data, removing duplicates, and addressing inconsistencies. Data transformation includes normalization and scaling, ensuring that features are on a consistent scale to prevent biases in model training. Feature selection involves identifying and selecting the most relevant features for the model, reducing complexity and improving efficiency. Feature engineering involves creating new features or modifying existing ones to enhance the model's ability to capture patterns and make accurate predictions.

Additionally, pre-processing techniques are crucial for handling imbalanced data, where one class significantly outnumbers the others. Techniques like oversampling, undersampling, or generating synthetic samples can help address this imbalance and improve model performance. Handling categorical data through techniques like one-hot encoding or label encoding is another vital pre-processing step to convert categorical variables into a format suitable for model training.

\subsubsection{Oversampling and Undersampling}
Oversampling and undersampling are techniques used to address class imbalance, where one class significantly outnumbers the others in a dataset. In the context of fairness, these techniques are employed to ensure that the AI model is not biased towards the majority class and that the predictions are fair and equitable for all classes, particularly when sensitive attributes like race, gender, or ethnicity are involved.
\begin{enumerate}
    \item \textbf{Oversampling}

    \begin{itemize}
        \item \textbf{Definition:} Oversampling involves increasing the number of instances in the minority class by generating synthetic samples or replicating existing ones.
        \item \textbf{Fairness Context:} Oversampling aims to boost the representation of underrepresented groups, promoting fairness and equal consideration of all groups. It prevents the model from exhibiting bias towards the majority group.
    \end{itemize}

    \item \textbf{Undersampling}

    \begin{itemize}
        \item \textbf{Definition:} Undersampling involves reducing the number of instances in the majority class by removing samples, ideally in a strategic and unbiased manner.
        \item \textbf{Fairness Context:} Undersampling can be employed to level the playing field by reducing the dominance of the majority group. This ensures that the model's predictions are not disproportionately influenced by the majority group, promoting fairness in the model's outcomes.
    \end{itemize}

\end{enumerate}

\textbf{Example:} In a credit approval scenario, if a particular group is underrepresented in the dataset, oversampling can be employed to create synthetic instances for that group, ensuring fair credit evaluations.

%%%% CONTINUE FROM HERE
\subsubsection{Noise Removal}
Noise removal in the context of fairness typically refers to the process of identifying and mitigating the effects of noisy or incorrect data points that may introduce biases or distortions in the data used to train or evaluate machine learning models. In fairness considerations, noise removal plays a crucial role in ensuring that the AI system's predictions and decisions are as accurate and unbiased as possible, particularly when sensitive attributes like race, gender, or ethnicity are involved.

\textbf{Example:} In sentiment analysis, if a dataset contains biased reviews based on certain demographics, such as gender, removing these biased reviews helps in training a fair sentiment analysis model.

\subsubsection{Data Augmentation}
Data augmentation is a technique often used in machine learning and data preprocessing to artificially increase the size and diversity of a training dataset by generating new data points based on the existing ones. In the context of fairness, data augmentation can play a crucial role in addressing imbalances and biases in the data, particularly when sensitive attributes are involved. Here's how data augmentation can be applied in the context of fairness:
\begin{enumerate}
    \item \textbf{Generating Additional Data for Underrepresented Groups}
    \begin{itemize}
        \item In scenarios where certain groups or classes are underrepresented in the training data, data augmentation techniques can be used to create additional examples for those groups.
    \end{itemize}
    \item \textbf{Balancing Class or Group Representation}
    \begin{itemize}
        \item Data augmentation can be employed to balance class or group representation in the training data. By creating synthetic data points for underrepresented groups, it helps ensure that the model is not biased towards majority groups.
    \end{itemize}
    \item \textbf{Feature Engineering for Fairness}
    \begin{itemize}
        \item Data augmentation can also involve feature engineering that considers sensitive attributes. For example, it can create new features that better capture the nuances and characteristics of underrepresented groups.
    \end{itemize}
    \item \textbf{Fair Data Augmentation}
    \begin{itemize}
        \item In the fairness context, it's important to ensure that data augmentation techniques do not introduce additional biases. Care should be taken to create synthetic data that aligns with the fairness and equity goals of the AI system.
    \end{itemize}
\end{enumerate}
\textbf{Example:} In image recognition, augmenting images by rotating or flipping them can provide a more balanced representation of different features, promoting fairness in the model's predictions.

\subsubsection{Bias Mitigation Algorithms}
Bias mitigation in the context of fairness refers to the process of identifying, reducing, or eliminating biases within machine learning models and algorithms, particularly those that could lead to unfair or discriminatory outcomes, often associated with sensitive attributes like race, gender, age, or ethnicity. The goal of bias mitigation is to ensure that AI systems provide equitable and unbiased predictions and decisions for all individuals or groups.

\textbf{Example:} When training a model for a hiring platform, using a bias mitigation algorithm to adjust the dataset to have an equal representation of genders can lead to a more fair hiring process.

\subsubsection{Sensitive Attribute Removal or Neutralization}
In some cases, sensitive attributes (e.g., race, gender) can be removed from the dataset or transformed into more neutral representations. This prevents the model from relying on these attributes to make predictions, promoting fairness.

\textbf{Example:} In a loan approval system, sensitive attributes like race can be neutralized by removing them or encoding them in a way that the model cannot identify them, ensuring unbiased loan decisions.


These pre-processing techniques are essential steps in the AI development pipeline to ensure that the subsequent models are fair, unbiased, and capable of providing equitable outcomes across various demographic categories.

\subsection{In-processing Techniques for Addressing Fairness in AI}

In-processing techniques aim to mitigate fairness issues directly during the model training phase, influencing the learning process to ensure fairness in model predictions. These approaches target bias reduction and fairness promotion within the model's decision-making process. Several techniques can be employed during model training to achieve fairness:

\subsubsection{Regularization}
Regularization  aims to address and mitigate potential biases within models, particularly when sensitive attributes like race, gender, or ethnicity are involved. Regularization techniques work by adding constraints or penalties to the model's training process to reduce the impact of sensitive attributes on predictions and ensure that fairness is maintained.

\textbf{Example:} In a hiring model, a regularization term can be added to the loss function to penalize the model for making biased predictions related to gender or ethnicity.

\subsubsection{Reweighting Training Samples}
Reweighting training samples is a technique used to address bias and promote fairness in machine learning models, particularly when sensitive attributes are involved. This approach involves assigning different weights to training samples to influence the learning process of the model in a way that mitigates bias and ensures that predictions are more equitable.

\textbf{Example:} In a credit scoring model, training samples from underrepresented demographics can be assigned higher weights to ensure their impact on model training is more significant.

\subsubsection{Prejudice Remover Regularizer}
The Prejudice Remover Regularizer is a technique used to mitigate bias and promote equitable outcomes in machine learning models. It's a form of regularization that aims to reduce discrimination by encouraging the model to make predictions that are less influenced by sensitive attributes such as race, gender, or ethnicity.

\textbf{Example:} In a recommendation system, the prejudice remover regularizer can help ensure that recommendations are not influenced by the gender of the user.

\subsubsection{Demographic Parity Loss}
The Demographic Parity Loss is a fairness metric and regularization technique used to promote fairness and reduce bias, particularly with regard to sensitive attributes like race, gender, or ethnicity. It is designed to ensure that the predictions made by a model are distributed equally or fairly across different demographic groups.

\textbf{Example:} In a loan approval system, the demographic parity loss can be added to the loss function to minimize disparities in approval rates between different ethnic groups.

\subsubsection{Fair Adversarial Training}
air adversarial training is a technique used in the context of fairness to reduce bias and discrimination in machine learning models, particularly when sensitive attributes like race, gender, or ethnicity are involved. This approach incorporates adversarial networks into the training process to promote fairness and equitable outcomes.

\textbf{Example:} In a criminal recidivism prediction model, fair adversarial training can help mitigate the biased prediction rates for different racial groups.

These in-processing techniques are vital tools in promoting fairness within AI models. Integrating them appropriately during model training can significantly contribute to reducing biases and achieving equitable outcomes across various demographic categories.

\subsection{Post-processing Techniques for Addressing Fairness in AI}

Post-processing techniques are applied after the model has been trained and predictions have been generated. Their purpose is to rectify any biases or disparities in the model's outputs and ensure fairness in the final outcomes. Several techniques can be employed during post-processing to promote fairness:

\subsubsection{Re-ranking or Re-scoring}
Re-ranking or re-scoring is a technique used to adjust the order or scores of items or candidates generated by a machine learning model to promote fairness and reduce bias in the outcomes. This technique is often applied in various applications, including search engines, recommendation systems, and ranking algorithms, where the order of items or candidates matters and where bias or discrimination may be a concern.

\textbf{Example:} In a college admissions system, the initial ranking of applicants can be adjusted to ensure a fair representation of students from diverse socioeconomic backgrounds.

\subsubsection{Threshold Adjustments}
Threshold adjustment is a technique used to promote equity and reduce bias in machine learning models, especially in scenarios where sensitive attributes like race, gender, or age play a significant role. It involves modifying the decision threshold that determines whether a model's output is classified as a positive or negative prediction. This adjustment aims to balance the rates of false positives and false negatives across different demographic or group categories, ensuring that all groups are treated more fairly.

\textbf{Example:} In a predictive policing model, different arrest thresholds can be applied to balance false arrest rates between different neighborhoods, ensuring fair law enforcement practices.

\subsubsection{Additive Counterfactuals}
Additive counterfactual explanations in the context of fairness refer to a method used to assess and promote fairness in machine learning models. Counterfactual explanations are designed to provide insights into the impact of sensitive attributes on model predictions and help identify potential bias or discrimination. The "additive" aspect suggests that changes are made to the original input to create counterfactual scenarios, allowing for a better understanding of the fairness implications.

\textbf{Example:} In a loan approval system, generating counterfactuals can show rejected applicants how their outcomes would have differed with a fairer decision-making process.

\subsubsection{Equalized Odds Post-processing}
Equalized Odds Post-processing is a technique used to mitigate bias and promote equal treatment in machine learning models, particularly in scenarios involving binary classification tasks. This technique is applied after a model has made predictions and aims to adjust those predictions to ensure that equal error rates are achieved across different demographic or group categories.

\textbf{Example:} In a job application screening model, the predicted probabilities of being shortlisted can be equalized for candidates from different age groups to minimize age-based discrimination.

\subsubsection{Reject Option Classification}
Reject Option Classification (ROC) is a technique used to address potential bias and promote equitable outcomes in machine learning models, particularly in classification tasks. ROC introduces an additional class, the "reject" class, to allow for more nuanced and fair decision-making, especially when sensitive attributes like race, gender, or ethnicity are involved.

\textbf{Example:} In a credit scoring model, if the prediction is close to the approval threshold, the model may opt for rejection to prevent biased decisions.

\subsubsection{Oversampling for Minority Groups}
Oversampling for minority groups is a data preprocessing technique used to address the issue of imbalanced datasets, particularly when sensitive attributes like race, gender, or ethnicity play a role. The goal of this technique is to ensure that minority groups are adequately represented in the training data, reducing bias and promoting fair and equitable outcomes in machine learning models.

\textbf{Example:} In a medical diagnosis model, oversampling predictions for rare diseases ensures that the model is equally proficient in predicting both common and rare conditions.

\subsubsection{Confidence Calibration}
Confidence calibration is a technique used to address bias and promote equitable outcomes in machine learning models, particularly in scenarios where sensitive attributes like race, gender, or age may lead to disparities in predictions. Confidence calibration involves adjusting the level of confidence or certainty associated with model predictions to achieve fairness objectives.

\textbf{Example:} In a fraud detection system, confidence calibration ensures that the predicted fraud probabilities accurately reflect the actual likelihood of fraud, promoting fair treatment of all cases.

These post-processing techniques are crucial in rectifying biases and promoting fairness in AI models. Utilizing them effectively can lead to more equitable outcomes and decisions across various demographic categories.


In conclusion, fairness in AI is not just an ethical necessity but a fundamental requirement for fostering trust and ensuring the responsible and equitable deployment of AI systems. As we navigate the evolving landscape of AI, our collective efforts must prioritize fairness to harness the true potential of AI for the greater good.

%Write background here.

%This section is likely to contain a lot of citations.
%
%For instance in \cite{AnzengruberSocInfo2013} the authors propose a novel means for tackling with the problem of preventing bad things from happening.

%----------------------------------------------------------------------------------------
\chapter{Design} % possible chapter for Projects
\label{chap:design}
%----------------------------------------------------------------------------------------
In the subsequent discussion, we will delve into algorithms specifically crafted to address and mitigate fairness concerns. Fairness in algorithms has gained significant attention in recent years due to the realization that traditional computational processes can inadvertently perpetuate biases and disparities. Various approaches, ranging from re-weighting instances to modifying decision boundaries, aim to rectify these issues and promote a more equitable and just application of algorithms across diverse demographics. By exploring these approaches, we aim to shed light on the potential solutions that can contribute to a more inclusive and fair technological landscape.

\section{Fairness through unawareness with proxy detection}

\subsection{Algorithm description}
In the following we are going to describe the algorithm presented here in its two different versions: the one that comes out using the \textbf{apriori} algorithm and the one that comes out using \textbf{variables only} to perform proxy detection. \\

Let's consider a dataset \( D \) belonging to \( \mathbb{R}^{n \times m} \) with \( k \) protected variables. \\

We define \textit{fairness\_evaluation} as follows:
\[
\text{fairness\_evaluation}(v_i, Y) = \lambda(v_i, Y) \quad \forall i \in [1, k]
\]

where:
\begin{align*}
v\_i & : \text{ith attribute belonging to the protected variables}, \\
Y & : \text{output column}.
\end{align*}

The fairness function \( \lambda \) evaluates the relationship between a protected attribute \( v_i \) and the output \( Y \), producing a value that represents the level of fairness for that protected attribute. \\

We define \textit{dataset\_fair} as follows: a dataset \( D \) is considered fair if for every value \( v \) belonging to \textit{fairness\_evaluation}, the following condition holds:
\[ 0.8  < v < 1.25 \] \\

Before going on it's important to remark that we delve into the critical task of identifying proxy variables within the dataset. We specifically concentrate on leveraging fairness metrics, normal variables, and protected variables to determine proxies. Proxy variables are indirect indicators or correlates that may indirectly affect sensitive attributes, potentially introducing bias in our dataset. By examining the relationships between these variables and evaluating them based on fairness metrics, we aim to discern which variables exhibit significant associations with the sensitive attributes, leading to their identification as potential proxies. We focus on utilizing fairness metrics to ensure a comprehensive evaluation that considers both the fairness and equity aspects of the dataset, with the ultimate goal of achieving a more equitable and unbiased data representation. \\

So, considering the relevance of proxy detection in this algorithm it's important to move on and define the two approaches to detect these variables. \\

\subsubsection{Proxy detection via attributes only}
Let's consider the set \( A \), represented as the set of all variables in the dataset excluding the protected variables, and the set \( B \), representing the protected variables.

We define \textit{proxy\_detection} as follows: for every variable \( \text{var} \) belonging to \( A \) and for every protected variable \( \text{var\_protected} \) belonging to \( B \), the variable is a proxy if the fairness metric \( \lambda(\text{var}, \text{var\_protected}) \) satisfies the condition:

\[
\lambda(\text{var}, \text{var\_protected}) <= 0.8 \quad \text{or} \quad \lambda(\text{var}, \text{var\_protected}) => 1.25
\]

In other words, a variable \( \text{var} \) is considered a proxy if the fairness measure \( \lambda \) between \( \text{var} \) and a protected variable \( \text{var\_protected} \) falls outside the acceptable range ]0.8, 1.25[. \\

\subsubsection{Proxy detection via apriori}
It's important to give a brief background of the Association Rule Mining, the family algorithm \textbf{apriori} belongs to.

\begin{enumerate}
    \item \textbf{Association rule mining}: this is a data mining technique used to discover interesting relationships and patterns within large volumes of data. The goal is to identify associations and correlations among various elements present in the data.

    \item \textbf{Apriori algorithm}: Suppose we have a set of transactions $T$, each containing a set of items. Let's define:

\begin{itemize}
  \item $I$: the set of all distinct items in the data.
  \item $D$: the set of transactions, each represented by a set of items.
  \item $F_k$: the set of frequent itemsets of length $k$.
\end{itemize}

The Apriori algorithm operates in iterations, generating $F_k$ from $F_{k-1}$.

\textbf{Step 1:} Initialization:
$$F_1 = \{ \text{frequent item } i, i \in I \}$$

\textbf{Step 2:} Candidate itemset generation:
$$C_k = \{ \text{set } c \text{ of items such that } c \subseteq F_{k-1} \}$$

\textbf{Step 3:} Database scan:
$$\text{For each transaction } t \text{ in } D, \text{ increment the support count of each candidate in } C_k \text{ contained in } t.$$

\textbf{Step 4:} Selection of frequent itemsets:
$$F_k = \{ c \in C_k \text{ such that the support of } c \geq \text{ specified threshold} \}$$

\item \textbf{Row Selection with a Confidence Level}: 
suppose we have obtained association rules using the Apriori algorithm. Each rule $R$ is in the form $A \rightarrow B$, where $A$ is the antecedent and $B$ is the consequent. The confidence of $R$ is defined as:

\[
\text{Confidence}(R) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}
\]

To select rows with a certain confidence level, we consider a confidence threshold $C_{\text{min}}$. If the confidence of a rule $R$ exceeds this threshold, the rule is accepted.

Formally, a rule $A \rightarrow B$ is accepted if:

\[
\text{Confidence}(A \rightarrow B) \geq C_{\text{min}}
\]
In our scenario
\[
C_{\text{min}}
\]
is equal to 0.8. \\

Furthermore, in order to have a proper rows selection, we are particularly interested in privacy and data protection. We only want to consider rules where the consequent ($B$) contains at least one item representing a sensitive attribute or a variable that needs to be protected.

The consequent filtering process involves evaluating each rule to ensure it meets this criteria. If the consequent of a rule does not contain any sensitive attributes, we discard that rule from our final selection. \\

Formally, a rule $A \rightarrow B$ is considered if:

\[
\exists b \in B : b \in S
\]

If this condition is not met, the rule is discarded from our final selection.
\end{enumerate}
After this brief introduction of the concepts used to apply this algorithm we present our approach to detect proxy variables. \\
We define \textit{proxy\_detection} as follows: for each antecedent \( A_i \) belonging to the antecedent list \( \mathcal{A} \), for each consequent \( C_j \) belonging to the consequent list \( \mathcal{C} \), and for each protected variable \( V_k \) belonging to the protected variable list \( \mathcal{V} \), \( A_i \) is a proxy if the fairness metric \( \lambda(A_i, C_j) \) is such that:

\[\lambda(A_i, C_j) <= 0.8 \quad \text{or} \quad \lambda(A_i, C_j) >= 1.25\]

In other words, an antecedent \( A_i \) is considered a proxy if the fairness measure \( \lambda \) between \( A_i \) and a consequent \( C_j \) is outside the acceptable range ]0.8, 1.25[.
\\
\\
There are two other fucnctions that needs to be defined in order to complete our algorithm.
\\
\\
Considering the dataset \( D \) belonging to \( \mathbb{R}^{n \times m} \) and \( k \) as the number of identified proxy variables, we define the function \( \text{proxy\_free\_dataset} \) as follows:

\[
\text{\textbf{proxy\_free\_dataset}}: D \times \mathbb{R}^{k \times 1} \rightarrow \mathbb{R}^{n \times (m - k)}
\]

where \( j = m - k \) and \( \text{proxy\_free\_dataset}(D) \) yields a dataset \( \mathbb{R}^{n \times j} \) devoid of the proxy variables, ensuring the removal of any potential indirect indicators that may influence sensitive attributes. \\
\\
Considering the dataset \( D \) belonging to \( \mathbb{R}^{n \times j} \) where j are the columns obtained after the proxy removals and \( k \) as the number of identified protected variables, we define the function \( \text{protected\_attributes\_free\_dataset} \) as follows:

\[
\text{\textbf{protected\_attributes\_free\_dataset}}: D \times \mathbb{R}^{k \times 1} \rightarrow \mathbb{R}^{n \times (j - k)}
\]

where \( p = j - k \) and \( \text{protected\_attributes\_free\_dataset}(D) \) yields a dataset \( \mathbb{R}^{n \times p} \) devoid of the protected variables, ensuring the removal of any potential indirect indicators that may influence sensitive attributes.

\subsection{Pseudocode}
Here's the pseudocode of the \emph{Fairness through unawareness with proxy detection}:

\begin{algorithm}[H]
    \KwData{dataset}
    \KwResult{fair dataset}
    \While{\textbf{not} \textit{dataset\_fair(dataset)} \textbf{or} \textit{protected\_attributes} \textbf{in} \textit{dataset}}{
        proxies = \textit{proxy\_detection(dataset)}\;
        \If{proxies \textbf{is empty}}{
            dataset = \textit{protected\_attributes\_free\_dataset(dataset)}\;
        }
        \Else{
            dataset = \textit{proxy\_free\_dataset(dataset)}\;
        }
    }
    \KwRet{dataset}
\end{algorithm}

\subsubsection{dataset\textunderscore fair pseudocode}
Here's the pseudocode that establish if the dataset is either \emph{Fair} or not

\begin{algorithm}[H]
    \KwData{dataset, protected\_attribute}
    \KwResult{Boolean value indicating fairness}
    \If{\textit{fairness\_evaluation(dataset, protected\_attribute)} \textbf{is empty}}{
        \KwRet{\textbf{True}}\;
    }
    \KwRet{\textbf{False}}
\end{algorithm}

\subsubsection{fairness\textunderscore evaluation pseudocode}
Here's the pseudocode to perform the fairness evaluation, given a specific \emph{fairness metric}

\begin{algorithm}[H]
    \KwData{dataset, output\_column, protected\_attributes}
    \KwResult{List of attributes failing fairness evaluation}
    fairness\_evaluation\_list $\gets$ empty list\;
    output $\gets$ dataset[output\_column]\;
    \For{attribute \textbf{in} protected\_attributes}{
        \If{\textit{metric(attribute, output)} $\leq$ 0.8 \textbf{or} \textit{metric(attribute, output)} $\geq$ 1.25}{
            \textit{fairness\_evaluation\_list.append(attribute)}\;
        }
    }
    \KwRet{fairness\_evaluation\_list}
\end{algorithm}

\subsubsection{protected\textunderscore attributes\textunderscore free\textunderscore dataset pseudocode}
Here's the pseudocode to remove protected attributes from the dataset

\begin{algorithm}[H]
    \KwData{dataset, protected\_attributes}
    \KwResult{Dataset with protected attributes removed}
    \For{attribute \textbf{in} protected\_attributes}{
        \textit{dataset.remove(attribute)}\;
    }
    \KwRet{dataset}
\end{algorithm}

\subsubsection{proxy\textunderscore free\textunderscore dataset pdeudocode}
Here's the pseudocode to remove proxy attributes from the dataset

\begin{algorithm}[H]
    \KwData{dataset, proxy\_list}
    \KwResult{Dataset with proxies removed}
    \For{proxy \textbf{in} proxy\_list}{
        \textit{dataset.remove(proxy)}\;
    }
    \KwRet{dataset}
\end{algorithm}

\subsubsection{proxy\textunderscore detection pseudocode}
Here's the pseudocode of proxy\textunderscore detection function. There are 2 scenarios that are presented here, the one in which the \emph{variables only}  approach is used and the one in which the \emph{apriori} approach is used

\begin{enumerate}
    \item \textbf{Variables only}: 
            
\begin{algorithm}[H]
    \KwData{protected\_attributes\_list, attributes}
    \KwResult{List of proxies}
    proxy\_list $\gets$ empty list\;
    \For{protected\_attribute \textbf{in} protected\_attributes\_list}{
        \For{attribute \textbf{in} attributes \textbf{and not in} protected\_attributes\_list \textbf{and not in} proxy\_list}{
            \If{\textit{metric(attribute, protected\_attribute)} $\leq$ 0.8 \textbf{or} \textit{metric(attribute, protected\_attribute)} $\geq$ 1.25}{
                \textit{proxy\_list.append(attribute)}\;
            }
        }
    }
    \KwRet{proxy\_list}
\end{algorithm}

    \item \textbf{Apriori}:

    \begin{algorithm}[H]
    \KwData{apriori\_dataset}
    \KwResult{List of proxies}
    proxy\_list $\gets$ empty list\;
    \For{consequent \textbf{in} apriori\_dataset}{
        \For{antecedent \textbf{in} apriori\_dataset}{
            \If{\textit{metric(antecedent, consequent)} $\leq$ 0.8 \textbf{or} \textit{metric(antecedent, consequent)} $\geq$ 1.25}{
                \textit{proxy\_list.append(antecedent)}\;
            }
        }
    }
    \KwRet{proxy\_list}
\end{algorithm}
\end{enumerate}
\newpage

\section{Fairness through data rebalancing}
\subsection{A New Definition of Fairness}
In traditional fairness definitions, the focus often revolves around ensuring fair treatment for individual protected attributes, denoted as $A_1, A_2, \ldots, A_k$. While this is undoubtedly crucial, a more comprehensive understanding of fairness calls for an examination of fairness in the context of combinations of protected attributes. We propose a new definition of fairness that takes into account the representation of all combinations of $k$ protected attributes, aiming for equitable representation across these combinations.

\subsubsection{Equal Representation of Combinations}

We define a fair dataset as one in which, for each combination of protected attributes $\{A_1, A_2, \ldots, A_k\}$, the representation is equal and proportional. Mathematically, a dataset is fair if:

\[
\forall i_1, i_2, \ldots, i_k: \frac{|D_{i_1, i_2, \ldots, i_k}|}{|D|} = \text{constant}
\]

where:
- $D$ is the dataset,
- $|D_{i_1, i_2, \ldots, i_k}|$ is the number of samples with the specific combination of protected attributes $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$,
- $|D|$ is the total number of samples in the dataset.

This entails that any combination of demographic groups, defined by the protected attributes, should have comparable representation, thereby fostering a balanced and unbiased dataset.

\subsubsection{Promoting Comprehensive Fairness}

By striving for equal representation of combinations of protected attributes, we address a fundamental aspect of fairness that transcends individual attributes. This approach provides a more nuanced understanding of fairness by considering the intersections of various demographic groups. It encourages a broader examination of potential biases that may arise when considering multiple attributes simultaneously.

Incorporating this definition of fairness into the dataset rebalancing process enables us to promote a comprehensive notion of fairness, aligning with the principles of equal opportunity and non-discrimination across all combinations of protected attributes. Our subsequent algorithm and experimental evaluation are designed to actualize this definition and demonstrate its effectiveness in achieving a more equitable representation within the dataset.

\subsection{Algorithm description}

Let \( D \) be a dataset \( R^{n \times m} \), where \( n \) is the number of samples and \( m \) is the number of features. Let \( k \) be the number of protected variables represented as \( R^{n \times 1} \), and let there be a single output variable represented as \( R^{n \times 1} \).

A rebalancing function \( \mathcal{R} \) can be formally defined as a mapping:

\[
\mathcal{R}: R^{n \times m} \rightarrow R^{l \times m}
\]

where \( l > m \), and the function \( \mathcal{R} \) transforms the input dataset \( D \) of dimensions \( n \times m \) into an output dataset \( D' \) of dimensions \( l \times m \).\\
\\
Let \( k \) be the number of binary protected variables in the dataset \( D \), and consider the output variable to be binary as well. The number of possible combinations of these variables is \( 2^{(k+1)} \).

Consider a set \( \text{Combination-frequency} \) where we have occurrences of all \( 2^{(k+1)} \) combinations within the dataset. For each combination, the number of rows in which that combination appears should be equal to the maximum occurrence among all combinations present in the set \( \text{Combination\textunderscore frequency} \). This maximum value is denoted as \( \text{Max}(\text{Combination-frequency}) \).

Mathematically, the number of rows (\( l \)) the final dataset should have for each combination is given by:

\[
l = \text{Max}(\text{Combination-frequency})
\]\\
\\
Let \( l \) be the desired number of rows for the final dataset. For each combination of values, we calculate the occurrence count \( \text{occurrence}_i \), where \( i \) ranges from 1 to \( 2^{(k+1)} \), with \( k \) being the number of protected binary variables and considering the output variable as binary.

The total number of rows to be added is given by:

\[
\text{total\_rows\_to\_add} = l - \sum_{i=1}^{2^{(k+1)}} \text{occurrence}_i
\]

For each iteration:
\begin{itemize}
    \item The values of the protected and output variables are set according to the specific combination.
    \item For all other attributes, a random value \( \text{random\_value}_{ij} \) is generated, where \( j \) represents the specific attribute and \( i \) represents the row being added for that attribute. \( \text{random\_value}_{ij} \) is within the minimum and maximum range for attribute \( j \).
\end{itemize}

\subsection{Pseudocode}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{combination\_set, combination\_frequency, protected\_attributes, dataset\_attributes}
    \Output{Updated dataset}

    max\_frequency $\gets$ max(combination\_frequency)\;
    \For{index $\gets$ 0 \KwTo len(combination\_set)}{
        combination $\gets$ combination\_set[index]\;
        frequency $\gets$ combination\_frequency[index]\;
        \While{frequency $<$ max\_frequency}{
            new\_row $\gets$ empty\;
            \For{(attr, val) \textbf{in} (combination, protected\_attributes)}{
                new\_row[attr] $\gets$ val\;
            }
            \For{attr \textbf{in} dataset\_attributes \textbf{and not in} protected\_attributes}{
                new\_row[attr] $\gets$ random(min(attr), max(attr))\;
            }
            dataset.add(new\_row)\;
            frequency $+$= 1\;
        }
    }
\end{algorithm}

%----------------------------------------------------------------------------------------
\chapter{Implementation} % possible chapter for Projects
\label{chap:implementation}
%----------------------------------------------------------------------------------------
\section{Dataset description}
Before moving on into the several details of the implementation of the algorithms previously presented we have to present the dataset on which these algorithms have been applied.
The dataset chosen for this work is \emph{ULL dataset}. \\
This dataset contains information on the educational system in Canary Island. It's composed by the census of students enrolled in 4 different academic years.

\subsection{Pre-processing operations on ULL dataset}
A deep analysis of the dataset led us to make a first features selection. More specifically for this work only the \emph{important} and \emph{protected} attributes have been selected.
Furthermore, after a proper domain analysis the protected attributes selected to be passed to the algorithms have been a subset of the orginal selected:
\begin{enumerate}
    \item sex
    \item capital island: if the student comes from the capital of the city
    \item public\textunderscore private: if the school is public or private
    \item inmigrant: if the student is either inmigrant or not
    \item inmigrant second gen: if the student is either inmigrand of second gen or not
    \item parent expectation
    \item mothly houseold income
    \item cconomic, social and cultural satus index
\end{enumerate}
It's furthermore fundamental to remark the ways in which we decided to consider this protected attributes (together with the other ones not considered to fed the algorithm).
More specifically:
\begin{itemize}
    \item if the attribute is binary no transformation is applied
    \item if the attribue is not binary that it's trnasformed into a binary attribute where the value \emph{1} means over the mean and the \emph{0} under the mean.
\end{itemize}

\subsection{Goal of this work}
The goal of this work is the one to predict the level of english for the current grade based on past experience. To achieve this we computed on the \emph{level\textunderscore ing} attribute the same pre-processing transofrmation used for not-binary protected attributes. This means that we want \textbf{match} the students to establish if a given student is either under or over the mean.

%\lstinputlisting[
%	float,
%	language=Java,
%	caption={My very first program in Java},
%	label={lst:helloworld},
%]{listings/HelloWorld.java}

%You may need to reference listings in your thesis.
%
%In this case, you are encouraged to make them \emph{floating}, and reference them by means of labels.
%
%For instance, in \Cref{lst:helloworld}, we describe an hello world program in Java.

\section{Fairness through unawareness with proxy detection}
In this section we go through the several implementative issues that have been encountered during the development of this algorithm. We will explore in detail the two approaches, the one with \textbf{apriori} and the one in which only the \textbf{variables} have been taken in account. \\
Before moving on it's important to be more detailed about the fairness metric we decided to use.
\subsection{Disparate Impact}
isparate Impact (DI) is a fairness metric commonly used in the context of algorithmic decision-making and predictive modeling. It is designed to measure the potential discriminatory effects of a decision-making process on different demographic groups.

The Disparate Impact metric is defined as the ratio of the probability of a favorable outcome for the disadvantaged group to the probability of a favorable outcome for the advantaged group:

\[
\text{Disparate Impact (DI)} = \frac{P(\text{Favorable Outcome} | \text{Disadvantaged Group})}{P(\text{Favorable Outcome} | \text{Advantaged Group})}
\]

where $P(\text{Favorable Outcome} | \text{Disadvantaged Group})$ is the probability of a favorable outcome given membership in the disadvantaged group, and  \\ $P(\text{Favorable Outcome} | \text{Advantaged Group})$ is the probability of a favorable outcome given membership in the advantaged group.
\subsection{Proxy detection via apriori}
The implementation of this approach has proceeded according a modular approach and, more specifically, has been developed a module named \emph{fairness} in which there are all the sub-modules needed to implement this approach.
\subsubsection{fairness\textunderscore metric module}
This module has been thought to contain the implementation of the several fairness metrics considered in the context of the study. In our scenario it only contains the implementation of the \emph{Disparate Impact} metric. \\
This implementation provides to the user the information about the fairness status of the dataset provided together with the protected attributes, the otuput attribute and the possible values that this value may have: \\
\begin{lstlisting}
    def fairness_evaluation(self, dataset: pd.DataFrame, protected_attributes: list, output_column_values: list,
                            output_column: str) -> str:
        """
        This method perform an evaluation of the fairness of a given dataset according to the Disparate Impact metric
        :param dataset: this is the dataset on which to be labelled as fair or unfair
        :param protected_attributes: the list of the protected attributes on which compute the disparate impact value
        :param output_column: the column of the dataset that represents the output
        :return: return 'fair' if the dataset is fair, unfair 'otherwise'
        """
        bias_analysis_dataframe = self.bias_detection(dataset, protected_attributes, output_column_values, output_column)
        return_value = 'unfair'
        for value in bias_analysis_dataframe['Disparate Impact'].values:
            if value <= 0.80 or value >= 1.25:
                return_value = 'unfair'
                break
            else:
                return_value = 'fair'

        return return_value
\end{lstlisting}


\subsubsection{matching module}
This module only contains the method \emph{conscious\textunderscore fairness\textunderscore through\textunderscore unawareness} that can return the dataset in two possible configurations:
\begin{enumerate}
    \item if the dataset is fair it returns the dataset itself only with variables pre-processed. Since these pre-processing operations are performed before the fairness checking these are the same for both scenarios. These operations are the transformation of every \emph{categorical} attribute into a numerical one and the transformation of protected attributes as explained above.
    \item if the dataset is not fair the algorithm looks for the proxies and remove the proxies found if there are some and the protected attributes.
\end{enumerate}
\subsubsection{proxy module}
This module is the core of the whole algorithm. This module contains other two sub-modules. More specifically it contains a module whose goal is to detect proxy and the other one is the module in charge to translate the result of the proxy detection to provide a list of proxies to be used into the \emph{matching} module
\begin{enumerate}
    \item \emph{proxy\textunderscore detection}: this module is the one in which the \emph{apriori} algorithm is applied in order to found the relation between antecedents and consequents with a confidence value of 0.8. The key method of this module is the private method \emph{\textunderscore return\textunderscore apriori\textunderscore dataframe}. This method performs computation on the result of the application of the apriori algorithm. More specifically it returns a dataframe with 3 columns \textbf{Antecedent}, \textbf{Consequent}, \textbf{Confidence} \\
    \begin{lstlisting}
        def _return_apriori_dataframe(association_results: list) -> pd.DataFrame:
    """This function returns the dataframe with the association generated by the apriori algorithm

    Args:
        association_results (list): is the list of the association returned by the apriori algorithm

    Returns:
        pd.DataFrame: is the dataframe equivalent to the association result list
    """
    antecedent = []
    consequent = []
    confidence = []

    for association_result in association_results:
        for ordered_statistic in association_result.ordered_statistics:
            antecedent_elements = list(ordered_statistic.items_base)
            antecedent.append(antecedent_elements)
            consequent_elements = list(ordered_statistic.items_add)
            consequent.append(consequent_elements)
            confidence_elements = ordered_statistic.confidence
            confidence.append(confidence_elements)

    antecedent_series = pd.Series(antecedent)
    consequent_series = pd.Series(consequent)
    confidence_series = pd.Series(confidence)

    dataframe = pd.DataFrame(
        {'Antecedent': antecedent_series, 'Consequent': consequent_series, 'Confidence': confidence_series})

    return dataframe
    \end{lstlisting}
    \item \emph{proxy\textunderscore processing}: this module contains two key methods. The first method, \emph{\textunderscore return\textunderscore proxy\textunderscore protected\textunderscore attribute} returns a dataframe in which for each sensitive attribute as consequent are related the antecedents that are the possible proxies for that sensitive attribute. The second method is \emph{proxy\textunderscore fixing}, in which for each antecedent is computed the Disparate impact value related to that consequent with that specific value. Only the variables with DI value lesser than 0.8 or greater than 1.25 are considered as proxies. In the following is reported the code of both methods:
    \begin{lstlisting}
        def _return_proxy_protected_attribute(proxy_variables: pd.DataFrame, protected_attributes: list) -> pd.DataFrame:
    """This method returns a dataframe containing the proxy variables for each sensitive attribute

    Args:
        proxy_variables (pd.DataFrame): the dataframe of the proxy variables (antecedent, consequent, confidence)
        protected_attributes (list): the list of the protected attributes

    Returns:
        pd.DataFrame: _description_
    """
    sensitive_antecedent = []
    sensitive_consequent = []
    for index, proxy_row in proxy_variables.iterrows():
        for consequent in proxy_row['Consequent']:
            for sensitive_attribute in protected_attributes:
                if str(consequent).startswith(sensitive_attribute):
                    sensitive_antecedent.append(proxy_row['Antecedent'])
                    sensitive_consequent.append(consequent)

    dataframe = pd.DataFrame(
        {'Antecedent': pd.Series(sensitive_antecedent), 'Consequent': pd.Series(sensitive_consequent)})

    return dataframe


    \end{lstlisting}
    \begin{lstlisting}
        def proxy_fixing(original_dataset: pd.DataFrame, protected_attributes: list) -> pd.DataFrame:
    """This method returns a dataset with proxy variables founded in the original dataset analyzed.
    In case these proxies lead to unfairness the proxies are deleted

    Args: original_dataset (pd.DataFrame): the dataset on which the proxy variable have to be deleted if they lead to
    unfairness
    protected_attributes (list): the list of protected attribute on which perform the proxy analysis

    Returns:
        pd.DataFrame: returns the dataframe in which the proxies do not lead to fairness
    """
    proxy_variables = return_proxy_variables(original_dataset)
    proxy_variables_for_sensitive_attributes = _return_proxy_protected_attribute(proxy_variables,
                                                                                 protected_attributes)
    
    dataset = original_dataset
    for index, row in proxy_variables_for_sensitive_attributes.iterrows():
        for antecedent in row['Antecedent']:
            consequent = row['Consequent']

            disparate_impact_value = _compute_disparate_impact_for_proxy(antecedent, consequent,
                                                                         original_dataset)
            if not 0.8 < disparate_impact_value < 1.25 and _proxy_format_to_column(antecedent) not in protected_attributes:
                dataset = _remove_proxy_from_dataset(original_dataset, antecedent)

            else:
                continue

    return dataset

    \end{lstlisting}
\end{enumerate}

\subsection{Proxy detection via variables only}
The implementation of this approach differs with the implementation of the apriori approach because of the operation of proxy detection and processing have been mixed together and the proxy removal operation has been delegated to the \emph{matching} module. In the following there's the code that computes the proxy detection:
\begin{lstlisting}
    def proxy_detection(dataset: pd.DataFrame, protected_attributes: list, output_column: str) -> list:
    attributes_list = []
    for attr in dataset.columns:
        if attr not in protected_attributes:
            attributes_list.append(attr)
            
          
    if output_column in attributes_list:
        attributes_list.remove(output_column)
        
    proxy_list = []
    for attribute in attributes_list:
        for protected_attribute in protected_attributes:
            if _compute_disparate_impact_for_proxy(attribute, protected_attribute, dataset) == 'PROXY':
                proxy_list.append(attribute)

    return proxy_list

\end{lstlisting}
\section{Fairness through data rebalancing}
The first big difference between this approach and the two previous ones is that no fairness-metrics is used here. The reason behind this is the new definition we provide for fairness, where we assume it as a parity in combination occurrance. So the core of this algorithm is the \emph{rebalancing} module in which we add rows to dataset according to the most frequent combination occurrance in the original dataset.
The key part of this module is the one in which the other attributes are inserted. In this scenario we add a new value considering the minimum and the maximum in the temporary dataset, where this dataset is the dataset in which a certain combination of protected\textunderscore attributes and output is occurred. It's furthermore important to remark that since we choose a a value randomically this may alter the distribution for that attribute. So we do not look for a uniform distribution.
\begin{lstlisting}
    for index in range(0, len(combination_list)):
        combination = combination_list[index]
        temp_dataset = final_dataset.query(return_query_for_dataframe(combination, combination_attributes))
        if combination_frequency[index] == combination_frequency_target:
            continue
        else:
            for counter in range(0, combination_frequency_target - combination_frequency[index]):
                new_row = {}
                for (attr, value) in zip(combination_attributes, combination):
                    new_row[attr] = value
                for attribute in final_dataset.columns:
                    if attribute not in combination_attributes:
                        if is_variable_discrete(temp_dataset, attribute):
                            new_row[attribute] = random.randint(temp_dataset[attribute].min(), temp_dataset[attribute].max())
                        else:
                            new_row[attribute] = random.uniform(temp_dataset[attribute].min(), temp_dataset[attribute].max())
                
                final_dataset.loc[len(final_dataset)] = new_row

\end{lstlisting}
%----------------------------------------------------------------------------------------
\chapter{Validation} % possible chapter for Projects
\label{chap:validation}
%----------------------------------------------------------------------------------------
It's important to remark the goal of this work: predict if a student's english level is under or over the mean for each grade: 3,4,6. With this in mind we splitted the dataset into 3 different ones, any of them with their own grade selected.\\
For each grade have been applied the following approaches, in order to perform a proper comparaison:
\begin{enumerate}
    \item No fairness assumptions are made
    \item Fairness through unawareness
    \item Fairness through unawareness with proxy detection via apriori
    \item Fairness through unawareness with proxy detection via variables only
    \item Fairness through data rebalancing
\end{enumerate}

\section{Experiment setup}
For each of the previous approaches have been chosen 3 models to perform the prediction:
\begin{enumerate}
    \item RandomForest Classifier
    \item XGBoost Classifier
    \item DecisionTree Classifier
\end{enumerate}

On the previous models is applied the GridSearch with a cv of 10 in order to select the best parameters combination for each one. In the following are reported the specific parameters for each model:
\begin{enumerate}
    \item RandomForest Classifier:
    \begin{enumerate}
        \item \emph{n\textunderscore estimators}: [10, 100, 10]
        \item \emph{criterion}: [gini, entropy, log\textunderscore loss]
        \item \emph{max\textunderscore depth}: [10, 50, 10]
        \item \emph{max\textunderscore leaf\textunderscore nodes}: [10, 50, 10]
    \end{enumerate}
    \item XGBoost Classifier:
    \begin{enumerate}
        \item \emph{min\textunderscore child\textunderscore weight}: [1, 5, 10]
        \item \emph{gamma}: [0.5, 1, 1.5, 2.5]
        \item \emph{subsample}: [0.6, 0.8, 1.0]
        \item \emph{colsample\textunderscore bytree}: [0.6, 0.6, 1.0]
        \item \emph{max\textunderscore depth}: [3, 4, 5]
    \end{enumerate}
    \item DecisionTree Classifier:
    \begin{enumerate}
        \item \emph{criterion}: [gini, entropy, log\textunderscore loss]
        \item \emph{max\textunderscore depth}: [10, 50, 10]
        \item \emph{max\textunderscore leaf\textunderscore nodes}: [10, 50, 10]
    \end{enumerate}
\end{enumerate}

Each model is trained with the 67\% of the dataset while the 33\% is left for the test.
In the following are reported the results for each grade. More specifically for each model is reported the best model selected and the accuracy on the test set.

\section{Grade 3}
\subsection{No-fair approach}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fairness throguh unawareness}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fairness through unawareness with proxy detection via apriori}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fariness through unawareness with proxy detection via variables only}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Fairness through data rebalancing}
\subsubsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsubsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}


\section{Grade 4}
\subsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}
\section{Grade 6}
\subsection{Best models}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Best parameters} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

\subsection{Accuracy}
\begin{tabular}{|c|c|}
    \hline
    \textbf{Model} & \textbf{Accuracy} \\
    \hline
    RandomForest Classifier  &  \\
    \hline
    XGBoost Classifier & \\
    \hline
    DecisionTree Classifier &  \\
    \hline
\end{tabular}

%----------------------------------------------------------------------------------------
\chapter{Conclusion}
\label{chap:conclusions}
%----------------------------------------------------------------------------------------

Write conclusions here.


%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

\backmatter

%\nocite{*} % comment this to only show the referenced entries from the .bib file

\bibliographystyle{alpha}
\bibliography{bibliography}


\end{document}