\documentclass[12pt,a4paper,openright,twoside]{book}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{algpseudocode}


\newcommand{\thesislang}{english} % commentare in caso di tesi in italiano
\usepackage{thesis-style}
% version
\newcommand{\versionmajor}{0}
\newcommand{\versionminor}{1}
\newcommand{\versionpatch}{2}
\newcommand{\version}{\versionmajor.\versionminor.\versionpatch}
\typeout{Document version: \version}

\begin{document}
	
\frontmatter

\input{front.tex}

\begin{abstract}	
Nowdays a lot of companies and social entities are wondering about the implications of Artificial Intelligence usages in the daily life of people, with a 
huge interests for the implications in discriminations that may occour using these systems. \\
A lot of methodologies in several fields like computer science, statistics and social sciences have been formalized in order to 
afford the problem of the fairness in social systems and this problem is being dealt with during the development of the modern AI systems. \\
In this work we propose several methods that have been developed considering fairness from the design phase, that's why they are called \emph{fair-by-design-method}
In order to develop these methods we used well-known Python libraries such as \emph{Pandas}, \emph{Sci-kit-learn} and other libraries implemented by ourselves.
We begun our work with mitigation techniques in fairness, also known as pre-processing techniques.
Our work has been evaluated exploiting a real-world dataset on the education on Canary Island. So we compared the results of the prediction of a certain result for students applying fair-by-design methods and 
the prediction obtained without consider fairness.
\end{abstract}


\begin{acknowledgements} % this is optional
Never too far down, to come back
\end{acknowledgements}

%----------------------------------------------------------------------------------------
\tableofcontents   
\listoffigures     % (optional) comment if empty
\lstlistoflistings % (optional) comment if empty
%----------------------------------------------------------------------------------------

\mainmatter

%----------------------------------------------------------------------------------------
\chapter{\introductionname}
\label{chap:introduction}
%----------------------------------------------------------------------------------------

In recent years, there has been a notable increase in the utilization of artificial intelligence (AI) systems across various sectors. This rise is attributed to advancements in technology, the accessibility of extensive data, and heightened computational capabilities. AI has transcended its role as a mere performance enhancer and has begun exerting a profound impact on social contexts. These systems are now pervasive in healthcare, education, criminal justice, and other crucial domains, fundamentally reshaping decision-making processes and societal interactions. However, the reliance of AI systems on data for learning and decision-making implies that they inherit the biases embedded in these data. This inherent bias has the potential to perpetuate existing societal imbalances and prejudices. Addressing bias and ensuring fairness in AI algorithms is hence a pressing concern, necessitating continuous research, responsible development practices, and vigilant oversight to guarantee equitable outcomes for all. In this discussion, we will delve into the intricate dynamics of biases and fairness in AI systems, exploring viable strategies to effectively mitigate these challenges.

\paragraph{Artificial Intelligence}
In recent years, artificial intelligence (AI) has undergone a remarkable evolution, transforming from a theoretical concept to a pervasive force shaping various facets of our lives. Advances in machine learning, particularly deep learning, have fueled this growth, enabling AI systems to process vast amounts of data and extract meaningful insights. Notably, breakthroughs in natural language processing have led to more conversational AI, making interactions with machines increasingly human-like. Moreover, AI applications have expanded across sectors, encompassing healthcare, finance, transportation, and more. However, ethical considerations and the responsible development of AI have gained significant traction, prompting discussions on privacy, bias mitigation, and ensuring that AI benefits all of society. The future of AI promises continued innovation, fostering a symbiotic relationship between humans and machines for a more technologically enriched world.
\paragraph{Bias}
In the realm of artificial intelligence, the issue of bias is a critical concern. AI systems are trained on historical data, and if this data carries biases related to gender, race, or other factors, the AI can inadvertently perpetuate and amplify these biases in its predictions or decisions. Addressing bias in AI necessitates a proactive approach, from carefully curating training data to developing algorithms that are designed to detect and mitigate bias. Furthermore, promoting diversity and inclusivity in the development teams is vital to ensuring that biases are recognized and addressed effectively.
\paragraph{Fairness}
Ensuring fairness in AI is an ongoing and essential goal. It involves designing and implementing AI systems in a way that does not discriminate against individuals or groups based on characteristics like race, gender, or socioeconomic status. Achieving fairness demands a multidimensional approach, including a clear definition of fairness metrics, active involvement of diverse stakeholders, and regular audits of AI systems to detect and rectify any unfair biases. Additionally, incorporating transparency and accountability into the AI development process is crucial for building trust and ensuring fair outcomes for all users.
\paragraph{Fair-by-design method}
Since the huge impact that AI systems are having on daily life and more in general on the whole socio-technical systems it becomes crucial to design new AI methods and model that take account of fairness since the design step. That's why we call them fair-by-design methods.
%----------------------------------------------------------------------------------------
\chapter{State of the Art} % or Background
\label{chap:background}
%----------------------------------------------------------------------------------------

This chapter provides an overview of the previous works and scientific literature that led us to the implementation of fair-by-design methods. This chapter strats
with an overview on \textbf{artificial intelligence} and its application in both critical sectors and socio-technical systems. Then we go deeply into the issues of \textbf{bias} and \textbf{fairness} in AI systems.

\section{Artificial Intelligence}
Artificial Intelligence (AI) and Machine Learning (ML) are transformative technologies that are redefining the way we interact with the world. AI refers to the development of computer systems capable of performing tasks that typically require human intelligence, such as speech recognition, problem-solving, and learning. Machine Learning is a subset of AI, involving the creation of algorithms that allow computers to learn patterns and make predictions based on data, improving their performance over time. \\
AI encompasses a broad range of techniques, including natural language processing, computer vision, robotics, and expert systems. Machine Learning, on the other hand, involves algorithms that enable machines to learn and improve their performance without being explicitly programmed. ML algorithms are designed to identify patterns and make informed decisions based on data input. \\
If we consider examples of AI in critical sectors such as healthcare and autonomous driving it becomes clear how AI could impact our lives. Beyond these applications it becomes necessary to consider the applications of AI in socio-technical systems. \\
Socio-technical systems refer to complex interplays between people, technology, and social institutions. AI and ML have a profound impact on these systems, influencing how individuals interact with technology and how society functions as a whole. \\
The integration of AI and ML raises ethical questions regarding privacy, biases in algorithms, and job displacement due to automation. It is crucial to develop frameworks that ensure fairness, transparency, and accountability in AI systems to mitigate potential harm and ensure equitable benefits. \\

\section{Bias}
Artificial Intelligence (AI) holds immense promise for revolutionizing various aspects of our lives. From aiding decision-making processes to automating routine tasks, AI systems are increasingly becoming integrated into our daily lives. However, a growing concern within the AI community and society at large is the issue of biases embedded in AI algorithms.

\subsection{Understanding Bias in AI}
Bias in AI refers to the unfair and skewed representation or treatment of individuals or groups based on factors such as race, gender, age, socioeconomic status, or other characteristics. These biases can be unintentionally perpetuated during the development and training of AI models due to historical data imbalances, societal prejudices, or flawed algorithms.

\subsection{Sources of Bias}
\begin{itemize}
    \item \textbf{Historical data}: AI models are often trained on historical data, reflecting past societal biases and inequalities. If the data contains biased patterns, the AI system can learn and replicate those biases.
    \item \textbf{Human bias}: The biases of the people involved in collecting, selecting, and labeling the data used for training AI models can seep into the algorithms.
    \item \textbf{Algorithmic bias}: The algorithms themselves can inadvertently introduce or magnify biases due to their design, assumptions, or mathematical formulations.
\end{itemize}

\subsection{Example of Bias in AI}
\subsubsection{Race and Gender Bias in Facial Recognition}
Facial recognition systems have shown significant biases in accurately identifying individuals based on race and gender. Studies have revealed that these systems are more accurate for lighter-skinned individuals, while misidentifying or underrepresenting people with darker skin tones, especially women.

\subsubsection{Criminal Justice Bias}
AI algorithms used in criminal justice for risk assessment and sentencing have been found to exhibit racial biases. These algorithms tend to overpredict the likelihood of reoffending for certain minority groups, leading to unfair and discriminatory outcomes.

\subsubsection{Recruitment Bias}
AI algorithms used in recruitment processes have shown gender bias by favoring male candidates over equally or more qualified female candidates. This bias can perpetuate gender imbalances in the workplace.

\section{Fairness}
Fairness in AI refers to the absence of discrimination or bias in the creation, deployment, and outcomes of AI systems. It's about treating individuals fairly and equitably, irrespective of their gender, race, age, ethnicity, or any other characteristic. Achieving fairness in AI is fundamental to building trust and ensuring that AI benefits all members of society. \\
Unfair AI can perpetuate and amplify existing societal inequalities, reinforcing stereotypes and excluding certain groups. Inaccurate credit scoring, biased hiring practices, and unfair sentencing are just a few examples of how unfair AI systems can have real and damaging consequences. \\
To build AI systems that are fair and just, interdisciplinary collaboration and a commitment to inclusivity are essential. Developers, policymakers, and communities must work together to identify, understand, and rectify biases and disparities in AI. By embedding fairness as a core principle in AI development, we can create a future where AI benefits all, contributing to a more equitable society. \\

\subsection{Fairness Techniques in AI}

\subsubsection{Pre-processing:}
Pre-processing focuses on handling the initial data before it is used to train the AI model. This stage is crucial to ensure that the data is balanced and representative of the diversity in the population. Common pre-processing techniques include oversampling, undersampling, noise removal from the data, and creating balanced synthetic datasets.

\subsubsection{In-processing:}
In-processing techniques intervene directly during the model training phase. This is a more specific approach to ensure fairness in the model. For example, regularization techniques can be used to penalize the model if it shows a discriminatory tendency based on certain sensitive features (such as gender or ethnic origin). Other techniques include modifying the cost functions or manipulating the model's predictions to ensure fair treatment across different categories.

\subsubsection{Post-processing:}
Post-processing occurs after the model has been trained and has generated predictions. This phase focuses on corrections or modifications to the model's predictions to ensure fairness. For instance, realignments or adjustments can be applied to balance predictions so that there are no unjustified disparities among demographic groups. This process may involve recalibrations or other transformations of the model's results to mitigate biases.

Implementing these techniques requires a deep understanding of the specific fairness challenges in the data and models, as well as ongoing evaluation to ensure that AI adheres to ethical standards and promotes fairness.

\subsection{Pre-processing Techniques for Addressing Fairness in AI}

Pre-processing plays a vital role in mitigating fairness issues within AI systems by preparing the data before it is utilized to train a model. The objective is to ensure that the data is unbiased, representative, and equitable across different demographic groups. Various techniques can be employed during pre-processing to promote fairness:

\subsubsection{Oversampling and Undersampling}
Oversampling involves increasing the representation of underrepresented groups in the dataset, while undersampling reduces the overrepresentation of certain groups. This rebalancing technique helps to achieve a more equal distribution of samples from different categories, addressing potential biases.

\textbf{Example:} In a credit approval scenario, if a particular group is underrepresented in the dataset, oversampling can be employed to create synthetic instances for that group, ensuring fair credit evaluations.

\subsubsection{Noise Removal}
Identifying and removing noise or irrelevant data from the dataset is crucial to improve the quality of the training data. Noise may introduce biases or distortions in the model, and cleaning the data helps ensure a fair and accurate representation of each group.

\textbf{Example:} In sentiment analysis, if a dataset contains biased reviews based on certain demographics, such as gender, removing these biased reviews helps in training a fair sentiment analysis model.

\subsubsection{Data Augmentation}
Data augmentation techniques involve generating additional training samples by applying transformations (e.g., rotation, translation) to the existing data. This process helps to enhance the diversity and balance within the dataset, leading to a more equitable model.

\textbf{Example:} In image recognition, augmenting images by rotating or flipping them can provide a more balanced representation of different features, promoting fairness in the model's predictions.

\subsubsection{Bias Mitigation Algorithms}
Specialized algorithms can be applied to preprocess the data and reduce biases. These algorithms aim to adjust the dataset to ensure that the model is not unfairly influenced by specific attributes, such as gender or race. Techniques like re-weighting or re-sampling can be used to achieve a fairer representation.

\textbf{Example:} When training a model for a hiring platform, using a bias mitigation algorithm to adjust the dataset to have an equal representation of genders can lead to a more fair hiring process.

\subsubsection{Sensitive Attribute Removal or Neutralization}
In some cases, sensitive attributes (e.g., race, gender) can be removed from the dataset or transformed into more neutral representations. This prevents the model from relying on these attributes to make predictions, promoting fairness.

\textbf{Example:} In a loan approval system, sensitive attributes like race can be neutralized by removing them or encoding them in a way that the model cannot identify them, ensuring unbiased loan decisions.

\subsubsection{Synthetic Data Generation}
Creating synthetic data that accurately represents underrepresented groups can help balance the dataset and improve fairness. This process involves generating new data points based on the existing information, particularly focusing on minority groups.

\textbf{Example:} In healthcare AI, generating synthetic medical records for underrepresented medical conditions ensures that the AI system is trained fairly, considering all health conditions adequately.

These pre-processing techniques are essential steps in the AI development pipeline to ensure that the subsequent models are fair, unbiased, and capable of providing equitable outcomes across various demographic categories.

\subsection{In-processing Techniques for Addressing Fairness in AI}

In-processing techniques aim to mitigate fairness issues directly during the model training phase, influencing the learning process to ensure fairness in model predictions. These approaches target bias reduction and fairness promotion within the model's decision-making process. Several techniques can be employed during model training to achieve fairness:

\subsubsection{Regularization}
Regularization is a common in-processing technique used to penalize the model for exhibiting biased behavior based on sensitive attributes. By adjusting the regularization terms in the model's training objective, we can discourage the model from relying heavily on such attributes.

\textbf{Example:} In a hiring model, a regularization term can be added to the loss function to penalize the model for making biased predictions related to gender or ethnicity.

\subsubsection{Reweighting Training Samples}
This technique involves assigning different weights to training samples based on their sensitive attributes to balance the influence of different groups during training. It aims to ensure that the model learns equally from all demographic categories.

\textbf{Example:} In a credit scoring model, training samples from underrepresented demographics can be assigned higher weights to ensure their impact on model training is more significant.

\subsubsection{Adversarial Training}
Adversarial training involves training a fairness-specific adversarial network alongside the primary model. The adversarial network aims to predict the sensitive attributes from the model's hidden representations, encouraging the primary model to generate features that are independent of these attributes.

\textbf{Example:} In facial recognition, an adversarial network can be used to ensure that the model's features are not biased towards specific racial characteristics.

\subsubsection{Prejudice Remover Regularizer}
This technique involves adding a prejudice remover regularizer to the model's training process. The regularizer encourages the model to learn a fair representation by minimizing the correlation between the learned features and the sensitive attributes.

\textbf{Example:} In a recommendation system, the prejudice remover regularizer can help ensure that recommendations are not influenced by the gender of the user.

\subsubsection{Demographic Parity Loss}
Demographic parity loss is a technique that incorporates fairness directly into the model's loss function. It ensures that the model's predictions are as similar as possible across different demographic groups.

\textbf{Example:} In a loan approval system, the demographic parity loss can be added to the loss function to minimize disparities in approval rates between different ethnic groups.

\subsubsection{Fair Adversarial Training}
Fair adversarial training extends adversarial training to explicitly reduce disparate treatment among different demographic groups. It adds an adversarial component to the loss function to minimize the unfairness of the model's predictions.

\textbf{Example:} In a criminal recidivism prediction model, fair adversarial training can help mitigate the biased prediction rates for different racial groups.

These in-processing techniques are vital tools in promoting fairness within AI models. Integrating them appropriately during model training can significantly contribute to reducing biases and achieving equitable outcomes across various demographic categories.

\subsection{Post-processing Techniques for Addressing Fairness in AI}

Post-processing techniques are applied after the model has been trained and predictions have been generated. Their purpose is to rectify any biases or disparities in the model's outputs and ensure fairness in the final outcomes. Several techniques can be employed during post-processing to promote fairness:

\subsubsection{Re-ranking or Re-scoring}
This technique involves adjusting the ranking or scores assigned by the model to instances. The scores are modified based on fairness criteria, aiming to minimize any discriminatory impact on different demographic groups.

\textbf{Example:} In a college admissions system, the initial ranking of applicants can be adjusted to ensure a fair representation of students from diverse socioeconomic backgrounds.

\subsubsection{Threshold Adjustments}
Threshold adjustments involve modifying the decision thresholds of the model based on fairness considerations. By setting different decision thresholds for various demographic groups, we can balance false positives and false negatives across these groups.

\textbf{Example:} In a predictive policing model, different arrest thresholds can be applied to balance false arrest rates between different neighborhoods, ensuring fair law enforcement practices.

\subsubsection{Additive Counterfactuals}
This technique generates counterfactuals or alternative outcomes by modifying the model's predictions while keeping the sensitive attributes fixed. It aims to provide individuals with insights into how they might have been treated in a fairer system.

\textbf{Example:} In a loan approval system, generating counterfactuals can show rejected applicants how their outcomes would have differed with a fairer decision-making process.

\subsubsection{Equalized Odds Post-processing}
Equalized odds post-processing adjusts the predicted probabilities for each group to ensure that the false positive and false negative rates are equal across different demographic categories. This promotes fairness in the allocation of benefits or opportunities.

\textbf{Example:} In a job application screening model, the predicted probabilities of being shortlisted can be equalized for candidates from different age groups to minimize age-based discrimination.

\subsubsection{Reject Option Classification}
This technique involves adding a "reject" category to the model's output, providing an alternative for cases where the model's predictions may be biased. The reject option aims to ensure fairness by withholding potentially biased predictions.

\textbf{Example:} In a credit scoring model, if the prediction is close to the approval threshold, the model may opt for rejection to prevent biased decisions.

\subsubsection{Oversampling for Minority Groups}
In this technique, the predictions for the minority group are oversampled to balance the representation of outcomes, aiming to reduce biases and ensure fair treatment.

\textbf{Example:} In a medical diagnosis model, oversampling predictions for rare diseases ensures that the model is equally proficient in predicting both common and rare conditions.

\subsubsection{Confidence Calibration}
Confidence calibration adjusts the predicted probabilities to align with the actual probabilities of the predicted outcomes. This helps in reducing overconfidence or underconfidence in the model's predictions.

\textbf{Example:} In a fraud detection system, confidence calibration ensures that the predicted fraud probabilities accurately reflect the actual likelihood of fraud, promoting fair treatment of all cases.

These post-processing techniques are crucial in rectifying biases and promoting fairness in AI models. Utilizing them effectively can lead to more equitable outcomes and decisions across various demographic categories.


In conclusion, fairness in AI is not just an ethical necessity but a fundamental requirement for fostering trust and ensuring the responsible and equitable deployment of AI systems. As we navigate the evolving landscape of AI, our collective efforts must prioritize fairness to harness the true potential of AI for the greater good.

%Write background here.

%This section is likely to contain a lot of citations.
%
%For instance in \cite{AnzengruberSocInfo2013} the authors propose a novel means for tackling with the problem of preventing bad things from happening.

%----------------------------------------------------------------------------------------
\chapter{Design} % possible chapter for Projects
\label{chap:design}
%----------------------------------------------------------------------------------------
In the subsequent discussion, we will delve into algorithms specifically crafted to address and mitigate fairness concerns. Fairness in algorithms has gained significant attention in recent years due to the realization that traditional computational processes can inadvertently perpetuate biases and disparities. Various approaches, ranging from re-weighting instances to modifying decision boundaries, aim to rectify these issues and promote a more equitable and just application of algorithms across diverse demographics. By exploring these approaches, we aim to shed light on the potential solutions that can contribute to a more inclusive and fair technological landscape.

\subsection{Fairness through Unawareness with Proxy Detection}
Let's consider a dataset \( D \) belonging to \( \mathbb{R}^{n \times m} \) with \( k \) protected variables.

\subsubsection{Formalization of Fairness Evaluation}
We define \textit{fairness\_evaluation} as follows:
\[
\text{fairness\_evaluation}(v_i, Y) = \lambda(v_i, Y) \quad \forall i \in [1, k]
\]

where:
\begin{align*}
v\_i & : \text{ith attribute belonging to the protected variables}, \\
Y & : \text{output column}.
\end{align*}

The fairness function \( \lambda \) evaluates the relationship between a protected attribute \( v_i \) and the output \( Y \), producing a value that represents the level of fairness for that protected attribute.

\subsubsection{Formalization of dataset\_fair}
We define \textit{dataset\_fair} as follows: a dataset \( D \) is considered fair if for every value \( v \) belonging to \textit{fairness\_evaluation}, the following condition holds:
\[ 0.8  < v < 1.25 \]

In this section, we delve into the critical task of identifying proxy variables within the dataset. We specifically concentrate on leveraging fairness metrics, normal variables, and protected variables to determine proxies. Proxy variables are indirect indicators or correlates that may indirectly affect sensitive attributes, potentially introducing bias in our dataset. By examining the relationships between these variables and evaluating them based on fairness metrics, we aim to discern which variables exhibit significant associations with the sensitive attributes, leading to their identification as potential proxies. We focus on utilizing fairness metrics to ensure a comprehensive evaluation that considers both the fairness and equity aspects of the dataset, with the ultimate goal of achieving a more equitable and unbiased data representation.

\subsubsection{Proxy-Free Dataset Function}

Considering the dataset \( D \) belonging to \( \mathbb{R}^{n \times m} \) and \( k \) as the number of identified proxy variables, we define the function \( \text{proxy\_free\_dataset} \) as follows:

\[
\text{proxy\_free\_dataset}: D \times \mathbb{R}^{k \times 1} \rightarrow \mathbb{R}^{n \times (m - k)}
\]

where \( j = m - k \) and \( \text{proxy\_free\_dataset}(D) \) yields a dataset \( \mathbb{R}^{n \times j} \) devoid of the proxy variables, ensuring the removal of any potential indirect indicators that may influence sensitive attributes.

\subsubsection{Protected-Attributes-Free Dataset Function}

Considering the dataset \( D \) belonging to \( \mathbb{R}^{n \times j} \) where j are the columns obtained after the proxy removals and \( k \) as the number of identified protected variables, we define the function \( \text{protected\_attributes\_free\_dataset} \) as follows:

\[
\text{protected\_attributes\_free\_dataset}: D \times \mathbb{R}^{k \times 1} \rightarrow \mathbb{R}^{n \times (j - k)}
\]

where \( p = j - k \) and \( \text{protected\_attributes\_free\_dataset}(D) \) yields a dataset \( \mathbb{R}^{n \times p} \) devoid of the protected variables, ensuring the removal of any potential indirect indicators that may influence sensitive attributes.

Let us now explore the methodology and approach involved in identifying these crucial proxy variables.

\subsection{Proxy detection via attributes only}
Let's consider the set \( A \), represented as the set of all variables in the dataset excluding the protected variables, and the set \( B \), representing the protected variables.

We define \textit{proxy\_detection} as follows: for every variable \( \text{var} \) belonging to \( A \) and for every protected variable \( \text{var\_protected} \) belonging to \( B \), the variable is a proxy if the fairness metric \( \lambda(\text{var}, \text{var\_protected}) \) satisfies the condition:

\[
\lambda(\text{var}, \text{var\_protected}) <= 0.8 \quad \text{or} \quad \lambda(\text{var}, \text{var\_protected}) => 1.25
\]

In other words, a variable \( \text{var} \) is considered a proxy if the fairness measure \( \lambda \) between \( \text{var} \) and a protected variable \( \text{var\_protected} \) falls outside the acceptable range ]0.8, 1.25[.

\subsection{Proxy detection via apriori}

It's important to give a brief background of the Association Rule Mining, the family algorithm Apriori belongs to.

\subsubsection{Association Rule Mining}

Association Rule Mining is a data mining technique used to discover interesting relationships and patterns within large volumes of data. The goal is to identify associations and correlations among various elements present in the data.

\subsubsection{Apriori algorithm}

Suppose we have a set of transactions $T$, each containing a set of items. Let's define:

\begin{itemize}
  \item $I$: the set of all distinct items in the data.
  \item $D$: the set of transactions, each represented by a set of items.
  \item $F_k$: the set of frequent itemsets of length $k$.
\end{itemize}

The Apriori algorithm operates in iterations, generating $F_k$ from $F_{k-1}$.

\textbf{Step 1:} Initialization:
$$F_1 = \{ \text{frequent item } i, i \in I \}$$

\textbf{Step 2:} Candidate itemset generation:
$$C_k = \{ \text{set } c \text{ of items such that } c \subseteq F_{k-1} \}$$

\textbf{Step 3:} Database scan:
$$\text{For each transaction } t \text{ in } D, \text{ increment the support count of each candidate in } C_k \text{ contained in } t.$$

\textbf{Step 4:} Selection of frequent itemsets:
$$F_k = \{ c \in C_k \text{ such that the support of } c \geq \text{ specified threshold} \}$$

\subsubsection{Row Selection with a Confidence Level}
Suppose we have obtained association rules using the Apriori algorithm. Each rule $R$ is in the form $A \rightarrow B$, where $A$ is the antecedent and $B$ is the consequent. The confidence of $R$ is defined as:

\[
\text{Confidence}(R) = \frac{\text{Support}(A \cup B)}{\text{Support}(A)}
\]

To select rows with a certain confidence level, we consider a confidence threshold $C_{\text{min}}$. If the confidence of a rule $R$ exceeds this threshold, the rule is accepted.

Formally, a rule $A \rightarrow B$ is accepted if:

\[
\text{Confidence}(A \rightarrow B) \geq C_{\text{min}}
\]
In our scenario
\[
C_{\text{min}}
\]
is equal to 0.8. \\

Furthermore, in order to have a proper rows selection, we are particularly interested in privacy and data protection. We only want to consider rules where the consequent ($B$) contains at least one item representing a sensitive attribute or a variable that needs to be protected.

The consequent filtering process involves evaluating each rule to ensure it meets this criteria. If the consequent of a rule does not contain any sensitive attributes, we discard that rule from our final selection. \\

Formally, a rule $A \rightarrow B$ is considered if:

\[
\exists b \in B : b \in S
\]

If this condition is not met, the rule is discarded from our final selection.

\subsubsection{Identifying Proxy Variables with Apriori}
Now we can go on with the definition of proxies using Apriori. \\

We define \textit{proxy\_detection} as follows: for each antecedent \( A_i \) belonging to the antecedent list \( \mathcal{A} \), for each consequent \( C_j \) belonging to the consequent list \( \mathcal{C} \), and for each protected variable \( V_k \) belonging to the protected variable list \( \mathcal{V} \), \( A_i \) is a proxy if the fairness metric \( \lambda(A_i, C_j) \) is such that:

\[\lambda(A_i, C_j) <= 0.8 \quad \text{or} \quad \lambda(A_i, C_j) >= 1.25\]

In other words, an antecedent \( A_i \) is considered a proxy if the fairness measure \( \lambda \) between \( A_i \) and a consequent \( C_j \) is outside the acceptable range ]0.8, 1.25[.

\section{Pseudocode}

Here's the pseudocode of the \emph{Fairness through unawareness with proxy detection}:

\begin{verbatim}
    while not dataset_fair(dataset) or protected_attributes in dataset:
        proxies = proxy_detection(dataset)
        if proxies is empty:
            dataset = protected_attributes_free_dataset(dataset)
        else:
            dataset = proxy_free_dataset(dataset)
    return dataset
\end{verbatim}

\subsection{dataset\textunderscore fair pseudocode}
Here's the pseudocode that establish if the dataset is either \emph{Fair} or not
\begin{verbatim}
    if fairness_evaluation(dataset, protcted_attribute) is empty:
        return True
    return False
\end{verbatim}

\subsection{fairness\textunderscore evaluation pseudocode}
Here's the pseudocode to perform the fairness evaluation, given a specific \emph{fairness metric}
\begin{verbatim}
    fairness_evaluation_list
    output = dataset[output_column]
    for attribute in protected attribute:
        if metric(attribute, output) <= 0.8 or metric(attribute, output) >= 1.25:
            fairness_evaluation_list.append(attribute)
\end{verbatim}

\subsection{protected\textunderscore attributes\textunderscore free\textunderscore dataset pseudocode}
Here's the pseudocode to remove protected attributes from the dataset
\begin{verbatim}
    for attribute in protected_attributes:
        dataset.remove(attribute)
    return dataset
\end{verbatim}

\subsection{proxy\textunderscore free\textunderscore dataset pdeudocode}
Here's the pseudocode to remove proxy attributes from the dataset
\begin{verbatim}
    for attribute in proxy_list:
        dataset.remove(proxy)
    return dataset
\end{verbatim}

\subsection{proxy\textunderscore detection pseudocode}
Here's the pseudocode of proxy\textunderscore detection function. There are 2 scenarios that are presented here, the one in which the \emph{variables only}  approach is used and the one in which the \emph{apriori} approach is used

\subsubsection{variables only}
\begin{verbatim}
    proxy_list = []
    for protected_attribute in protected_attributes_list:
        for attribute in attributes and not in protected_attribues_list and not in proxy_list:
            if metric(attribute, protected_attribute) <= 0.8 or metric(attribute, protected_attribute) >= 1.25:
                proxy_list.append(attribute)
    return proxy_list
\end{verbatim}

\subsubsection{apriori}
\begin{verbatim}
    proxy_list = []
    for consequent in apriori_dataset:
        for antecedent in apriori_dataset:
            if metric(antecedent, consequent) <= 0.8 or metric(antecedent, consequent) >= 1.25:
                proxy_list.append(antecent)
    return proxy_list
\end{verbatim}

\subsection{Fairness through data rebalancing}

In recent years, the rise of automated decision-making systems has necessitated an examination of fairness and equity within algorithmic processes. Addressing fairness concerns, particularly in domains involving sensitive attributes, has become crucial to ensure that the resulting decisions are not biased against specific groups or individuals. Traditional approaches to fairness often revolve around defining and optimizing for specific fairness metrics, such as demographic parity, equal opportunity, or disparate impact. However, these metrics might not capture the complex interplay of variables that influence fairness.

Here, we propose an innovative approach to rebalancing algorithms that transcends the limitations of conventional fairness metrics. Our focus is on achieving a state of equilibrium, where the occurrences of all possible combinations of values for protected variables and the output variable are balanced. This approach aims to mitigate bias and promote fairness by distributing the outcomes more evenly across different segments of the dataset, while ensuring that no specific value combination is disproportionately favored. By fostering a sense of balance and inclusivity in the decision-making process, we contribute to a more equitable framework for automated systems.

\subsubsection{General concepts}
Let \( D \) be a dataset \( R^{n \times m} \), where \( n \) is the number of samples and \( m \) is the number of features. Let \( k \) be the number of protected variables represented as \( R^{n \times 1} \), and let there be a single output variable represented as \( R^{n \times 1} \).

A rebalancing function \( \mathcal{R} \) can be formally defined as a mapping:

\[
\mathcal{R}: R^{n \times m} \rightarrow R^{l \times m}
\]

where \( l > m \), and the function \( \mathcal{R} \) transforms the input dataset \( D \) of dimensions \( n \times m \) into an output dataset \( D' \) of dimensions \( l \times m \).

\subsubsection{Number of Rows of the Resulting Dataset}

Let \( k \) be the number of binary protected variables in the dataset \( D \), and consider the output variable to be binary as well. The number of possible combinations of these variables is \( 2^{(k+1)} \).

Consider a set \( \text{Combination-frequency} \) where we have occurrences of all \( 2^{(k+1)} \) combinations within the dataset. For each combination, the number of rows in which that combination appears should be equal to the maximum occurrence among all combinations present in the set \( \text{Combination\textunderscore frequency} \). This maximum value is denoted as \( \text{Max}(\text{Combination-frequency}) \).

Mathematically, the number of rows (\( l \)) the final dataset should have for each combination is given by:

\[
l = \text{Max}(\text{Combination-frequency})
\]


\subsubsection{Rebalancing}

Let \( l \) be the desired number of rows for the final dataset. For each combination of values, we calculate the occurrence count \( \text{occurrence}_i \), where \( i \) ranges from 1 to \( 2^{(k+1)} \), with \( k \) being the number of protected binary variables and considering the output variable as binary.

The total number of rows to be added is given by:

\[
\text{total\_rows\_to\_add} = l - \sum_{i=1}^{2^{(k+1)}} \text{occurrence}_i
\]

For each iteration:
\begin{itemize}
    \item The values of the protected and output variables are set according to the specific combination.
    \item For all other attributes, a random value \( \text{random\_value}_{ij} \) is generated, where \( j \) represents the specific attribute and \( i \) represents the row being added for that attribute. \( \text{random\_value}_{ij} \) is within the minimum and maximum range for attribute \( j \).
\end{itemize}

\subsection{Pseudocode}
\begin{verbatim}
    max_frequency = max(combination_frequency)
    for index in range(0, len(combination_set)):
        combination = combination_set[index]
        frequency = combination_frequency[index]
        while frequency < max_frequency:
            new_row = empty
            for attr, val in (combination, protected_attributes)
                new_row[attr] = val

            for attr in dataset_attributes and not in protected_attribues:
                new_row[attr] = random(max(attr), min(attr))

            dataset.add(new_row)
            frequency += 1
    
\end{verbatim}

%You may want to reference images in your thesis.
%
%In this case, you are encouraged to make them \emph{floating}, and reference them by means of labels.
%
%For instance, in \Cref{fig:classes}, we describe a class diagram produced by means of \href{http://plantuml.com}{PlantUML}.

%----------------------------------------------------------------------------------------
\chapter{Implementation} % possible chapter for Projects
\label{chap:implementation}
%----------------------------------------------------------------------------------------

Write implementation here.

\lstinputlisting[
	float,
	language=Java,
	caption={My very first program in Java},
	label={lst:helloworld},
]{listings/HelloWorld.java}

You may need to reference listings in your thesis.
%
In this case, you are encouraged to make them \emph{floating}, and reference them by means of labels.
%
For instance, in \Cref{lst:helloworld}, we describe an hello world program in Java.

%----------------------------------------------------------------------------------------
\chapter{Validation} % possible chapter for Projects
\label{chap:validation}
%----------------------------------------------------------------------------------------

Write implementation here

%----------------------------------------------------------------------------------------
\chapter{\conclusionsname}
\label{chap:conclusions}
%----------------------------------------------------------------------------------------

Write conclusions here.


%----------------------------------------------------------------------------------------
% BIBLIOGRAPHY
%----------------------------------------------------------------------------------------

%\nocite{*} % uncomment this to show all the reference in the .bib file
\bibliographystyle{plain}
\bibliography{bibliography}


\end{document}