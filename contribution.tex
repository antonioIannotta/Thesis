\section{Model Training and Evaluation}
\label{section:model-training}

\subsection{Introduction}

This section provides a comprehensive account of the model training and evaluation process within the Fair-by-Design framework. Following the meticulous application of the selected pre-processing, in-processing, or post-processing algorithm, the training phase becomes pivotal for crafting a machine learning model that aligns not only with accuracy objectives but also with the ethical considerations of fairness.

\subsection{Training Process}

\subsubsection{Data Splitting}

The dataset undergoes a thoughtful split into training, validation, and test sets. This division serves a strategic purpose: the training set propels the model's learning, the validation set aids in hyperparameter tuning to enhance generalization, and the test set evaluates the final model's performance, providing a reliable measure of its predictive capabilities.

\subsubsection{Model Architecture}

The architecture of the chosen model is crafted with a delicate balance between accuracy and fairness considerations. Architectures may include innovative components, such as adversarial layers or customized modules designed to enforce fairness constraints. The emphasis is on creating a model that is not only proficient in capturing complex patterns but also interpretable and transparent in its decision-making.

\subsubsection{Training Parameters}

Key training parameters, including the learning rate, batch size, and convergence criteria, are meticulously tuned. The learning rate dictates the step size in the optimization process, influencing the rate of model parameter updates. The batch size determines the number of samples processed in each iteration, impacting the efficiency and resource utilization. Convergence criteria ensure that the training process halts when the model reaches optimal performance, preventing overfitting.

\subsubsection{Fairness Constraints (if applicable)}

In scenarios where fairness constraints are integrated into the training process, they are defined and enforced. This may involve the introduction of regularization terms designed to penalize disparate treatment of different demographic groups. Adversarial training components might also be employed to mitigate bias and promote equitable outcomes.

\subsubsection{Model Training}

The model undergoes training for a predetermined number of epochs. Leveraging the training set, the model iteratively updates its parameters to minimize the loss function. This iterative process aims at achieving a balance between accuracy and fairness, contributing to the development of a machine learning model that is not only adept at capturing underlying patterns but also conscious of potential biases.

\subsection{Evaluation Metrics}

\subsubsection{Accuracy Metrics}

A suite of accuracy metrics is employed to holistically assess the model's overall performance:

\begin{itemize}
    \item \emph{Accuracy:} The ratio of correctly predicted instances to the total instances.
    
    \item \emph{Precision:} The proportion of true positive predictions among instances predicted as positive.
    
    \item \emph{Recall:} The proportion of true positive predictions among actual positive instances.
    
    \item \emph{F1-score:} The harmonic mean of precision and recall, providing a balanced measure of accuracy.
\end{itemize}

\subsubsection{Fairness Metrics}

Fairness metrics are instrumental in evaluating the model's behavior across different demographic groups:

\begin{itemize}
    
    \item \emph{Equalized Odds:} Evaluates whether the model provides similar false positive and false negative rates across different demographic groups.
    
    \item \emph{Statistical Parity:} Examines the proportion of positive outcomes for each demographic group to identify disparities.
\end{itemize}

\subsection{Results and Analysis}

\subsubsection{Accuracy Results}

A meticulous analysis of accuracy results provides valuable insights into how well the model predicts the target variable. Precision, recall, and F1-score metrics offer a nuanced understanding of the trade-offs between true positives, false positives, and false negatives. The goal is not just accuracy but a balanced and informed prediction capability.

\subsubsection{Fairness Results}

Fairness metrics serve as a critical lens to identify and address biases within the model. Disparate impact, equalized odds, and statistical parity results are analyzed to assess whether the model exhibits disparate treatment of different demographic groups. The focus is on identifying and mitigating disparities, fostering a model that is fair and equitable.

\subsubsection{Trade-offs}

Understanding the trade-offs between accuracy and fairness is vital. In pursuit of a balanced model, adjustments may be necessary. Techniques such as re-weighting, re-sampling, or customized loss functions may be employed to address specific fairness concerns. Striking the right balance becomes a central theme in refining the model for optimal performance.

\subsection{Discussion}

\subsubsection{Interpretability}

The interpretability of the model's decisions is paramount. Techniques such as SHAP (SHapley Additive exPlanations) values or LIME (Local Interpretable Model-agnostic Explanations) are explored to enhance transparency in the decision-making process. Understanding how the model arrives at its predictions contributes to the model's trustworthiness.

\subsubsection{Ethical Considerations}

The discussion extends to ethical considerations arising from the model's behavior. Attention is given to potential biases, unintended consequences, and the impact on different demographic groups. Strategies for addressing ethical concerns are proposed, underscoring the commitment to responsible and ethical AI practices.

\subsubsection{Further Refinement}

Based on the observed results, proposals for further refinement are put forth. This may involve adjusting fairness constraints, fine-tuning hyperparameters, or exploring alternative algorithms to enhance both accuracy and fairness in model predictions. The iterative nature of this refinement process reflects the commitment to continuous improvement and the pursuit of fair

\section{Model Deployment}
\label{section:model-deployment}

\subsection{Introduction}

Model deployment is a critical phase in the fair-by-design framework, where the fair machine learning model transitions from development to practical application in real-world decision-making processes. This phase emphasizes the integration of the model into decision systems, ongoing monitoring to ensure fairness, user education, and iterative refinement based on feedback.

\subsection{Integration with Decision Systems}

The seamless integration of the fair machine learning model into decision systems is guided by the principles of fairness and ethical AI practices. Attention is given to the alignment of the model with existing infrastructure, ensuring that its predictions contribute meaningfully to decision-making processes. This integration involves a careful consideration of potential biases and fairness implications at every stage of the decision-making pipeline.

\subsection{Monitoring and Feedback Loops}

A robust monitoring system is established to continuously evaluate the model's performance in real-world scenarios. This monitoring process extends beyond traditional accuracy metrics, placing a specific focus on fairness indicators. Feedback loops are implemented to capture any deviations from fairness objectives, changes in the data distribution, or emerging ethical considerations.

User feedback becomes a crucial component of the monitoring process. Users interacting with the model contribute valuable insights into its real-world impact. The feedback loops serve as a mechanism for users to report any perceived biases, disparities, or unintended consequences in the model's predictions. This user-centric approach ensures that the model's deployment remains responsive to the needs and concerns of the diverse stakeholders involved.

\subsection{User Education and Awareness}

Stakeholders engaging with the fair machine learning model are educated about the underlying fairness considerations. Awareness programs are designed to foster an understanding of the ethical use of the model, potential biases that may arise, and the significance of interpreting predictions within the context of fairness objectives. The goal is to empower users to make informed decisions while considering the model's capabilities and limitations.

User education extends to explaining the model's predictions, the factors influencing its decisions, and the steps taken to ensure fairness. Transparent communication is key to building trust among users, encouraging responsible and ethical use of the model in diverse decision-making scenarios.

\subsection{Iterative Refinement}

Model deployment marks a milestone, but it is not the endpoint in the fair-by-design workflow. An iterative refinement process is integral to addressing emerging fairness challenges. User feedback, monitoring results, and ethical considerations inform ongoing refinements to the model. The iterative nature of this approach ensures that the fair machine learning model remains adaptive and responsive to evolving fairness concerns.

Refinements may include adjustments to the model's algorithms, re-evaluations of fairness metrics, and enhancements to the user interface for better interpretability. The iterative refinement process embodies a commitment to continuous improvement, reinforcing the ethical foundation of the fair-by-design framework and this iterative approach must always involve the objective definition, to detect if any of the previous ones are changed or not and have an adaptive behavior about that.

\subsection{Discussion}

\subsubsection{Reflection on Model Deployment}

Reflecting on the model deployment phase involves a thorough examination of the fairness implications observed in real-world applications. Considerations include the effectiveness of fairness measures, the impact on decision-making outcomes, and any unforeseen challenges that arose during deployment. This reflection provides valuable insights into the model's practical performance and its alignment with fairness objectives.

\subsubsection{User Feedback and Fairness}

User feedback plays a pivotal role in assessing the fairness of the deployed model. Explore specific instances where user feedback contributed to the identification and mitigation of biases. Discuss the mechanisms in place for users to provide feedback, the responsiveness of the system to reported concerns, and the overall impact on fairness in decision-making.

\subsubsection{Future Directions}

In proposing future directions, emphasize the importance of refining model deployment strategies based on fairness implications and user feedback. Consider how emerging technologies, evolving ethical standards, and advancements in AI research can be leveraged to enhance the fairness and ethical considerations of the deployed model. Outline potential research avenues that focus on user-centric fairness in real-world applications.

%----------------------------------------------------------------------------------------
\chapter{Technical details}
\label{chap:technicals}

\section{Proposed algorithm}

It's important to provide technical details about the implementation of the two algorithms proposed above.

\subsection{Fairness through data rebalancing}

An essential facet of the Fairness through Rebalancing algorithm within our workflow lies in the nuanced process of generating data to augment the dataset. At each iteration of the algorithm, the combination value is selected, pinpointing the subset of the dataframe where that specific combination occurs. Within this subset, a meticulous computation unfolds.

\lstinputlisting[
	%float,
	language=Python,
	caption={Frequent values in the sub-dataset},
	label={lst:frequent_value},
]{listings/frequent_value.py}

For each value outside the combination set, the algorithm discerns both the least frequent and most frequent occurrences. The next step involves randomly selecting a value for the new row to be added to the dataset. This random selection occurs within the range defined by the least frequent and most frequent values. The careful orchestration of this process ensures that the augmented dataset maintains statistical coherence while introducing the necessary variations to rebalance the fairness aspects within the model.

This deliberate approach to data generation underscores the commitment to preserving the integrity of the dataset while mitigating potential biases, contributing to the overarching goal of developing fair and unbiased machine learning models.

\section{Workflow impact on Software Architecture}

The proposed workflow, encompassing objective definition, stakeholder identification, data collection, data pre-processing, algorithms design, model training, and performance evaluation, presents a unique challenge and opportunity for integration within the software architecture. While the first two steps, objective definition and stakeholder identification, are inherently external to the software architecture, focusing on the overarching goals and involved parties, the subsequent steps demand a meticulous organizational structure within the software development process.

To ensure a modular and well-organized architecture, each step from data collection to performance evaluation can be encapsulated into distinct packages. This modularization enhances maintainability, scalability, and the overall robustness of the system. Importantly, this encapsulation promotes the concept of isolation, allowing each step of the algorithm to function independently with information exchange occurring exclusively through well-defined interfaces.

However, the introduction of fairness metrics adds a layer of complexity to this organizational structure. Since fairness considerations permeate multiple steps of the workflow, creating a dedicated package for fairness metrics becomes crucial. This separation is vital to ensure the decoupling of fairness concepts from the intricacies of individual steps such as data pre-processing or algorithm design.

In this approach, fairness is not treated as a step-specific concern but as a cross-cutting and orthogonal concept. By encapsulating fairness metrics within a dedicated package, the software architecture acknowledges the overarching nature of fairness in the workflow. This design choice facilitates the systematic integration of fairness considerations, allowing for a unified approach that transcends individual steps and aligns with the holistic perspective embedded in the fair-by-design methodology.

In summary, the proposed workflow aligns with common software development practices by organizing each step into separate packages, fostering modularity and encapsulation. The introduction of a dedicated fairness metrics package further emphasizes the universal and pervasive nature of fairness throughout the workflow, ensuring a cohesive and well-structured software architecture that reflects the core principles of the fair-by-design approach.

%----------------------------------------------------------------------------------------
\chapter{Implementation of Fair-by-Design Workfow in Canary Island Educational Performance Prediction}
\label{chap:real-world-scenario}

\section{Introduction}

The implementation of machine learning models in real-world contexts demands a careful consideration of ethical and fairness implications, especially in domains with significant societal impact, such as education. In this chapter, we delve into the application of the fair-by-design workflow to the specific context of predicting educational performance in the Canary Islands. The objective is not merely to develop an accurate predictive model but to ensure that the predictions align with fairness principles, fostering equitable opportunities for all students.

The educational landscape presents a complex interplay of factors, including socio-economic backgrounds, cultural diversity, and historical disparities. Traditional machine learning models may inadvertently perpetuate biases, leading to unequal treatment and outcomes for different student groups. The fair-by-design workflow offers a systematic approach to address these challenges, providing a framework where fairness is ingrained from data preprocessing to model deployment.

In the following sections, we navigate through the key stages of the fair-by-design workflow as applied to the educational performance prediction scenario in the Canary Islands. This journey encompasses stakeholder identification, meticulous data collection, thoughtful consideration of protected attributes, and the design and evaluation of machine learning algorithms. The chapter concludes with reflections on the deployment of the fair machine learning model, emphasizing the iterative refinement process guided by user feedback and ongoing monitoring.

\section{Objective Definition}

The primary objective of this study is to predict the English proficiency level for individual students in the educational landscape of the Canary Islands. The predictive model aims to provide accurate assessments, contributing to a better understanding of students' language capabilities. However, beyond accuracy alone, a crucial emphasis is placed on the integration of fairness considerations throughout the prediction process. This entails a meticulous examination of potential biases and disparities related to sensitive attributes, ensuring that the model's predictions are not only precise but also ethically sound and equitable for all student groups. The fair-by-design approach is woven into the fabric of the study to uphold the principles of fairness and inclusivity, fostering a predictive model that aligns with both technical excellence and ethical considerations.

\subsection{Performance Evaluation Metrics}

The performance of the predictive model will be assessed using two key metrics: accuracy and fairness. Accuracy, representing the model's ability to precisely predict English proficiency levels, is paramount for ensuring the practical utility of the system. A highly accurate model contributes to effective decision-making, offering valuable insights into students' language skills.

Simultaneously, the fairness of predictions across different demographic groups is of utmost importance. Protected attributes, such as gender, socio-economic background, or ethnic origin, often play a role in educational disparities. To address potential biases, the model will be evaluated based on fairness metrics, ensuring that predictions do not disproportionately favor or disfavor specific subpopulations. This dual evaluation approach aligns with the fair-by-design framework, aiming to create a predictive model that excels both in accuracy and equity.

\subsection{Fairness as a Core Principle}

Fairness is not an ancillary consideration in this study but a core principle embedded in the entire predictive workflow. The goal is to develop a machine learning model that not only excels in accuracy but also demonstrates equitable treatment across diverse student groups. By explicitly considering protected attributes, such as gender, socio-economic background, or ethnic origin, the model aims to mitigate potential biases that could adversely impact historically marginalized or disadvantaged populations. This approach reflects a commitment to fairness from the outset, ensuring that the model's predictions contribute to educational insights without perpetuating or exacerbating existing disparities.

\subsection{Protected Attributes and Ethical Considerations}

The identification of protected attributes, such as gender, socio-economic background, and ethnicity, is crucial for the fair-by-design approach. Understanding the potential impact of these attributes on predictions allows for targeted interventions to ensure fairness. Ethical considerations guide the responsible handling of sensitive information, emphasizing the importance of transparency and accountability in the entire predictive process.

\subsection{Balancing Accuracy and Fairness}

The challenge lies in striking a balance between maximizing accuracy and ensuring fairness. While accuracy is pivotal for providing valuable insights, fairness guarantees that the benefits of accurate predictions are distributed equitably across diverse student populations. The fair-by-design workflow incorporates algorithmic strategies and preprocessing techniques to achieve this delicate equilibrium.

In summary, the objective of this study is two-fold: to predict the English proficiency level for individual students with a high degree of accuracy and to ensure the fairness of these predictions across protected attributes. This dual commitment reflects a dedication to not only advancing the state-of-the-art in educational performance prediction but also contributing to a more equitable and ethical application of machine learning in education.

\section{Stakeholder Identification}

In any data-driven research endeavor, identifying and understanding the key stakeholders is paramount to the success and ethical conduct of the study. In the context of predicting English proficiency levels for students in the Canary Islands, one of the primary stakeholders playing a pivotal role is the University of La Laguna (ULL).

\subsection{Primary Stakeholder: University of La Laguna (ULL)}

The University of La Laguna serves as a central figure in this study, functioning as the primary data provider. ULL's comprehensive database encompasses a wealth of information, including student demographics, academic performance records, and English proficiency assessments. The institution's commitment to academic excellence and research makes it a key stakeholder in shaping the trajectory and outcomes of this study.

As the custodian of the data used for predictive modeling, ULL's engagement is critical for ensuring the accuracy, relevance, and ethical use of the information. The university's active involvement in the study is not merely as a data source but as a collaborator in the pursuit of equitable and insightful predictions regarding students' English proficiency levels.

\subsection{Additional Stakeholders}

While ULL holds a central role, other stakeholders contribute to the broader context of this study. These may include educational policymakers, English language instructors, students, and parents, each with unique perspectives and interests in the outcomes of the predictive model. Identifying and engaging with these stakeholders enriches the study by incorporating diverse viewpoints and fostering a more comprehensive understanding of the implications of the research.

\section{Data collection}

It's important, once having presented the main stakeholder, present the data collected.

\subsection{Dataset description}

Before delving into the intricate details of the algorithm implementations presented earlier, it is imperative to provide a comprehensive overview of the dataset on which these algorithms have been applied. The chosen dataset for this work is the \emph{Canary Island Educational dataset}, an invaluable resource that underpins the empirical exploration of bias mitigation strategies in the context of the educational system in the Canary Islands. 

The Canary Island Educational dataset is a rich and expansive repository of information, meticulously compiled to capture various facets of the educational landscape within the Canary Islands. This dataset comprises the comprehensive census of students enrolled over four distinct academic years, offering a multifaceted glimpse into the educational ecosystem. 

The dataset encompasses a diverse array of attributes and data points, encapsulating critical information such as student demographics, academic performance, socioeconomic factors, and other pertinent variables. These attributes collectively provide a holistic perspective on the educational landscape, enabling a nuanced analysis of the factors that influence student outcomes and experiences. 

The temporal dimension of the dataset, spanning four academic years, further enriches the analytical potential. It allows for the investigation of temporal trends, shifts in educational policies, and the evolution of student characteristics over time. This temporal depth is particularly valuable when examining the efficacy of bias mitigation strategies, as it facilitates the assessment of their impact across different academic years. 

The Canary Island Educational dataset is not merely a repository of numbers and statistics; it is a window into the educational opportunities and challenges faced by students in the Canary Islands. By harnessing the insights gleaned from this dataset, it becomes possible to proactively address biases and promote equity within the educational system, ultimately striving for a more inclusive and just educational landscape.

\subsubsection{Pre-processing operations on Canary Island Educational dataset}

The first operation required to choose the subset on which implement the chosen algorithm. It has been choice the subset of the students enrolled in the 3rd grade due its amount of data, greater than the other subsets, and then more suitable to fit a real world scenario.

A deep analysis of the dataset led us to make a first features selection. More specifically for this work only the \emph{important} and \emph{protected} attributes have been selected.

After a proper domain analysis the protected attributes selected to be passed to the algorithms have been a subset of the orginal selected:

\begin{enumerate}

    \item sex

    \item capital island: if the student comes from the capital of the city

    \item public\textunderscore private: if the school is public or private

    \item parent expectation
    
    \item mothly houseold income

    \item economic, social and cultural satus index

\end{enumerate}

The sensitive attributes detection, and its filtering to detect the main relevant avoiding semantic duplicates, has been possible only through the information provided by the ULL about the dataset. As specified in the general workflow the interation between the technical figuers and the stakeholder is even more fundamental when the sensitive attributes are taken into account in the AI system design.

The dataset documentation provided the information related to the type of each attribute (e.g. Continuous or Categorical). Starting from this information the variables have been pre-processed as established in the usual data pre-processing sub-step of the \emph{Fair-by-Design Workflow} proposed in this work.

\subsection{Fairness Assessment}

Having detected the sensitive attributes within the dataset, a crucial step in the fair-by-design workflow involves a comprehensive fairness assessment. This assessment aims to quantify and evaluate any potential biases that may exist in the model predictions, focusing on the disparate impact over the identified protected attributes. The fairness assessment is not merely a formality but a meticulous examination designed to ensure the model's equitable treatment across different demographic groups. By leveraging appropriate fairness metrics, statistical analyses, and quantitative evaluations, this assessment plays a pivotal role in upholding the ethical foundation of the machine learning model and guiding further refinements in pursuit of fairness and unbiased outcomes.

\section{Data pre-processing}

The protected attributes and the output have been pre-processed according to the workflow's rule described in \cref{section:pre-proc}

\section{Algorithm Design}

The success of the fair-by-design workflow in predicting English proficiency levels for students in the Canary Islands hinges on the thoughtful selection and implementation of fairness algorithms across three key categories: pre-processing, in-processing, and post-processing. Each category plays a distinctive role in mitigating biases and ensuring equitable predictions.

\subsection{Pre-processing Algorithms}

In the realm of pre-processing, where the focus is on transforming the input data before it reaches the core machine learning model, the novel fairness algorithm have been proposed and integrated into the fair-by-design workflow.

\subsubsection{Fairness Through Data Rebalancing}

The second pre-processing algorithm, fairness through data rebalancing, addresses imbalances in the distribution of protected attributes. Recognizing that an uneven representation of different demographic groups can lead to biased predictions, this algorithm employs strategic rebalancing techniques. By adjusting the weights assigned to instances based on their protected attributes, the algorithm promotes a more equitable learning process, fostering fair treatment across diverse student populations.

\subsection{In-Processing Algorithm}

Moving to the in-processing stage, where fairness considerations are incorporated directly into the learning process, a state-of-the-art algorithm from the IBM's \emph{AI Fairness 360} (AIF360) library has been employed.

\subsubsection{GridSearch Reduction}

The in-processing algorithm chosen for fairness enhancement is GridSearch Reduction from the \emph{Fairlearn} library. GridSearch Reduction is a powerful technique that systematically explores a grid of hyperparameters to find the optimal combination that minimizes bias in model predictions. It employs a reduction-based approach, optimizing the trade-off between accuracy and fairness by iteratively adjusting model parameters.

GridSearch Reduction operates by conducting a grid search over specified hyperparameter values, considering fairness metrics along with accuracy. This approach allows for the identification of an optimal set of hyperparameters that achieves a balance between predictive performance and fairness objectives. The algorithm aims to discover the most suitable configuration that mitigates bias in the model's decision-making process, ensuring equitable outcomes for diverse student groups. \cite{agarwal2018reductions}

\subsection{Post-Processing Algorithm}

In the post-processing phase, which occurs after the model has generated predictions, an algorithm from the \emph{Fairlearn} library has been selected to further refine fairness.

\subsubsection{Equalized Odds Post-Processing with Threshold Optimizer}

The chosen post-processing algorithm is Equalized Odds Post-Processing with Threshold Optimizer from Fairlearn. This algorithm focuses on adjusting decision thresholds to ensure equalized odds across different demographic groups. By optimizing the threshold values based on protected attributes, the algorithm seeks to rectify any remaining disparities in prediction outcomes, striving for a more balanced and fair distribution of positive and negative predictions. \cite{10.5555/3157382.3157469}

Equalized Odds Post-Processing with Threshold Optimizer involves fine-tuning the decision boundaries to align with fairness considerations. The threshold optimizer dynamically adjusts thresholds for different groups, aiming to achieve equalized odds without compromising predictive accuracy. This meticulous post-processing step contributes to the overall fairness of the model's predictions, promoting equity and mitigating bias in decision outcomes. 

\subsection{Technology-Agnostic Workflow}

One of the notable strengths of the fair-by-design workflow is its technology-agnostic nature. The workflow is designed to be adaptable and flexible, accommodating various technologies and frameworks to implement its principles. This adaptability allows researchers and practitioners the freedom to choose the tools and technologies that best suit their preferences, expertise, and the specific requirements of their environment.

The real-world scenario of predicting English proficiency levels in the Canary Islands can be implemented using different technologies, and the fair-by-design workflow embraces this diversity. The adoption of diverse technologies may stem from considerations such as the existing technology stack within an organization, the availability of specific machine learning libraries, or the preferences of the data scientists involved in the project.

For instance, the workflow can be implemented using popular machine learning libraries such as scikit-learn, TensorFlow, PyTorch, or proprietary tools specific to a particular organization's infrastructure. The choice of technologies for data storage, processing, and model deployment can also vary based on the organizational context.

The technology-agnostic nature of the fair-by-design workflow underscores its universality and applicability across different machine learning ecosystems. This adaptability allows for the replication of the workflow in various settings, fostering collaboration and knowledge sharing among researchers who may have different technology preferences.

By being agnostic to specific technologies, the fair-by-design workflow encourages innovation and experimentation, enabling the broader machine learning community to apply ethical and fair machine learning practices regardless of the technological landscape. This adaptability ensures that the principles of fairness, transparency, and accountability are not confined to a particular technology stack but can be integrated seamlessly into diverse machine learning environments.

\section{Model Training and Evaluation}

The successful implementation of the fair-by-design workflow requires not only the training of an accurate predictive model but also a meticulous evaluation process that considers both predictive performance and fairness. In this section, we detail the steps involved in training the model, evaluating its accuracy over the test set, and assessing fairness using functions from the fairlearn library.

\subsection{Model Training}

The predictive model undergoes a rigorous training process using the pre-processed and fairness-enhanced dataset. Leveraging the fair-by-design algorithms, the model is trained to predict English proficiency levels for students in the Canary Islands. The learning process involves optimizing parameters, minimizing loss functions, and adapting the model to the intricacies of the dataset.

\subsection{Model Evaluation}

Following the training phase, the model's performance is assessed through rigorous evaluation over a dedicated test set. The accuracy of the model is a primary focus, providing insights into its ability to make precise predictions. The accuracy metric is calculated by comparing the model's predictions to the true English proficiency levels in the test set, quantifying the proportion of correct predictions.

\subsection{Fairness Assessment}

Beyond accuracy, the fair-by-design workflow places a significant emphasis on fairness considerations. To assess the fairness of the model, two functions from the fairlearn library are employed: \emph{equalized\textunderscore odds\textunderscore ratio} and \emph{demographic\textunderscore parity\textunderscore ratio}

\subsubsection{Equalized Odds Ratio}

This function measures the equality of odds for different demographic groups. It computes the ratio of true positive rates between groups, highlighting any disparities in the model's ability to make accurate positive predictions across protected attributes. A value close to 1 signifies equalized odds, indicating fair treatment across diverse student populations.

\subsubsection{Demographic Parity Ratio}

This function evaluates demographic parity, assessing whether the model's predictions are independent of protected attributes. It calculates the ratio of positive predictions for different demographic groups, emphasizing the need for equal representation in positive predictions. A demographic parity ratio close to 1 signals fairness in the distribution of positive predictions.

\section{Model deployment}
\label{section:model-deployment}

After the evaluation step in which is assessed whether the model perform its predcition task with a reasonable trade-off between the accuracy and the fairness the model can be deployed and be used to prediction.
