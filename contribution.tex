\chapter{Contribution} % possible chapter for Projects
\label{chap:contribution}
%----------------------------------------------------------------------------------------
\

\section{Fairness through data rebalancing}

In this approach, the paradigm of bias mitigation takes on a unique and innovative perspective, one that prioritizes data augmentation over attribute removal. Unlike traditional approaches that center on the exclusion of specific attributes, this methodology embraces the concept of data augmentation, introducing a distinctive definition of fairness and equity within the AI system.

The essence of this approach revolves around the augmentation of the dataset by introducing new data instances that offer a more comprehensive and inclusive representation of the underlying population. This expanded dataset is designed to be more diverse, representative, and balanced, transcending the limitations of the original data and fostering a more nuanced understanding of fairness. 

The introduction of augmented data instances leads to a redefined notion of fairness within the AI system. Instead of solely focusing on the absence of biased attributes, fairness is now measured in terms of the dataset's inclusivity and its ability to capture the diversity and nuances present within the population it seeks to serve. 

This approach aligns with the broader philosophy of ensuring that AI systems are equitable, just, and capable of making informed and unbiased decisions. By augmenting the dataset, it strives to bridge the gaps in representation and provide a more equitable playing field for all individuals, regardless of their background or characteristics. 

The process of data augmentation necessitates a careful selection of techniques and methodologies that can introduce new data instances while maintaining the integrity and quality of the dataset. These techniques may encompass oversampling, synthetic data generation, or other data synthesis methods, each tailored to the specific context and objectives of the AI system.

\subsection{A New Definition of Fairness}

In traditional fairness definitions, the focus often revolves around ensuring fair treatment for individual protected attributes, denoted as $A_1, A_2, \ldots, A_k$. While this is undoubtedly crucial, a more comprehensive understanding of fairness calls for an examination of fairness in the context of combinations of protected attributes and the output. A new definition of fairness is proposed, which takes into account the representation of all combinations of $k$ protected attributes and the output, aiming for equitable representation across these combinations.

\subsubsection{Equal Representation of Combinations}

A fair dataset is defined as one in which, for each combination of protected attributes $\{A_1, A_2, \ldots, A_k\}$ and the output $O_j$, the representation is equal and proportional. Mathematically, a dataset is fair if:

\[
\forall i_1, i_2, \ldots, i_k, j: \frac{|D_{i_1, i_2, \ldots, i_k, j}|}{|D|} = \text{constant}
\]

where:
- $D$ is the dataset,
- $|D_{i_1, i_2, \ldots, i_k, j}|$ is the number of samples with the specific combination of protected attributes $A_{i_1}, A_{i_2}, \ldots, A_{i_k}$ and output $O_j$,
- $|D|$ is the total number of samples in the dataset.

This entails that any combination of demographic groups, defined by the protected attributes, and the output should have comparable representation, thereby fostering a balanced and unbiased dataset.

\subsubsection{Promoting Comprehensive Fairness}

By striving for equal representation of combinations of protected attributes, is addressed a fundamental aspect of fairness that transcends individual attributes. This approach provides a more nuanced understanding of fairness by considering the intersections of various demographic groups. It encourages a broader examination of potential biases that may arise when considering multiple attributes simultaneously.

Incorporating this definition of fairness into the dataset rebalancing process enables us to promote a comprehensive notion of fairness, aligning with the principles of equal opportunity and non-discrimination across all combinations of protected attributes. Our subsequent algorithm and experimental evaluation are designed to actualize this definition and demonstrate its effectiveness in achieving a more equitable representation within the dataset.

\subsection{Algorithm description}

Let \( D \) be a dataset \( R^{n \times m} \), where \( n \) is the number of samples and \( m \) is the number of features. Let \( k \) be the number of protected variables represented as \( R^{n \times 1} \), and let there be a single output variable represented as \( R^{n \times 1} \).

A rebalancing function \( \mathcal{R} \) can be formally defined as a mapping:

\[
\mathcal{R}: R^{n \times m} \rightarrow R^{l \times m}
\]

where \( l > m \), and the function \( \mathcal{R} \) transforms the input dataset \( D \) of dimensions \( n \times m \) into an output dataset \( D' \) of dimensions \( l \times m \).



Let \( k \) be the number of binary protected variables in the dataset \( D \), and consider the output variable to be binary as well. The number of possible combinations of these variables is \( 2^{(k+1)} \).

Consider a set \( \text{Combination-frequency} \) with occurrences of all \( 2^{(k+1)} \) combinations within the dataset. For each combination, the number of rows in which that combination appears should be equal to the maximum occurrence among all combinations present in the set \( \text{Combination\textunderscore frequency} \). This maximum value is denoted as \( \text{Max}(\text{Combination-frequency}) \).

Mathematically, the number of rows (\( l \)) the final dataset should have for each combination is given by:

\[
l = \text{Max}(\text{Combination-frequency})
\]



Let \( l \) be the desired number of rows for the final dataset. For each combination of values, is calculated the occurrence count \( \text{occurrence}_i \), where \( i \) ranges from 1 to \( 2^{(k+1)} \), with \( k \) being the number of protected binary variables and considering the output variable as binary.

The total number of rows to be added is given by:

\[
\text{total\_rows\_to\_add} = l - \sum_{i=1}^{2^{(k+1)}} \text{occurrence}_i
\]

For each iteration:
\begin{itemize}
    \item The values of the protected and output variables are set according to the specific combination.
    \item For all other attributes, a random value \( \text{random\_value}_{ij} \) is generated, where \( j \) represents the specific attribute and \( i \) represents the row being added for that attribute. \( \text{random\_value}_{ij} \) is within the minimum and maximum range for attribute \( j \).
\end{itemize}

\subsection{Pseudocode}
\begin{algorithm}[H]
    \SetKwInOut{Input}{Input}
    \SetKwInOut{Output}{Output}

    \Input{combination\_set, combination\_frequency, protected\_attributes, dataset\_attributes}
    \Output{Updated dataset}

    max\_frequency $\gets$ max(combination\_frequency)\;
    \For{index $\gets$ 0 \KwTo len(combination\_set)}{
        combination $\gets$ combination\_set[index]\;
        frequency $\gets$ combination\_frequency[index]\;
        \While{frequency $<$ max\_frequency}{
            new\_row $\gets$ empty\;
            \For{(attr, val) \textbf{in} (combination, protected\_attributes)}{
                new\_row[attr] $\gets$ val\;
            }
            \For{attr \textbf{in} dataset\_attributes \textbf{and not in} protected\_attributes}{
                new\_row[attr] $\gets$ random(min(attr), max(attr))\;
            }
            dataset.add(new\_row)\;
            frequency $+$= 1\;
        }
    }
\end{algorithm}
\newpage
%----------------------------------------------------------------------------------------
%\chapter{Implementation} % possible chapter for Projects
%\label{chap:implementation}
%----------------------------------------------------------------------------------------
\section{Dataset description}
Before delving into the intricate details of the algorithm implementations presented earlier, it is imperative to provide a comprehensive overview of the dataset on which these algorithms have been applied. The chosen dataset for this work is the "ULL dataset," an invaluable resource that underpins the empirical exploration of bias mitigation strategies in the context of the educational system in the Canary Islands. 

The ULL dataset is a rich and expansive repository of information, meticulously compiled to capture various facets of the educational landscape within the Canary Islands. This dataset comprises the comprehensive census of students enrolled over four distinct academic years, offering a multifaceted glimpse into the educational ecosystem. 

The dataset encompasses a diverse array of attributes and data points, encapsulating critical information such as student demographics, academic performance, socioeconomic factors, and other pertinent variables. These attributes collectively provide a holistic perspective on the educational landscape, enabling a nuanced analysis of the factors that influence student outcomes and experiences. 

The temporal dimension of the dataset, spanning four academic years, further enriches the analytical potential. It allows for the investigation of temporal trends, shifts in educational policies, and the evolution of student characteristics over time. This temporal depth is particularly valuable when examining the efficacy of bias mitigation strategies, as it facilitates the assessment of their impact across different academic years. 

The ULL dataset is not merely a repository of numbers and statistics; it is a window into the educational opportunities and challenges faced by students in the Canary Islands. By harnessing the insights gleaned from this dataset, it becomes possible to proactively address biases and promote equity within the educational system, ultimately striving for a more inclusive and just educational landscape.

\subsection{Pre-processing operations on ULL dataset}
A deep analysis of the dataset led us to make a first features selection. More specifically for this work only the \emph{important} and \emph{protected} attributes have been selected.
\subsubsection{Protected attributes choice}
After a proper domain analysis the protected attributes selected to be passed to the algorithms have been a subset of the orginal selected:
\begin{enumerate}
    \item sex
    \item capital island: if the student comes from the capital of the city
    \item public\textunderscore private: if the school is public or private
    \item inmigrant: if the student is either inmigrant or not
    \item inmigrant second gen: if the student is either inmigrand of second gen or not
    \item parent expectation
    \item mothly houseold income
    \item cconomic, social and cultural satus index
\end{enumerate}
\subsubsection{Attributes pre-processing}
The dataset documentation provided the information related to the type of each attribute (e.g. Continuous or Categorical). Starting from this information the variables have been pre-processed as follows.
\begin{itemize}
    \item Each categorical attribute has been replaced with an encoded value
    \item For each protected attribute has been computed the following operation:
    \begin{itemize}
        \item If the protected attirbute is a binary one, assuming that the binary values may not be onlu 0 and 1, then the greatest value has been replaced with 1 and the other one with 0.
        \item If the protected attribute is a categorical one then is performed a search of the most frequent value in the column. Then this value in the column is replaced with 1, otherwise with 0. With this policy is established that the fairness must be guaranteed between the most frequent value and the others.
        \item if the protected attribute is a continuous one then is computed the the theshold resulting from the sum of minimum value and maximum value in the column divided by 2. Every value in the column above this threshold is replaced with 1, otherwise with 0. This means that the fairness must be guaranteed between the students that are above the threshold for a specific attribute and the other ones.
    \end{itemize}
\end{itemize}

\subsection{Goal of this work}
The several output in this dataset are considered as protected attributes. More specifically the goal of this work is to predict the english level of a given student. Since the \emph{level\textunderscore ing} attribute is a categorical ones then the output states if a student has the same level of the majority of the students or if not. The goal is to make the predictor and the prediction fair, this means that the belonging of a student to a specific output, the match indeed, must no be influenced by any other factors rather than the predictor's mechanisms.
\newpage
\section{Fairness through unawareness with proxy detection: implementation}
In this section are reported the several implementative issues and features that have been encountered and designed during the development of this algorithm. Two approaches are explored in detail, the one with \textbf{apriori} and the one in which only the \textbf{variables} have been taken in account. 

\subsection{Proxy detection via apriori}
The implementation of this approach has proceeded according a modular approach and, more specifically, has been developed a module named \emph{fairness} in which there are all the sub-modules needed to implement this approach.
\subsubsection{fairness\textunderscore metric module}
This module has been thought to contain the implementation of the several fairness metrics considered in the context of the study. In our scenario it only contains the implementation of the \emph{Disparate Impact} metric. 

This implementation provides to the user the information about the fairness status of the dataset provided together with the protected attributes, the otuput attribute and the possible values that this value may have: 

\begin{lstlisting}
    def fairness_evaluation(self, dataset: pd.DataFrame, protected_attributes: list, output_column_values: list,
                            output_column: str) -> str:
        """
        This method perform an evaluation of the fairness of a given dataset according to the Disparate Impact metric
        :param dataset: this is the dataset on which to be labelled as fair or unfair
        :param protected_attributes: the list of the protected attributes on which compute the disparate impact value
        :param output_column: the column of the dataset that represents the output
        :return: return 'fair' if the dataset is fair, unfair 'otherwise'
        """
        bias_analysis_dataframe = self.bias_detection(dataset, protected_attributes, output_column_values, output_column)
        return_value = 'unfair'
        for value in bias_analysis_dataframe['Disparate Impact'].values:
            if value <= 0.80 or value >= 1.25:
                return_value = 'unfair'
                break
            else:
                return_value = 'fair'

        return return_value
\end{lstlisting}


\subsubsection{matching module}
This module only contains the method \emph{conscious\textunderscore fairness\textunderscore through\textunderscore unawareness} that can return the dataset in two possible configurations:
\begin{enumerate}
    \item if the dataset is fair it returns the dataset itself only with variables pre-processed. Since these pre-processing operations are performed before the fairness checking these are the same for both scenarios. These operations are the transformation of every \emph{categorical} attribute into a numerical one and the transformation of protected attributes as explained above.
    \item if the dataset is not fair the algorithm looks for the proxies and remove the proxies found if there are some and the protected attributes.
\end{enumerate}
\subsubsection{proxy module}
This module is the core of the whole algorithm. This module contains other two sub-modules. More specifically it contains a module whose goal is to detect proxy and the other one is the module in charge to translate the result of the proxy detection to provide a list of proxies to be used into the \emph{matching} module
\begin{enumerate}
    \item \emph{proxy\textunderscore detection}: this module is the one in which the \emph{apriori} algorithm is applied in order to found the relation between antecedents and consequents with a confidence value of 0.8. The key method of this module is the private method \emph{\textunderscore return\textunderscore apriori\textunderscore dataframe}. This method performs computation on the result of the application of the apriori algorithm. More specifically it returns a dataframe with 3 columns \textbf{Antecedent}, \textbf{Consequent}, \textbf{Confidence} 

    \begin{lstlisting}
        def _return_apriori_dataframe(association_results: list) -> pd.DataFrame:
    """This function returns the dataframe with the association generated by the apriori algorithm

    Args:
        association_results (list): is the list of the association returned by the apriori algorithm

    Returns:
        pd.DataFrame: is the dataframe equivalent to the association result list
    """
    antecedent = []
    consequent = []
    confidence = []

    for association_result in association_results:
        for ordered_statistic in association_result.ordered_statistics:
            antecedent_elements = list(ordered_statistic.items_base)
            antecedent.append(antecedent_elements)
            consequent_elements = list(ordered_statistic.items_add)
            consequent.append(consequent_elements)
            confidence_elements = ordered_statistic.confidence
            confidence.append(confidence_elements)

    antecedent_series = pd.Series(antecedent)
    consequent_series = pd.Series(consequent)
    confidence_series = pd.Series(confidence)

    dataframe = pd.DataFrame(
        {'Antecedent': antecedent_series, 'Consequent': consequent_series, 'Confidence': confidence_series})

    return dataframe
    \end{lstlisting}
    \item \emph{proxy\textunderscore processing}: this module contains two key methods. The first method, \emph{\textunderscore return\textunderscore proxy\textunderscore protected\textunderscore attribute} returns a dataframe in which for each sensitive attribute as consequent are related the antecedents that are the possible proxies for that sensitive attribute. The second method is \emph{proxy\textunderscore fixing}, in which for each antecedent is computed the Disparate impact value related to that consequent with that specific value. Only the variables with DI value lesser than 0.8 or greater than 1.25 are considered as proxies. In the following is reported the code of both methods:
    \begin{lstlisting}
        def _return_proxy_protected_attribute(proxy_variables: pd.DataFrame, protected_attributes: list) -> pd.DataFrame:
    """This method returns a dataframe containing the proxy variables for each sensitive attribute

    Args:
        proxy_variables (pd.DataFrame): the dataframe of the proxy variables (antecedent, consequent, confidence)
        protected_attributes (list): the list of the protected attributes

    Returns:
        pd.DataFrame: _description_
    """
    sensitive_antecedent = []
    sensitive_consequent = []
    for index, proxy_row in proxy_variables.iterrows():
        for consequent in proxy_row['Consequent']:
            for sensitive_attribute in protected_attributes:
                if str(consequent).startswith(sensitive_attribute):
                    sensitive_antecedent.append(proxy_row['Antecedent'])
                    sensitive_consequent.append(consequent)

    dataframe = pd.DataFrame(
        {'Antecedent': pd.Series(sensitive_antecedent), 'Consequent': pd.Series(sensitive_consequent)})

    return dataframe


    \end{lstlisting}
    \begin{lstlisting}
        def proxy_fixing(original_dataset: pd.DataFrame, 
        protected_attributes: list) -> pd.DataFrame:
    """This method returns a dataset with proxy variables founded in the original dataset analyzed.
    In case these proxies lead to unfairness the proxies are deleted

    Args: original_dataset (pd.DataFrame): the dataset on which the proxy variable have to be deleted if they lead to
    unfairness
    protected_attributes (list): the list of protected attribute on which perform the proxy analysis

    Returns:
        pd.DataFrame: returns the dataframe in which the proxies do not lead to fairness
    """
    proxy_variables = return_proxy_variables(original_dataset)
    proxy_variables_for_sensitive_attributes = 
    _return_proxy_protected_attribute(proxy_variables, protected_attributes)
    
    dataset = original_dataset
    for index, row in proxy_variables_for_sensitive_attributes.iterrows():
        for antecedent in row['Antecedent']:
            consequent = row['Consequent']

            disparate_impact_value = _compute_disparate_impact_for_proxy(
                antecedent, consequent,original_dataset)

            if not 0.8 < disparate_impact_value < 1.25 and _proxy_format_to_column(antecedent) not in protected_attributes:
                dataset = _remove_proxy_from_dataset(original_dataset, antecedent)

            else:
                continue

    return dataset

    \end{lstlisting}
\end{enumerate}

\subsection{Proxy detection via variables only: implementation}
The implementation of this approach differs with the implementation of the apriori approach because of the operation of proxy detection and processing have been mixed together and the proxy removal operation has been delegated to the \emph{matching} module. In the following there's the code that computes the proxy detection:
\begin{lstlisting}
    def proxy_detection(dataset: pd.DataFrame, protected_attributes: list, output_column: str) -> list:
    attributes_list = []
    for attr in dataset.columns:
        if attr not in protected_attributes:
            attributes_list.append(attr)
            
          
    if output_column in attributes_list:
        attributes_list.remove(output_column)
        
    proxy_list = []
    for attribute in attributes_list:
        for protected_attribute in protected_attributes:
            if _compute_disparate_impact_for_proxy(attribute, protected_attribute, dataset) == 'PROXY':
                proxy_list.append(attribute)

    return proxy_list

\end{lstlisting}
\newpage
\section{Fairness through data rebalancing: implementation}
The first big difference between this approach and the two previous ones is that no fairness-metrics is used here. The reason behind this is the new definition provided for fairness, where is assumed as a parity in combination occurrance. So the core of this algorithm is the \emph{rebalancing} module in which rows are added to the dataset according to the most frequent combination occurrance in the original dataset.
The key part of this module is the one in which the other attributes are inserted. In this scenario a new value is added considering the minimum and the maximum in the temporary dataset, where this dataset is the dataset in which a certain combination of protected\textunderscore attributes and output is occurred. It's furthermore important to remark that since a value has been  randomically chosen this may alter the distribution for that attribute.
\begin{lstlisting}
    for index in range(0, len(combination_list)):
        combination = combination_list[index]
        temp_dataset = final_dataset.query(return_query_for_dataframe(combination, combination_attributes))
        if combination_frequency[index] == combination_frequency_target:
            continue
        else:
            for counter in range(0, combination_frequency_target - combination_frequency[index]):
                new_row = {}
                for (attr, value) in zip(combination_attributes, combination):
                    new_row[attr] = value
                for attribute in final_dataset.columns:
                    if attribute not in combination_attributes:
                        if is_variable_discrete(temp_dataset, attribute):
                            new_row[attribute] = random.randint(temp_dataset[attribute].min(), temp_dataset[attribute].max())
                        else:
                            new_row[attribute] = random.uniform(temp_dataset[attribute].min(), temp_dataset[attribute].max())
                
                final_dataset.loc[len(final_dataset)] = new_row

\end{lstlisting}